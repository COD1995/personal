---
layout: page
title: CSE 455/555 Introduction to Pattern Recognition
description:
img: assets/img/itpr.png
year: 2025
category: undergraduate/graduate
related_publications: false
toc:
    sidebar: right
back_link: '/teaching'
back_text: 'Courses Page'
enable_heading_styles: true

semesters:
  - value: "summer-2025"
    label: "Summer 2025"
    selected: true
  - value: "fall-2024"
    label: "Fall 2024"
  - value: "spring-2026"
    label: "Spring 2026"
---

<div class="course-description-box">
  <p>
    In this updated Pattern Recognition course, we’ll learn not only by lectures and problem sets but by diving directly into the original research papers that shaped the field. Each week, you’ll read several papers—ranging from classic template-matching methods and statistical classifiers to modern deep-learning approaches—then discuss and implement key ideas.
  </p>
  <p>
    Our in-class sessions will be a mix of presentations, guided code walkthroughs, and Q&A. You’ll write brief critical analyses for each paper, code up core algorithms in Python (using NumPy, scikit-learn or PyTorch), and compare performance on benchmark datasets. By the end of the semester, you’ll not only master pattern-recognition techniques but also develop the critical reading and implementation skills needed to engage with cutting-edge research.
  </p>
  <p class="course-note">
    <strong>Prerequisite:</strong> Pre-Requisites: CSE 250 or EAS 230 or EAS 240  or CSE 115 or EAS 999TRCP and EAS 305 or STA 301 and STA 301  or MTH 411; Computer Science, Computer Engineering, or Bioinformatics majors only. Students must complete a mandatory advisement session with their faculty advisor. 
    <br>
    <strong>Resources:</strong> A curated PDF reading list will be provided; expect weekly code exercises and written critiques.
  </p>
</div>


<h2>Instructor Information</h2>
<p><strong>Course Instructor</strong>: Jue Guo</p>
<ul>
  <li><em>Research Area:</em> Optimization for machine learning, Adversarial Learning,
      Continual Learning and Graph Learning
  </li>
  <li>Interested in participating in our research?
      Reach to me by <a href="mailto:jueguo@buffalo.edu">email</a>.
  </li>
</ul>

<!-- Outline heading + short intro text -->
<div class="outline-top">
  <h2>Course Outline and Logistics</h2>
  <p>
    Check out the <a href="#lecture-notes">course material</a> under lecture notes.
  </p>
</div>

<!-- The semester-year selector snippet, placed just below the outline -->
<div class="semester-year-snippet">
  {% include semester-year-toggle.liquid semesters=page.semesters %}
</div>

<!-- 2) Blocks for each semester-year -->
<div data-semester-year="summer-2025">
  {% include teaching/intro_pr/summer-2025.liquid %}
</div>

<div data-semester-year="fall-2024" style="display:none;">

</div>

<div data-semester-year="spring-2026" style="display:none;">

</div>

<h3>Note on Logistics</h3>
<ul>
  <li>A week-ahead notice for mid-term, based on the pace of the course.</li>
  <li>The logistic is <span style="color:red;">subject to change</span> based on
      the overall pace and the performance of the class.</li>
</ul>

<h2>Grading</h2>
<p>The following is the outline of the grading:</p>



<h3>Grading Rubric</h3>
<p>
  This course is <strong>absolute</strong> grading, meaning no curve, as there is a certain standard we need to uphold
  for students to have a good knowledge of algorithm.
</p>

<table class="styled-table">
  <thead>
    <tr>
      <th>Percentage</th>
      <th>Letter Grade</th>
      <th>Percentage</th>
      <th>Letter Grade</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>95-100</td><td>A</td>
      <td>70-74</td><td>C+</td>
    </tr>
    <tr>
      <td>90-94</td><td>A-</td>
      <td>65-69</td><td>C</td>
    </tr>
    <tr>
      <td>85-89</td><td>B+</td>
      <td>60-64</td><td>C-</td>
    </tr>
    <tr>
      <td>80-84</td><td>B</td>
      <td>55-59</td><td>D</td>
    </tr>
    <tr>
      <td>75-79</td><td>B-</td>
      <td>0-54</td><td>F</td>
    </tr>
  </tbody>
</table>


<h3>AI Research Reading List</h3>

<h4>Neural Network Foundations</h4>
<ul>
  <li><strong>Back-Propagation</strong> (Rumelhart, Hinton & Williams, 1986) – <a href="https://gwern.net/doc/ai/nn/1986-rumelhart-2.pdf" target="_blank">PDF</a></li>
  <li><strong>Dropout</strong> (Srivastava et al., 2014) – <a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" target="_blank">PDF</a></li>
  <li><strong>ReLU</strong> (Nair & Hinton, 2010) – <a href="https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf" target="_blank">PDF</a></li>
  <li><strong>Adam</strong> (Kingma & Ba, 2014) – <a href="https://arxiv.org/abs/1412.6980" target="_blank">arXiv:1412.6980</a></li>
  <li><strong>Batch Normalization</strong> (Ioffe & Szegedy, 2015) – <a href="https://arxiv.org/abs/1502.03167" target="_blank">arXiv:1502.03167</a></li>
  <li><strong>RNN Encoder–Decoder</strong> (Cho et al., 2014) – <a href="https://aclanthology.org/D14-1179.pdf" target="_blank">PDF</a></li>
</ul>

<h4>Computer Vision</h4>
<ul>
  <li><strong>AlexNet</strong> (Krizhevsky, Sutskever & Hinton, 2012) – <a href="https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank">NeurIPS 2012 PDF</a></li>
  <li><strong>VGG</strong> (Simonyan & Zisserman, 2014) – <a href="https://arxiv.org/abs/1409.1556" target="_blank">arXiv:1409.1556</a></li>
  <li><strong>ResNet</strong> (He et al., 2015) – <a href="https://arxiv.org/abs/1512.03385" target="_blank">arXiv:1512.03385</a></li>
  <li><strong>Inception Net</strong> (Szegedy et al., 2015) – <a href="https://arxiv.org/abs/1409.4842" target="_blank">arXiv:1409.4842</a></li>
  <li><strong>U-Net</strong> (Ronneberger et al., 2015) – <a href="https://arxiv.org/abs/1505.04597" target="_blank">arXiv:1505.04597</a></li>
  <li><strong>Faster R-CNN</strong> (Ren et al., 2015) – <a href="https://arxiv.org/abs/1506.01497" target="_blank">arXiv:1506.01497</a></li>
  <li><strong>YOLO</strong> (Redmon et al., 2016) – <a href="https://arxiv.org/abs/1506.02640" target="_blank">arXiv:1506.02640</a></li>
  <li><strong>Mask R-CNN</strong> (He et al., 2017) – <a href="https://arxiv.org/abs/1703.06870" target="_blank">arXiv:1703.06870</a></li>
  <li><strong>EfficientNet</strong> (Tan & Le, 2019) – <a href="https://arxiv.org/abs/1905.11946" target="_blank">arXiv:1905.11946</a></li>
</ul>

<h4>Natural Language Processing</h4>
<ul>
  <li><strong>BERT</strong> (Devlin et al., 2018) – <a href="https://arxiv.org/abs/1810.04805" target="_blank">arXiv:1810.04805</a></li>
  <li><strong>GPT-2</strong> (Radford et al., 2019) – <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">PDF</a></li>
  <li><strong>RoBERTa</strong> (Liu et al., 2019) – <a href="https://arxiv.org/abs/1907.11692" target="_blank">arXiv:1907.11692</a></li>
  <li><strong>T5</strong> (Raffel et al., 2020) – <a href="https://arxiv.org/abs/1910.10683" target="_blank">arXiv:1910.10683</a></li>
  <li><strong>GPT-3</strong> (Brown et al., 2020) – <a href="https://arxiv.org/abs/2005.14165" target="_blank">arXiv:2005.14165</a></li>
</ul>

<h4>Graph Neural Networks</h4>
<ul>
  <li><strong>Graph Attention Networks</strong> (Veličković et al., 2018) – <a href="https://arxiv.org/abs/1710.10903" target="_blank">arXiv:1710.10903</a></li>
  <li><strong>Deep Graph Infomax</strong> (Veličković et al., 2019) – <a href="https://arxiv.org/abs/1809.10341" target="_blank">arXiv:1809.10341</a></li>
</ul>

<h4>Adversarial Machine Learning</h4>
<ul>
  <li><strong>Adversarial Examples in the Physical World</strong> (Kurakin, Goodfellow & Bengio, 2016) – <a href="https://arxiv.org/abs/1607.02533" target="_blank">arXiv:1607.02533</a></li>
  <li><strong>Fast Is Better Than Free</strong> (Wong, Rice & Kolter, 2020) – <a href="https://arxiv.org/abs/2001.03994" target="_blank">arXiv:2001.03994</a></li>
</ul>

<h4>Continual Learning</h4>
<ul>
  <li><strong>Overcoming Catastrophic Forgetting</strong> (Kirkpatrick et al., 2017) – <a href="https://arxiv.org/abs/1612.00796" target="_blank">arXiv:1612.00796</a></li>
  <li><strong>Learning without Forgetting</strong> (Li & Hoiem, 2018) – <a href="https://arxiv.org/abs/1606.09282" target="_blank">arXiv:1606.09282</a></li>
</ul>


