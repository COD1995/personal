<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      PyTorch Computer Vision | Jue Guo
    
  
</title>
<meta name="author" content="Jue Guo">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/personal/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/personal/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/personal/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/personal/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/personal/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/personal/assets/courses/basicai/04_pytorch_computer_vision/">

<!-- Dark Mode -->
<script src="/personal/assets/js/theme.js?5ce9accf63efc46cd59e47ccb47fd172"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->





  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/personal//">
          
            
              <span class="font-weight-bold">Jue</span>
            
            
            Guo
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/personal/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/teaching/">courses
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>

    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- main content area -->
            <div class="col-sm-9">
              

<!-- Always apply separation lines (scoped to article) -->

  <style type="text/css">
    /* Heading 1 */
    .post > article h1 {
      border-bottom: 4px solid #4CAF50; /* Green separation line */
      padding-bottom: 12px;
      margin-top: 40px; /* Increased spacing above */
      margin-bottom: 32px; /* Increased spacing below */
    }

    /* Heading 2 */
    .post > article h2 {
      border-bottom: 3px solid #2196F3; /* Blue separation line */
      padding-bottom: 10px;
      margin-top: 36px; /* Increased spacing above */
      margin-bottom: 28px; /* Increased spacing below */
    }

    /* Heading 3 */
    .post > article h3 {
      border-bottom: 3px dashed #FFC107; /* Yellow dashed line */
      padding-bottom: 8px;
      margin-top: 32px; /* Increased spacing above */
      margin-bottom: 24px; /* Increased spacing below */
    }

    /* Heading 4 */
    .post > article h4 {
      border-bottom: 2px dotted #9C27B0; /* Purple dotted line */
      padding-bottom: 6px;
      margin-top: 28px; /* Increased spacing above */
      margin-bottom: 20px; /* Increased spacing below */
    }

    /* Heading 5 */
    .post > article h5 {
      border-bottom: 2px solid #FF5722; /* Orange solid line */
      padding-bottom: 4px;
      margin-top: 24px; /* Increased spacing above */
      margin-bottom: 16px; /* Increased spacing below */
    }
  </style>



  <!-- Numbered Headings CSS -->
  <style type="text/css">
    /* Base reset for counters */
    body {
      counter-reset: h1-counter 3;
    }

    /* Heading 1 */
    .post > article h1 {
      counter-increment: h1-counter;
      counter-reset: h2-counter;
      
      /* Show h1 number */
      
    }

    
    .post > article h1::before {
      content: counter(h1-counter) ". ";
    }
    

    /* Heading 2 */
    .post > article h2 {
      counter-increment: h2-counter;
      counter-reset: h3-counter;
    }

    .post > article h2::before {
      content: counter(h1-counter) "." counter(h2-counter) " ";
    }

    /* Heading 3 */
    .post > article h3 {
      counter-increment: h3-counter;
      counter-reset: h4-counter;
    }

    .post > article h3::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) " ";
    }

    /* Heading 4 */
    .post > article h4 {
      counter-increment: h4-counter;
      counter-reset: h5-counter;
    }

    .post > article h4::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) " ";
    }

    /* Heading 5 */
    .post > article h5 {
      counter-increment: h5-counter;
    }

    .post > article h5::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) "." counter(h5-counter) " ";
    }
  </style>



<div class="post">
  <article>
    <header class="post-header">
      <h1 class="post-title">PyTorch Computer Vision</h1>
      <p class="post-description"></p>
    </header>

    <p><a href="https://en.wikipedia.org/wiki/Computer_vision" rel="external nofollow noopener" target="_blank">Computer vision</a> is the art of teaching a computer to see.</p>

<p>For example, it could involve building a model to classify whether a photo is of a cat or a dog (<a href="https://developers.google.com/machine-learning/glossary#binary-classification" rel="external nofollow noopener" target="_blank">binary classification</a>).</p>

<p>Or whether a photo is of a cat, dog or chicken (<a href="https://developers.google.com/machine-learning/glossary#multi-class-classification" rel="external nofollow noopener" target="_blank">multi-class classification</a>).</p>

<p>Or identifying where a car appears in a video frame (<a href="https://en.wikipedia.org/wiki/Object_detection" rel="external nofollow noopener" target="_blank">object detection</a>).</p>

<p>Or figuring out where different objects in an image can be separated (<a href="https://arxiv.org/abs/1801.00868" rel="external nofollow noopener" target="_blank">panoptic segmentation</a>).</p>

<div class="row mt-3">
  
  <div class="col-sm mt-3 mt-md-0">
    <figure id="example_computer_vision_problems1">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png" class="img-fluid rounded" style="
        
          
          max-width: 100%;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 1: Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.
    </figcaption>
  
</figure>

  </div>
</div>

<h2 id="where-does-computer-vision-get-used">Where does computer vision get used?</h2>

<p>If you use a smartphone, you’ve already used computer vision.</p>

<p>Camera and photo apps use <a href="https://machinelearning.apple.com/research/panoptic-segmentation" rel="external nofollow noopener" target="_blank">computer vision to enhance</a> and sort images.</p>

<p>Modern cars use <a href="https://youtu.be/j0z4FweCy4M?t=2989" rel="external nofollow noopener" target="_blank">computer vision</a> to avoid other cars and stay within lane lines.</p>

<p>Manufacturers use computer vision to identify defects in various products.</p>

<p>Security cameras use computer vision to detect potential intruders.</p>

<p>In essence, anything that can be described in a visual sense can be a potential computer vision problem.</p>

<h2 id="what-were-going-to-cover">What we’re going to cover</h2>

<p>We’re going to apply the PyTorch Workflow we’ve been learning in the past couple of sections to computer vision.</p>

<div class="row mt-3">
  
  <div class="col-sm mt-3 mt-md-0">
    <figure id="pytorch_workflow_computer_vision2">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png" class="img-fluid rounded" style="
        
          
          max-width: 100%;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 2: A PyTorch workflow with a computer vision focus.
    </figcaption>
  
</figure>

  </div>
</div>

<p>Specifically, we’re going to cover:</p>

<div class="table-wrapper">
  <table class="styled-table">
    <thead>
      <tr>
        <th><strong>Topic</strong></th>
        <th><strong>Contents</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>0. Computer vision libraries in PyTorch</strong></td>
        <td>PyTorch has a bunch of built-in helpful computer vision libraries, let's check them out.</td>
      </tr>
      <tr>
        <td><strong>1. Load data</strong></td>
        <td>To practice computer vision, we'll start with some images of different pieces of clothing from <a href="https://github.com/zalandoresearch/fashion-mnist" rel="external nofollow noopener" target="_blank">FashionMNIST</a>.</td>
      </tr>
      <tr>
        <td><strong>2. Prepare data</strong></td>
        <td>We've got some images, let's load them in with a <a href="https://pytorch.org/docs/stable/data.html" rel="external nofollow noopener" target="_blank">PyTorch `DataLoader`</a> so we can use them with our training loop.</td>
      </tr>
      <tr>
        <td><strong>3. Model 0: Building a baseline model</strong></td>
        <td>Here we'll create a multi-class classification model to learn patterns in the data, we'll also choose a <strong>loss function</strong>, <strong>optimizer</strong>, and build a <strong>training loop</strong>.</td>
      </tr>
      <tr>
        <td><strong>4. Making predictions and evaluating model 0</strong></td>
        <td>Let's make some predictions with our baseline model and evaluate them.</td>
      </tr>
      <tr>
        <td><strong>5. Setup device agnostic code for future models</strong></td>
        <td>It's best practice to write device-agnostic code, so let's set it up.</td>
      </tr>
      <tr>
        <td><strong>6. Model 1: Adding non-linearity</strong></td>
        <td>Experimenting is a large part of machine learning, let's try and improve upon our baseline model by adding non-linear layers.</td>
      </tr>
      <tr>
        <td><strong>7. Model 2: Convolutional Neural Network (CNN)</strong></td>
        <td>Time to get computer vision specific and introduce the powerful convolutional neural network architecture.</td>
      </tr>
      <tr>
        <td><strong>8. Comparing our models</strong></td>
        <td>We've built three different models, let's compare them.</td>
      </tr>
      <tr>
        <td><strong>9. Evaluating our best model</strong></td>
        <td>Let's make some predictions on random images and evaluate our best model.</td>
      </tr>
      <tr>
        <td><strong>10. Making a confusion matrix</strong></td>
        <td>A confusion matrix is a great way to evaluate a classification model, let's see how we can make one.</td>
      </tr>
      <tr>
        <td><strong>11. Saving and loading the best performing model</strong></td>
        <td>Since we might want to use our model for later, let's save it and make sure it loads back in correctly.</td>
      </tr>
    </tbody>
  </table>
</div>

<h2 id="computer-vision-libraries-in-pytorch">Computer vision libraries in PyTorch</h2>

<p>Before we get started writing code, let’s talk about some PyTorch computer vision libraries you should be aware of.</p>

<div class="table-wrapper">
  <table class="styled-table">
    <thead>
      <tr>
        <th><strong>PyTorch module</strong></th>
        <th><strong>What does it do?</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="https://pytorch.org/vision/stable/index.html" rel="external nofollow noopener" target="_blank"><code>torchvision</code></a></td>
        <td>Contains datasets, model architectures, and image transformations often used for computer vision problems.</td>
      </tr>
      <tr>
        <td><a href="https://pytorch.org/vision/stable/datasets.html" rel="external nofollow noopener" target="_blank"><code>torchvision.datasets</code></a></td>
        <td>Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification, and more. It also contains <a href="https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets" rel="external nofollow noopener" target="_blank">a series of base classes for making custom datasets</a>.</td>
      </tr>
      <tr>
        <td><a href="https://pytorch.org/vision/stable/models.html" rel="external nofollow noopener" target="_blank"><code>torchvision.models</code></a></td>
        <td>This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch. You can use these with your own problems.</td>
      </tr>
      <tr>
        <td><a href="https://pytorch.org/vision/stable/transforms.html" rel="external nofollow noopener" target="_blank"><code>torchvision.transforms</code></a></td>
        <td>Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model. Common image transformations are found here.</td>
      </tr>
      <tr>
        <td><a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" rel="external nofollow noopener" target="_blank"><code>torch.utils.data.Dataset</code></a></td>
        <td>Base dataset class for PyTorch.</td>
      </tr>
      <tr>
        <td><a href="https://pytorch.org/docs/stable/data.html#module-torch.utils.data" rel="external nofollow noopener" target="_blank"><code>torch.utils.data.DataLoader</code></a></td>
        <td>Creates a Python iterable over a dataset (created with <code>torch.utils.data.Dataset</code>).</td>
      </tr>
    </tbody>
  </table>
</div>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    The <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code> classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.
  </p>
</div>

<p>Now we’ve covered some of the most important PyTorch computer vision libraries, let’s import the relevant dependencies.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Import PyTorch
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Import torchvision
</span><span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>

<span class="c1"># Import matplotlib for visualization
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Check versions
# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PyTorch version: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="se">\n</span><span class="s">torchvision version: </span><span class="si">{</span><span class="n">torchvision</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> PyTorch version: 2.0.1+cu118
torchvision version: 0.15.2+cu118
  </code></pre>
</div>

<h2 id="getting-a-dataset">Getting a dataset</h2>

<p>We’re going to start with FashionMNIST.</p>

<p>MNIST stands for Modified National Institute of Standards and Technology.</p>

<p>The <a href="https://en.wikipedia.org/wiki/MNIST_database" rel="external nofollow noopener" target="_blank">original MNIST dataset</a> contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.</p>

<p><a href="https://github.com/zalandoresearch/fashion-mnist" rel="external nofollow noopener" target="_blank">FashionMNIST</a>, made by Zalando Research, is a similar setup.</p>

<p>Except it contains grayscale images of 10 different kinds of clothing.</p>

<div class="row mt-3">
  
  <div class="col-sm mt-3 mt-md-0">
    <figure id="fashionmnist_example3">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-fashion-mnist-slide.png" class="img-fluid rounded" style="
        
          
          max-width: 100%;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 3: `torchvision.datasets` contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.
    </figcaption>
  
</figure>

  </div>
</div>

<p>Later, we’ll be building a computer vision neural network to identify the different styles of clothing in these images.</p>

<p>PyTorch has a bunch of common computer vision datasets stored in <code class="language-plaintext highlighter-rouge">torchvision.datasets</code>.</p>

<p>Including FashionMNIST in <a href="https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torchvision.datasets.FashionMNIST()</code></a>.</p>

<p>To download it, we provide the following parameters:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">root: str</code> - which folder do you want to download the data to?</li>
  <li>
<code class="language-plaintext highlighter-rouge">train: Bool</code> - do you want the training or test split?</li>
  <li>
<code class="language-plaintext highlighter-rouge">download: Bool</code> - should the data be downloaded?</li>
  <li>
<code class="language-plaintext highlighter-rouge">transform: torchvision.transforms</code> - what transformations would you like to do on the data?</li>
  <li>
<code class="language-plaintext highlighter-rouge">target_transform</code> - you can transform the targets (labels) if you like too.</li>
</ul>

<p>Many other datasets in <code class="language-plaintext highlighter-rouge">torchvision</code> have these parameter options.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Setup training data
</span><span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">FashionMNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># where to download data to?
</span>    <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># get training data
</span>    <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="c1"># download data if it doesn't exist on disk
</span>    <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">(),</span> <span class="c1"># images come as PIL format, we want to turn into Torch tensors
</span>    <span class="n">target_transform</span><span class="o">=</span><span class="bp">None</span> <span class="c1"># you can transform labels as well
</span><span class="p">)</span>

<span class="c1"># Setup testing data
</span><span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">FashionMNIST</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># get test data
</span>    <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">()</span>
<span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz

100%|██████████| 26421880/26421880 [00:01&lt;00:00, 16189161.14it/s]

Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz

100%|██████████| 29515/29515 [00:00&lt;00:00, 269809.67it/s]

Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz

100%|██████████| 4422102/4422102 [00:00&lt;00:00, 4950701.58it/s]

Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz

100%|██████████| 5148/5148 [00:00&lt;00:00, 4744512.63it/s]

Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw
  </code></pre>
</div>

<p>Let’s check out the first sample of the training data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td>
<td class="rouge-code"><pre><span class="c1"># See first training sample
</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         ...
          (remaining rows truncated for brevity)
         ...]]),
 9)
  </code></pre>
</div>

<h3 id="input-and-output-shapes-of-a-computer-vision-model">Input and output shapes of a computer vision model</h3>

<p>We’ve got a big tensor of values (the image) leading to a single value for the target (the label).</p>

<p>Let’s see the image shape.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td>
<td class="rouge-code"><pre><span class="c1"># What's the shape of the image?
</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> torch.Size([1, 28, 28])
  </code></pre>
</div>

<p>The shape of the image tensor is <code class="language-plaintext highlighter-rouge">[1, 28, 28]</code> or more specifically:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td>
<td class="rouge-code"><pre>[color_channels=1, height=28, width=28]
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Having <code class="language-plaintext highlighter-rouge">color_channels=1</code> means the image is grayscale.</p>

<div class="row mt-3">
  
  <div class="col-sm mt-3 mt-md-0">
    <figure id="fashionmnist_input_output_shapes4">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png" class="img-fluid rounded" style="
        
          
          max-width: 100%;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 4: Various problems will have various input and output shapes. But the premise remains: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.
    </figcaption>
  
</figure>

  </div>
</div>

<p>If <code class="language-plaintext highlighter-rouge">color_channels=3</code>, the image comes in pixel values for red, green and blue (this is also known as the <a href="https://en.wikipedia.org/wiki/RGB_color_model" rel="external nofollow noopener" target="_blank">RGB color model</a>).</p>

<p>The order of our current tensor is often referred to as <code class="language-plaintext highlighter-rouge">CHW</code> (Color Channels, Height, Width).</p>

<p>There’s debate on whether images should be represented as <code class="language-plaintext highlighter-rouge">CHW</code> (color channels first) or <code class="language-plaintext highlighter-rouge">HWC</code> (color channels last).</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    You'll also see <code>NCHW</code> and <code>NHWC</code> formats where <code>N</code> stands for <em>number of images</em>. For example, if you have a <code>batch_size=32</code>, your tensor shape may be <code>[32, 1, 28, 28]</code>. We'll cover batch sizes later.
  </p>
</div>

<p>PyTorch generally accepts <code class="language-plaintext highlighter-rouge">NCHW</code> (channels first) as the default for many operators.</p>

<p>However, PyTorch also explains that <code class="language-plaintext highlighter-rouge">NHWC</code> (channels last) performs better and is <a href="https://pytorch.org/blog/tensor-memory-format-matters/#pytorch-best-practice" rel="external nofollow noopener" target="_blank">considered best practice</a>.</p>

<p>For now, since our dataset and models are relatively small, this won’t make too much of a difference.</p>

<p>But keep it in mind for when you’re working on larger image datasets and using convolutional neural networks (we’ll see these later).</p>

<p>Let’s check out more shapes of our data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td>
<td class="rouge-code"><pre><span class="c1"># How many samples are there?
</span><span class="nf">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">.</span><span class="n">data</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">.</span><span class="n">targets</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">.</span><span class="n">data</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">.</span><span class="n">targets</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> (60000, 60000, 10000, 10000)
</code></pre>
</div>

<p>So we’ve got 60,000 training samples and 10,000 testing samples.</p>

<p>What classes are there?</p>

<p>We can find these via the <code class="language-plaintext highlighter-rouge">.classes</code> attribute.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td>
<td class="rouge-code"><pre><span class="c1"># See classes
</span><span class="n">class_names</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">classes</span>
<span class="n">class_names</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> ['T-shirt/top',
 'Trouser',
 'Pullover',
 'Dress',
 'Coat',
 'Sandal',
 'Shirt',
 'Sneaker',
 'Bag',
 'Ankle boot']
  </code></pre>
</div>

<p>Sweet! It looks like we’re dealing with 10 different kinds of clothes.</p>

<p>Because we’re working with 10 different classes, it means our problem is <strong>multi-class classification</strong>.</p>

<p>Let’s get visual.</p>

<h3 id="visualizing-our-data">Visualizing our data</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td>
<td class="rouge-code"><pre><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Image shape: </span><span class="si">{</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">())</span> <span class="c1"># image shape is [1, 28, 28] (colour channels, height, width)
</span><span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">label</span><span class="p">);</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> Image shape: torch.Size([1, 28, 28])
  </code></pre>
</div>
<div style="text-align: left;">
  <img src="/personal/assets/img/03_pytorch_computer_vision_files/03_pytorch_computer_vision_19_1.png" alt="03 PyTorch Computer Vision Sample" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<p>We can turn the image into grayscale using the <code class="language-plaintext highlighter-rouge">cmap</code> parameter of <code class="language-plaintext highlighter-rouge">plt.imshow()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td>
<td class="rouge-code"><pre><span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">gray</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">]);</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div style="text-align: left;">
  <img src="/personal/assets/img/03_pytorch_computer_vision_files/03_pytorch_computer_vision_21_0.png" alt="03 PyTorch Computer Vision Visualization" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<p>Beautiful, well as beautiful as a pixelated grayscale ankle boot can get.</p>

<p>Let’s view a few more.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Plot more images
</span><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">cols</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">random_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]).</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">random_idx</span><span class="p">]</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">gray</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="bp">False</span><span class="p">);</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>
<div style="text-align: left;">
  <img src="/personal/assets/img/03_pytorch_computer_vision_files/03_pytorch_computer_vision_23_0.png" alt="03 PyTorch Computer Vision Sample" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<div class="note-box">
  <strong>Question:</strong>
  <p>
    Do you think the above data can be modeled with only straight (linear) lines? Or do you think you'd also need non-straight (non-linear) lines?
  </p>
</div>

<h2 id="prepare-dataloader">Prepare DataLoader</h2>

<p>Now we’ve got a dataset ready to go.</p>

<p>The next step is to prepare it with a <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code></a> or <code class="language-plaintext highlighter-rouge">DataLoader</code> for short.</p>

<p>The <code class="language-plaintext highlighter-rouge">DataLoader</code> does what you think it might do.</p>

<p>It helps load data into a model.</p>

<p>For training and for inference.</p>

<p>It turns a large <code class="language-plaintext highlighter-rouge">Dataset</code> into a Python iterable of smaller chunks.</p>

<p>These smaller chunks are called <strong>batches</strong> or <strong>mini-batches</strong> and can be set by the <code class="language-plaintext highlighter-rouge">batch_size</code> parameter.</p>

<p>Why do this?</p>

<p>Because it’s more computationally efficient.</p>

<p>In an ideal world you could do the forward pass and backward pass across all of your data at once.</p>

<p>But once you start using really large datasets, unless you’ve got infinite computing power, it’s easier to break them up into batches.</p>

<p>It also gives your model more opportunities to improve.</p>

<p>With <strong>mini-batches</strong> (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).</p>

<p>What’s a good batch size?</p>

<p><a href="https://twitter.com/ylecun/status/989610208497360896?s=20&amp;t=N96J_jotN--PYuJk2WcjMw" rel="external nofollow noopener" target="_blank">32 is a good place to start</a> for a fair amount of problems.</p>

<p>But since this is a value you can set (a <strong>hyperparameter</strong>) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).</p>

<div class="row mt-3">
  
  <div class="col-sm mt-3 mt-md-0">
    <figure id="batching_fashionmnist5">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png" class="img-fluid rounded" style="
        
          
          max-width: 100%;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 5: Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.
    </figcaption>
  
</figure>

  </div>
</div>

<p>Let’s create <code class="language-plaintext highlighter-rouge">DataLoader</code>’s for our training and test sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td>
<td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Setup the batch size hyperparameter
</span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Turn datasets into iterables (batches)
</span><span class="n">train_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="c1"># dataset to turn into iterable
</span>    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="c1"># how many samples per batch?
</span>    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span> <span class="c1"># shuffle data every epoch?
</span><span class="p">)</span>

<span class="n">test_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span> <span class="c1"># don't necessarily have to shuffle the testing data
</span><span class="p">)</span>

<span class="c1"># Let's check out what we've created
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dataloaders: </span><span class="si">{</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Length of train dataloader: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s"> batches of </span><span class="si">{</span><span class="n">BATCH_SIZE</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Length of test dataloader: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span><span class="si">}</span><span class="s"> batches of </span><span class="si">{</span><span class="n">BATCH_SIZE</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> Dataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7fc991463cd0&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7fc991475120&gt;)
Length of train dataloader: 1875 batches of 32
Length of test dataloader: 313 batches of 32
  </code></pre>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Check out what's inside the training dataloader
</span><span class="n">train_features_batch</span><span class="p">,</span> <span class="n">train_labels_batch</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
<span class="n">train_features_batch</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">train_labels_batch</span><span class="p">.</span><span class="n">shape</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> (torch.Size([32, 1, 28, 28]), torch.Size([32]))
  </code></pre>
</div>

<p>And we can see that the data remains unchanged by checking a single sample.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Show a sample
</span><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_features_batch</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]).</span><span class="nf">item</span><span class="p">()</span>
<span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_features_batch</span><span class="p">[</span><span class="n">random_idx</span><span class="p">],</span> <span class="n">train_labels_batch</span><span class="p">[</span><span class="n">random_idx</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">gray</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">class_names</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">Off</span><span class="sh">"</span><span class="p">);</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Image size: </span><span class="si">{</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Label: </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s">, label size: </span><span class="si">{</span><span class="n">label</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> Image size: torch.Size([1, 28, 28])
Label: 6, label size: torch.Size([])
  </code></pre>
</div>

<div style="text-align: left;">
  <img src="/personal/assets/img/03_pytorch_computer_vision_files/03_pytorch_computer_vision_29_1.png" alt="03 PyTorch Computer Vision Visualization" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<h2 id="model-0-build-a-baseline-model">Model 0: Build a baseline model</h2>

<p>Data loaded and prepared!</p>

<p>Time to build a <strong>baseline model</strong> by subclassing <code class="language-plaintext highlighter-rouge">nn.Module</code>.</p>

<p>A <strong>baseline model</strong> is one of the simplest models you can imagine.</p>

<p>You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.</p>

<p>Our baseline will consist of two <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.Linear()</code></a> layers.</p>

<p>We’ve done this in a previous section but there’s going to be one slight difference.</p>

<p>Because we’re working with image data, we’re going to use a different layer to start things off.</p>

<p>And that’s the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.Flatten()</code></a> layer.</p>

<p><code class="language-plaintext highlighter-rouge">nn.Flatten()</code> compresses the dimensions of a tensor into a single vector.</p>

<p>This is easier to understand when you see it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Create a flatten layer
</span><span class="n">flatten_model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">()</span> <span class="c1"># all nn modules function as a model (can do a forward pass)
</span>
<span class="c1"># Get a single sample
</span><span class="n">x</span> <span class="o">=</span> <span class="n">train_features_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Flatten the sample
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">flatten_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># perform forward pass
</span>
<span class="c1"># Print out what happened
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Shape before flattening: </span><span class="si">{</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> -&gt; [color_channels, height, width]</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Shape after flattening: </span><span class="si">{</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> -&gt; [color_channels, height*width]</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Try uncommenting below and see what happens
#print(x)
#print(output)
</span></pre></td>
</tr></tbody></table></code></pre></div></div>
<div class="bash-block">
  <pre><code> Shape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]
Shape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]
  </code></pre>
</div>

<p>The <code class="language-plaintext highlighter-rouge">nn.Flatten()</code> layer took our shape from <code class="language-plaintext highlighter-rouge">[color_channels, height, width]</code> to <code class="language-plaintext highlighter-rouge">[color_channels, height*width]</code>.</p>

<p>Why do this?</p>

<p>Because we’ve now turned our pixel data from height and width dimensions into one long <strong>feature vector</strong>.</p>

<p>And <code class="language-plaintext highlighter-rouge">nn.Linear()</code> layers like their inputs to be in the form of feature vectors.</p>

<p>Let’s create our first model using <code class="language-plaintext highlighter-rouge">nn.Flatten()</code> as the first layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td>
<td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="k">class</span> <span class="nc">FashionMNISTModelV0</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span> <span class="c1"># neural networks like their inputs in vector form
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">),</span> <span class="c1"># in_features = number of features in a data sample (784 pixels)
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">output_shape</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Wonderful!</p>

<p>We’ve got a baseline model class we can use, now let’s instantiate a model.</p>

<p>We’ll need to set the following parameters:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">input_shape=784</code> - this is how many features you’ve got going in the model, in our case, it’s one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features).</li>
  <li>
<code class="language-plaintext highlighter-rouge">hidden_units=10</code> - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small we’ll start with <code class="language-plaintext highlighter-rouge">10</code>.</li>
  <li>
<code class="language-plaintext highlighter-rouge">output_shape=len(class_names)</code> - since we’re working with a multi-class classification problem, we need an output neuron per class in our dataset.</li>
</ul>

<p>Let’s create an instance of our model and send to the CPU for now (we’ll run a small test for running <code class="language-plaintext highlighter-rouge">model_0</code> on CPU vs. a similar model on GPU soon).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td>
<td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Need to setup model with input parameters
</span><span class="n">model_0</span> <span class="o">=</span> <span class="nc">FashionMNISTModelV0</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="c1"># one for every pixel (28x28)
</span>    <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># how many units in the hidden layer
</span>    <span class="n">output_shape</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span> <span class="c1"># one for every class
</span><span class="p">)</span>
<span class="n">model_0</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># keep model on CPU to begin with
</span></pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> FashionMNISTModelV0(
  (layer_stack): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=784, out_features=10, bias=True)
    (2): Linear(in_features=10, out_features=10, bias=True)
  )
)
  </code></pre>
</div>

<h3 id="setup-loss-optimizer-and-evaluation-metrics">Setup loss, optimizer and evaluation metrics</h3>

<p>Since we’re working on a classification problem, let’s bring in our <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">helper_functions.py</code> script</a> and subsequently the <code class="language-plaintext highlighter-rouge">accuracy_fn()</code> we defined in <a href="https://www.learnpytorch.io/02_pytorch_classification/" rel="external nofollow noopener" target="_blank">notebook 02</a>.</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the <a href="https://torchmetrics.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">TorchMetrics package</a>.
  </p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td>
<td class="rouge-code"><pre><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># Download helper functions from Learn PyTorch repo (if not already downloaded)
</span><span class="k">if</span> <span class="nc">Path</span><span class="p">(</span><span class="sh">"</span><span class="s">helper_functions.py</span><span class="sh">"</span><span class="p">).</span><span class="nf">is_file</span><span class="p">():</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">helper_functions.py already exists, skipping download</span><span class="sh">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Downloading helper_functions.py</span><span class="sh">"</span><span class="p">)</span>
  <span class="c1"># Note: you need the "raw" GitHub URL for this to work
</span>  <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py</span><span class="sh">"</span><span class="p">)</span>
  <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">helper_functions.py</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">request</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> helper_functions.py already exists, skipping download
  </code></pre> 
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Import accuracy metric
</span><span class="kn">from</span> <span class="n">helper_functions</span> <span class="kn">import</span> <span class="n">accuracy_fn</span> <span class="c1"># Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)
</span>
<span class="c1"># Setup loss function and optimizer
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span> <span class="c1"># this is also called "criterion"/"cost function" in some places
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_0</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<h3 id="creating-a-function-to-time-our-experiments">Creating a function to time our experiments</h3>

<p>Loss function and optimizer ready!</p>

<p>It’s time to start training a model.</p>

<p>But how about we do a little experiment while we train.</p>

<p>I mean, let’s make a timing function to measure the time it takes our model to train on CPU versus using a GPU.</p>

<p>We’ll train this model on the CPU but the next one on the GPU and see what happens.</p>

<p>Our timing function will import the <a href="https://docs.python.org/3/library/timeit.html#timeit.default_timer" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">timeit.default_timer()</code> function</a> from the Python <a href="https://docs.python.org/3/library/timeit.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">timeit</code> module</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td>
<td class="rouge-code"><pre><span class="kn">from</span> <span class="n">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span> <span class="k">as</span> <span class="n">timer</span>
<span class="k">def</span> <span class="nf">print_train_time</span><span class="p">(</span><span class="n">start</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Prints difference between start and end time.

    Args:
        start (float): Start time of computation (preferred in timeit format).
        end (float): End time of computation.
        device ([type], optional): Device that compute is running on. Defaults to None.

    Returns:
        float: time between start and end in seconds (higher is longer).
    </span><span class="sh">"""</span>
    <span class="n">total_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Train time on </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">total_time</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_time</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<h3 id="creating-a-training-loop-and-training-a-model-on-batches-of-data">Creating a training loop and training a model on batches of data</h3>

<p>Beautiful!</p>

<p>Looks like we’ve got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.</p>

<p>Let’s now create a training loop and a testing loop to train and evaluate our model.</p>

<p>We’ll be using the same steps as the previous notebook(s), though since our data is now in batch form, we’ll add another loop to loop through our data batches.</p>

<p>Our data batches are contained within our <code class="language-plaintext highlighter-rouge">DataLoader</code>s, <code class="language-plaintext highlighter-rouge">train_dataloader</code> and <code class="language-plaintext highlighter-rouge">test_dataloader</code> for the training and test data splits respectively.</p>

<p>A batch is <code class="language-plaintext highlighter-rouge">BATCH_SIZE</code> samples of <code class="language-plaintext highlighter-rouge">X</code> (features) and <code class="language-plaintext highlighter-rouge">y</code> (labels), since we’re using <code class="language-plaintext highlighter-rouge">BATCH_SIZE=32</code>, our batches have 32 samples of images and targets.</p>

<p>And since we’re computing on batches of data, our loss and evaluation metrics will be calculated <strong>per batch</strong> rather than across the whole dataset.</p>

<p>This means we’ll have to divide our loss and accuracy values by the number of batches in each dataset’s respective dataloader.</p>

<p>Let’s step through it:</p>
<ol>
  <li>Loop through epochs.</li>
  <li>Loop through training batches, perform training steps, calculate the train loss <em>per batch</em>.</li>
  <li>Loop through testing batches, perform testing steps, calculate the test loss <em>per batch</em>.</li>
  <li>Print out what’s happening.</li>
  <li>Time it all (for fun).</li>
</ol>

<p>A fair few steps but…</p>

<p>…if in doubt, code it out.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Import tqdm for progress bar
</span><span class="kn">from</span> <span class="n">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Set the seed and start the timer
</span><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_time_start_on_cpu</span> <span class="o">=</span> <span class="nf">timer</span><span class="p">()</span>

<span class="c1"># Set the number of epochs (we'll keep this small for faster training times)
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Create training and testing loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="se">\n</span><span class="s">-------</span><span class="sh">"</span><span class="p">)</span>
    <span class="c1">### Training
</span>    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Add a loop to loop through training batches
</span>    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        <span class="n">model_0</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="c1"># 1. Forward pass
</span>        <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model_0</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># 2. Calculate loss (per batch)
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span> <span class="c1"># accumulatively add up the loss per epoch
</span>
        <span class="c1"># 3. Optimizer zero grad
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

        <span class="c1"># 4. Loss backward
</span>        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

        <span class="c1"># 5. Optimizer step
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># Print out how many samples have been seen
</span>        <span class="k">if</span> <span class="n">batch</span> <span class="o">%</span> <span class="mi">400</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Looked at </span><span class="si">{</span><span class="n">batch</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="s"> samples</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Divide total train loss by length of train dataloader (average loss per batch per epoch)
</span>    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>

    <span class="c1">### Testing
</span>    <span class="c1"># Setup variables for accumulatively adding up loss and accuracy
</span>    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model_0</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>
            <span class="c1"># 1. Forward pass
</span>            <span class="n">test_pred</span> <span class="o">=</span> <span class="nf">model_0</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1"># 2. Calculate loss (accumulatively)
</span>            <span class="n">test_loss</span> <span class="o">+=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">test_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># accumulatively add up the loss per epoch
</span>
            <span class="c1"># 3. Calculate accuracy (preds need to be same as y_true)
</span>            <span class="n">test_acc</span> <span class="o">+=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Calculations on test metrics need to happen inside torch.inference_mode()
</span>        <span class="c1"># Divide total test loss by length of test dataloader (per batch)
</span>        <span class="n">test_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span>

        <span class="c1"># Divide total accuracy by length of test dataloader (per batch)
</span>        <span class="n">test_acc</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">test_dataloader</span><span class="p">)</span>

    <span class="c1">## Print out what's happening
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Train loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> | Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s">, Test acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Calculate training time
</span><span class="n">train_time_end_on_cpu</span> <span class="o">=</span> <span class="nf">timer</span><span class="p">()</span>
<span class="n">total_train_time_model_0</span> <span class="o">=</span> <span class="nf">print_train_time</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">train_time_start_on_cpu</span><span class="p">,</span>
                                           <span class="n">end</span><span class="o">=</span><span class="n">train_time_end_on_cpu</span><span class="p">,</span>
                                           <span class="n">device</span><span class="o">=</span><span class="nf">str</span><span class="p">(</span><span class="nf">next</span><span class="p">(</span><span class="n">model_0</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span><span class="p">))</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> 0%|          | 0/3 [00:00&lt;?, ?it/s]

Epoch: 0
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%

Epoch: 1
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%

Epoch: 2
-------
Looked at 0/60000 samples
Looked at 12800/60000 samples
Looked at 25600/60000 samples
Looked at 38400/60000 samples
Looked at 51200/60000 samples

Train loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%

Train time on cpu: 32.349 seconds
  </code></pre>
</div>

<p>Nice! Looks like our baseline model did fairly well.</p>

<p>It didn’t take too long to train either, even just on the CPU, I wonder if it’ll speed up on the GPU?</p>

<p>Let’s write some code to evaluate our model.</p>

<h2 id="make-predictions-and-get-model-0-results">Make predictions and get Model 0 results</h2>

<p>Since we’re going to be building a few models, it’s a good idea to write some code to evaluate them all in similar ways.</p>

<p>Namely, let’s create a function that takes in a trained model, a <code class="language-plaintext highlighter-rouge">DataLoader</code>, a loss function and an accuracy function.</p>

<p>The function will use the model to make predictions on the data in the <code class="language-plaintext highlighter-rouge">DataLoader</code> and then we can evaluate those predictions using the loss function and accuracy function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td>
<td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">,</span>
               <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">accuracy_fn</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Returns a dictionary containing the results of model predicting on data_loader.

    Args:
        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.
        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.
        loss_fn (torch.nn.Module): The loss function of model.
        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.

    Returns:
        (dict): Results of model making predictions on data_loader.
    </span><span class="sh">"""</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="c1"># Make predictions with the model
</span>            <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1"># Accumulate the loss and accuracy values per batch
</span>            <span class="n">loss</span> <span class="o">+=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">acc</span> <span class="o">+=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)
</span>
        <span class="c1"># Scale loss and acc to find the average loss/acc per batch
</span>        <span class="n">loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">model_name</span><span class="sh">"</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span> <span class="c1"># only works when model was created with a class
</span>            <span class="sh">"</span><span class="s">model_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">model_acc</span><span class="sh">"</span><span class="p">:</span> <span class="n">acc</span><span class="p">}</span>

<span class="c1"># Calculate model 0 results on test dataset
</span><span class="n">model_0_results</span> <span class="o">=</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_0</span><span class="p">,</span> <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span>
<span class="p">)</span>
<span class="n">model_0_results</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> {'model_name': 'FashionMNISTModelV0',
 'model_loss': 0.47663894295692444,
 'model_acc': 83.42651757188499}
  </code></pre>
</div>

<p>Looking good!</p>

<p>We can use this dictionary to compare the baseline model results to other models later on.</p>

<h2 id="setup-device-agnostic-code-for-using-a-gpu-if-there-is-one">Setup device agnostic-code (for using a GPU if there is one)</h2>
<p>We’ve seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    Model training time is dependent on hardware used. Generally, more processors mean faster training, and smaller models on smaller datasets will often train faster than large models and large datasets.
  </p>
</div>

<p>Now let’s setup some <a href="https://pytorch.org/docs/stable/notes/cuda.html#best-practices" rel="external nofollow noopener" target="_blank">device-agnostic code</a> for our models and data to run on GPU if it’s available.</p>

<p>If you’re running this notebook on Google Colab, and you don’t have a GPU turned on yet, it’s now time to turn one on via <code class="language-plaintext highlighter-rouge">Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>. If you do this, your runtime will likely reset and you’ll have to run all of the cells above by going <code class="language-plaintext highlighter-rouge">Runtime -&gt; Run before</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Setup device agnostic code
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="n">device</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> 'cuda'
  </code></pre>
</div>

<p>Beautiful!</p>

<p>Let’s build another model.</p>

<h2 id="model-1-building-a-better-model-with-non-linearity">Model 1: Building a better model with non-linearity</h2>

<p>We learned about <a href="/personal/assets/courses/basicai/03_pytorch_classification">the power of non-linearity</a> in previous session.</p>

<p>Seeing the data we’ve been working with, do you think it needs non-linear functions?</p>

<p>And remember, linear means straight and non-linear means non-straight.</p>

<p>Let’s find out.</p>

<p>We’ll do so by recreating a similar model to before, except this time we’ll put non-linear functions (<code class="language-plaintext highlighter-rouge">nn.ReLU()</code>) in between each linear layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Create a model with non-linear and linear layers
</span><span class="k">class</span> <span class="nc">FashionMNISTModelV1</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span> <span class="c1"># flatten inputs into single vector
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">output_shape</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>That looks good.</p>

<p>Now let’s instantiate it with the same settings we used before.</p>

<p>We’ll need <code class="language-plaintext highlighter-rouge">input_shape=784</code> (equal to the number of features of our image data), <code class="language-plaintext highlighter-rouge">hidden_units=10</code> (starting small and the same as our baseline model) and <code class="language-plaintext highlighter-rouge">output_shape=len(class_names)</code> (one output unit per class).</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    Notice how we kept most of the settings of our model the same except for one change: adding non-linear layers. This is a standard practice for running a series of machine learning experiments — change one thing and see what happens, then do it again, again, again.
  </p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td>
<td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="nc">FashionMNISTModelV1</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="c1"># number of input features
</span>    <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">output_shape</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span> <span class="c1"># number of output classes desired
</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># send model to GPU if it's available
</span><span class="nf">next</span><span class="p">(</span><span class="n">model_1</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span> <span class="c1"># check model device
</span></pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code>device(type='cuda', index=0)
  </code></pre>
</div>

<h3 id="setup-loss-optimizer-and-evaluation-metrics-1">Setup loss, optimizer and evaluation metrics</h3>

<p>As usual, we’ll setup a loss function, an optimizer and an evaluation metric (we could do multiple evaluation metrics but we’ll stick with accuracy for now).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td>
<td class="rouge-code"><pre><span class="kn">from</span> <span class="n">helper_functions</span> <span class="kn">import</span> <span class="n">accuracy_fn</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_1</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
                            <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<h3 id="functionizing-training-and-test-loops">Functionizing training and test loops</h3>

<p>So far we’ve been writing train and test loops over and over.</p>

<p>Let’s write them again but this time we’ll put them in functions so they can be called again and again.</p>

<p>And because we’re using device-agnostic code now, we’ll be sure to call <code class="language-plaintext highlighter-rouge">.to(device)</code> on our feature (<code class="language-plaintext highlighter-rouge">X</code>) and target (<code class="language-plaintext highlighter-rouge">y</code>) tensors.</p>

<p>For the training loop we’ll create a function called <code class="language-plaintext highlighter-rouge">train_step()</code> which takes in a model, a <code class="language-plaintext highlighter-rouge">DataLoader</code> a loss function and an optimizer.</p>

<p>The testing loop will be similar but it’ll be called <code class="language-plaintext highlighter-rouge">test_step()</code> and it’ll take in a model, a <code class="language-plaintext highlighter-rouge">DataLoader</code>, a loss function and an evaluation function.</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    Since these are functions, you can customize them in any way you like. What we're making here can be considered barebones training and testing functions for our specific classification use case.
  </p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
</pre></td>
<td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">,</span>
               <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Optimizer</span><span class="p">,</span>
               <span class="n">accuracy_fn</span><span class="p">,</span>
               <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="c1"># Send data to GPU
</span>        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># 1. Forward pass
</span>        <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># 2. Calculate loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">train_acc</span> <span class="o">+=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                 <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># Go from logits -&gt; pred labels
</span>
        <span class="c1"># 3. Optimizer zero grad
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

        <span class="c1"># 4. Loss backward
</span>        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

        <span class="c1"># 5. Optimizer step
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1"># Calculate loss and accuracy per epoch and print out what's happening
</span>    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="n">train_acc</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Train loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> | Train accuracy: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">,</span>
              <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
              <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
              <span class="n">accuracy_fn</span><span class="p">,</span>
              <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> <span class="c1"># put model in eval mode
</span>    <span class="c1"># Turn on inference context manager
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="c1"># Send data to GPU
</span>            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># 1. Forward pass
</span>            <span class="n">test_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="c1"># 2. Calculate loss and accuracy
</span>            <span class="n">test_loss</span> <span class="o">+=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">test_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">test_acc</span> <span class="o">+=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Go from logits -&gt; pred labels
</span>            <span class="p">)</span>

        <span class="c1"># Adjust metrics and print out
</span>        <span class="n">test_loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> | Test accuracy: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<p>Woohoo!</p>

<p>Now we’ve got some functions for training and testing our model, let’s run them.</p>

<p>We’ll do so inside another loop for each epoch.</p>

<p>That way, for each epoch, we’re going through a training step and a testing step.</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or, in our case, every epoch.
  </p>
</div>

<p>Let’s also time things to see how long our code takes to run on the GPU.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td>
<td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Measure time
</span><span class="kn">from</span> <span class="n">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span> <span class="k">as</span> <span class="n">timer</span>
<span class="n">train_time_start_on_gpu</span> <span class="o">=</span> <span class="nf">timer</span><span class="p">()</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="se">\n</span><span class="s">---------</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">train_step</span><span class="p">(</span><span class="n">data_loader</span><span class="o">=</span><span class="n">train_dataloader</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
        <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span>
    <span class="p">)</span>
    <span class="nf">test_step</span><span class="p">(</span><span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span>
        <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
        <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span>
    <span class="p">)</span>

<span class="n">train_time_end_on_gpu</span> <span class="o">=</span> <span class="nf">timer</span><span class="p">()</span>
<span class="n">total_train_time_model_1</span> <span class="o">=</span> <span class="nf">print_train_time</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">train_time_start_on_gpu</span><span class="p">,</span>
                                            <span class="n">end</span><span class="o">=</span><span class="n">train_time_end_on_gpu</span><span class="p">,</span>
                                            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> 0%|          | 0/3 [00:00&lt;?, ?it/s]

Epoch: 0
---------
Train loss: 1.09199 | Train accuracy: 61.34%
Test loss: 0.95636 | Test accuracy: 65.00%

Epoch: 1
---------
Train loss: 0.78101 | Train accuracy: 71.93%
Test loss: 0.72227 | Test accuracy: 73.91%

Epoch: 2
---------
Train loss: 0.67027 | Train accuracy: 75.94%
Test loss: 0.68500 | Test accuracy: 75.02%

Train time on cuda: 36.878 seconds
  </code></pre>
</div>

<p>Excellent!</p>

<p>Our model trained but the training time took longer?</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using. Read on for a more explained answer.
  </p>
</div>

<div class="note-box">
  <strong>Question:</strong>
  <p>
    "I used a GPU but my model didn't train faster, why might that be?"
  </p>
  <strong>Answer:</strong>
  <p>
    Well, one reason could be because your dataset and model are both so small (like the dataset and model we're working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.
  </p>
  <p>
    There's a small bottleneck between copying data from the CPU memory (default) to the GPU memory.
  </p>
  <p>
    So for smaller models and datasets, the CPU might actually be the optimal place to compute on.
  </p>
  <p>
    But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.
  </p>
  <p>
    However, this is largely dependent on the hardware you're using. With practice, you will get used to where the best place to train your models is.
  </p>
</div>

<p>Let’s evaluate our trained <code class="language-plaintext highlighter-rouge">model_1</code> using our <code class="language-plaintext highlighter-rouge">eval_model()</code> function and see how it went.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td>
<td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Note: This will error due to `eval_model()` not using device agnostic code
</span><span class="n">model_1_results</span> <span class="o">=</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span>
    <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span><span class="p">)</span>
<span class="n">model_1_results</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code class="traceback">
---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;cell line: 4&gt;
# Note: This will error due to `eval_model()` not using device agnostic code
----&gt; model_1_results = eval_model(model=model_1, 
                                     data_loader=test_dataloader,
                                     loss_fn=loss_fn)

&lt;ipython-input-20&gt; in eval_model(model, data_loader, loss_fn, accuracy_fn)
# Make predictions with the model
----&gt; y_pred = model(X)

&lt;ipython-input-22&gt; in forward(self, x)
----&gt; return self.layer_stack(x)

&lt;usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py&gt; in forward(self, input)
----&gt; return F.linear(input, self.weight, self.bias)

<span class="error-msg">RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</span>
  </code></pre>
</div>

<p>Oh no!</p>

<p>It looks like our <code class="language-plaintext highlighter-rouge">eval_model()</code> function errors out with:</p>

<div class="note-box">
  <strong>Error:</strong>
  <p>
    <code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</code>
  </p>
</div>

<p>It’s because we’ve setup our data and model to use device-agnostic code but not our evaluation function.</p>

<p>How about we fix that by passing a target <code class="language-plaintext highlighter-rouge">device</code> parameter to our <code class="language-plaintext highlighter-rouge">eval_model()</code> function?</p>

<p>Then we’ll try calculating the results again.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Move values to device
</span><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">data_loader</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">,</span>
               <span class="n">loss_fn</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">accuracy_fn</span><span class="p">,</span>
               <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Evaluates a given model on a given dataset.

    Args:
        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.
        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.
        loss_fn (torch.nn.Module): The loss function of model.
        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.
        device (str, optional): Target device to compute on. Defaults to device.

    Returns:
        (dict): Results of model making predictions on data_loader.
    </span><span class="sh">"""</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="c1"># Send data to the target device
</span>            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">acc</span> <span class="o">+=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Scale loss and acc
</span>        <span class="n">loss</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">/=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">model_name</span><span class="sh">"</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span> <span class="c1"># only works when model was created with a class
</span>            <span class="sh">"</span><span class="s">model_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">(),</span>
            <span class="sh">"</span><span class="s">model_acc</span><span class="sh">"</span><span class="p">:</span> <span class="n">acc</span><span class="p">}</span>

<span class="c1"># Calculate model 1 results with device-agnostic code
</span><span class="n">model_1_results</span> <span class="o">=</span> <span class="nf">eval_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_1</span><span class="p">,</span> <span class="n">data_loader</span><span class="o">=</span><span class="n">test_dataloader</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">accuracy_fn</span><span class="o">=</span><span class="n">accuracy_fn</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span>
<span class="p">)</span>
<span class="n">model_1_results</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code> {'model_name': 'FashionMNISTModelV1',
 'model_loss': 0.6850008964538574,
 'model_acc': 75.01996805111821}
  </code></pre>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Check baseline results
</span><span class="n">model_0_results</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>
<div class="bash-block">
  <pre><code> {'model_name': 'FashionMNISTModelV0',
 'model_loss': 0.47663894295692444,
 'model_acc': 83.42651757188499}
  </code></pre>
</div>

<p>Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.</p>

<p>That’s a thing to note in machine learning, sometimes the thing you thought should work doesn’t.</p>

<p>And then the thing you thought might not work does.</p>

<p>It’s part science, part art.</p>

<p>From the looks of things, it seems like our model is <strong>overfitting</strong> on the training data.</p>

<p>Overfitting means our model is learning the training data well but those patterns aren’t generalizing to the testing data.</p>

<p>Two of the main ways to fix overfitting include:</p>
<ol>
  <li>Using a smaller or different model (some models fit certain kinds of data better than others).</li>
  <li>Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).</li>
</ol>

<p>There are more, but I’m going to leave that as a challenge for you to explore.</p>

<p>Try searching online, “ways to prevent overfitting in machine learning” and see what comes up.</p>

<p>In the meantime, let’s take a look at number 1: using a different model.</p>

<h2 id="model-2-building-a-convolutional-neural-network-cnn">Model 2: Building a Convolutional Neural Network (CNN)</h2>

<p>Alright, time to step things up a notch.</p>

<p>It’s time to create a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="external nofollow noopener" target="_blank">Convolutional Neural Network</a> (CNN or ConvNet).</p>

<p>CNN’s are known for their capabilities to find patterns in visual data.</p>

<p>And since we’re dealing with visual data, let’s see if using a CNN model can improve upon our baseline.</p>

<p>The CNN model we’re going to be using is known as TinyVGG from the <a href="https://poloclub.github.io/cnn-explainer/" rel="external nofollow noopener" target="_blank">CNN Explainer</a> website.</p>

<p>It follows the typical structure of a convolutional neural network:</p>

<p><code class="language-plaintext highlighter-rouge">Input layer -&gt; [Convolutional layer -&gt; activation layer -&gt; pooling layer] -&gt; Output layer</code></p>

<p>Where the contents of <code class="language-plaintext highlighter-rouge">[Convolutional layer -&gt; activation layer -&gt; pooling layer]</code> can be upscaled and repeated multiple times, depending on requirements.</p>

<h3 id="what-model-should-i-use">What model should I use?</h3>

<div class="note-box">
  <strong>Question:</strong>
  <p>
    Wait, you say CNN's are good for images, are there any other model types I should be aware of?
  </p>
</div>

<p>Good question.</p>

<p>This table is a good general guide for which model to use (though there are exceptions).</p>

<div class="table-wrapper">
  <table class="styled-table">
    <thead>
      <tr>
        <th><strong>Problem type</strong></th>
        <th><strong>Model to use (generally)</strong></th>
        <th><strong>Code example</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Structured data (Excel spreadsheets, row and column data)</td>
        <td>Gradient boosted models, Random Forests, XGBoost</td>
        <td>
          <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble" rel="external nofollow noopener" target="_blank"><code>sklearn.ensemble</code></a>, 
          <a href="https://xgboost.readthedocs.io/en/stable/" rel="external nofollow noopener" target="_blank">XGBoost library</a>
        </td>
      </tr>
      <tr>
        <td>Unstructured data (images, audio, language)</td>
        <td>Convolutional Neural Networks, Transformers</td>
        <td>
          <a href="https://pytorch.org/vision/stable/models.html" rel="external nofollow noopener" target="_blank"><code>torchvision.models</code></a>, 
          <a href="https://huggingface.co/docs/transformers/index" rel="external nofollow noopener" target="_blank">HuggingFace Transformers</a>
        </td>
      </tr>
    </tbody>
  </table>
</div>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    The table above is only for reference. The model you end up using will be highly dependent on the problem you're working on and the constraints you have (amount of data, latency requirements).
  </p>
</div>

<p>Enough talking about models, let’s now build a CNN that replicates the model on the <a href="https://poloclub.github.io/cnn-explainer/" rel="external nofollow noopener" target="_blank">CNN Explainer website</a>.</p>

<div class="row mt-3">
  
  <div class="col-sm mt-3 mt-md-0">
    <figure id="tinyvgg_architecture6">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-cnn-explainer-model.png" class="img-fluid rounded" style="
        
          
          max-width: 100%;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 6: TinyVGG architecture, as setup by CNN explainer website.
    </figcaption>
  
</figure>

  </div>
</div>

<p>To do so, we’ll leverage the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.Conv2d()</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.MaxPool2d()</code></a> layers from <code class="language-plaintext highlighter-rouge">torch.nn</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
</pre></td>
<td class="rouge-code"><pre><span class="c1"># Create a convolutional neural network
</span><span class="k">class</span> <span class="nc">FashionMNISTModelV2</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Model architecture copying TinyVGG from:
    https://poloclub.github.io/cnn-explainer/
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="c1"># how big is the square that's going over the image?
</span>                      <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># default
</span>                      <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="c1"># options = "valid" (no padding) or "same" (output has same shape as input) or int for specific number
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                      <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                         <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># default stride value is same as kernel_size
</span>        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hidden_units</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
            <span class="c1"># Where did this in_features shape come from?
</span>            <span class="c1"># It's because each layer of our network compresses and changes the shape of our input data.
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_units</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="p">,</span>
                      <span class="n">out_features</span><span class="o">=</span><span class="n">output_shape</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># print(x.shape)
</span>        <span class="k">return</span> <span class="n">x</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model_2</span> <span class="o">=</span> <span class="nc">FashionMNISTModelV2</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">hidden_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">output_shape</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">class_names</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_2</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code>FashionMNISTModelV2(
  (block_1): Sequential(
    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (block_2): Sequential(
    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=490, out_features=10, bias=True)
  )
)
  </code></pre>
</div>

<p>Nice!</p>

<p>Our biggest model yet!</p>

<p>What we’ve done is a common practice in machine learning.</p>

<p>Find a model architecture somewhere and replicate it with code.</p>

<h3 id="stepping-through-nnconv2d">Stepping through <code class="language-plaintext highlighter-rouge">nn.Conv2d()</code>
</h3>

<p>In this course we will not go through the theoretical details of convolutional neural networks. If you are interested please check</p>

<p>We could start using our model above and see what happens but let’s first step through the two new layers we’ve added:</p>
<ul>
  <li>
<a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.Conv2d()</code></a>, also known as a convolutional layer.</li>
  <li>
<a href="https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.MaxPool2d()</code></a>, also known as a max pooling layer.</li>
</ul>

<div class="note-box">
  <strong>Question:</strong>
  <p>
    What does the "2d" in <code>nn.Conv2d()</code> stand for?
  </p>
  <strong>Answer:</strong>
  <p>
    The 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, there's a color channel dimension, but each of the color channel dimensions has two dimensions too: height and width.
  </p>
  <p>
    For other dimensional data (such as 1D for text or 3D for 3D objects), there's also <code>nn.Conv1d()</code> and <code>nn.Conv3d()</code>.
  </p>
</div>

<p>To test the layers out, let’s create some toy data just like the data used on CNN Explainer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td>
<td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create sample batch of random numbers with same size as image batch
</span><span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span> <span class="c1"># [batch_size, color_channels, height, width]
</span><span class="n">test_image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># get a single image for testing
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Image batch shape: </span><span class="si">{</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> -&gt; [batch_size, color_channels, height, width]</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Single image shape: </span><span class="si">{</span><span class="n">test_image</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> -&gt; [color_channels, height, width]</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Single image pixel values:</span><span class="se">\n</span><span class="si">{</span><span class="n">test_image</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code>Image batch shape: torch.Size([32, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]
Single image shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]
Single image pixel values:
tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],
         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],
         ...,
         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],
         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],
         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],

        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],
         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],
         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],
         ...,
         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],
         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],
         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],

        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],
         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],
         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],
         ...,
         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],
         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],
         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])
  </code></pre>
</div>

<p>Let’s create an example <code class="language-plaintext highlighter-rouge">nn.Conv2d()</code> with various parameters:</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">in_channels</code> (int) - Number of channels in the input image.</li>
  <li>
<code class="language-plaintext highlighter-rouge">out_channels</code> (int) - Number of channels produced by the convolution.</li>
  <li>
<code class="language-plaintext highlighter-rouge">kernel_size</code> (int or tuple) - Size of the convolving kernel/filter.</li>
  <li>
<code class="language-plaintext highlighter-rouge">stride</code> (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.</li>
  <li>
<code class="language-plaintext highlighter-rouge">padding</code> (int, tuple, str) - Padding added to all four sides of input. Default: 0.</li>
</ul>

<div class="row mt-3">
  
  <div class="col-sm mt-3 mt-md-0">
    <figure id="conv2d_layer_example7">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv2d-layer.gif" class="img-fluid rounded" style="
        
          
          max-width: 100%;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 7: Example of what happens when you change the hyperparameters of a <code>nn.Conv2d()</code> layer.
    </figcaption>
  
</figure>

  </div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td>
<td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create a convolutional layer with same dimensions as TinyVGG
# (try changing any of the parameters and see what happens)
</span><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                       <span class="n">out_channels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                       <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                       <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                       <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># also try using "valid" or "same" here
</span>
<span class="c1"># Pass the data through the convolutional layer
</span><span class="nf">conv_layer</span><span class="p">(</span><span class="n">test_image</span><span class="p">)</span> <span class="c1"># Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)
</span></pre></td>
</tr></tbody></table></code></pre></div></div>

<div class="bash-block">
  <pre><code>tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],
         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],
         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],
         ...,
         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],
         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],
         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],

        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],
         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],
         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],
         ...,
         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],
         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],
         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],

        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],
         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],
         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],
         ...,
         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],
         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],
         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],

        ...,

        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],
         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],
         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],
         ...,
         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],
         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],
         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],

        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],
         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],
         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],
         ...,
         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],
         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],
         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],

        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],
         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],
         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],
         ...,
         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],
         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],
         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],
       grad_fn=&lt;SqueezeBackward1&gt;)
  </code></pre>
</div>


    

    
  </article>
</div>

            </div>
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top">
                <!-- The TOC is injected by JS or jekyll-toc plugin -->
              </nav>
            </div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2025
      Jue
      
      Guo. 
      
      
        Last updated: January 06, 2025.
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/personal/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/personal/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/personal/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/personal/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/personal/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/personal/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/personal/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  

    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/personal/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    


    <!-- Inline JS snippet: always show the Back link if page.back_link is set,
     either in #toc-sidebar or, if that doesn't exist, in the navbar. -->
    <script>
      window.addEventListener("load", function() {
        // If page.back_link is present, show a back button
        
        // If page.back_text is missing, default to "Back"
        

        // We'll build two versions of the snippet:
        // 1) A <div> version for the TOC sidebar (no bullet marker)
        // 2) A <li> version for the navbar fallback

        const tocHTML = `
          <div class="back-link" style="margin-top:1rem;">
            <a href="/personal/teaching/aibasic" style="text-decoration: none; display: inline-flex; align-items: center;">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                  viewBox="0 0 24 24" style="margin-right:0.4rem;">
                <path d="M0 0h24v24H0z" fill="none"/>
                <path d="M8 7v4L2 6l6-5v4h5a8 8 0 1 1 0 16H4v-2h9a6 6 0 1 0 0-12H8z"/>
              </svg>
              <span>AI Basics</span>
            </a>
          </div>
        `;

        const navHTML = `
          <li class="nav-item back-link" style=""display: inline-flex; align-items: center;"">
            <a class="nav-link" href="/personal/teaching/aibasic" style="display: inline-flex; align-items: center;">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                  viewBox="0 0 24 24" style="margin-right:0.4rem;">
                <path d="M0 0h24v24H0z" fill="none"/>
                <path d="M8 7v4L2 6l6-5v4h5a8 8 0 1 1 0 16H4v-2h9a6 6 0 1 0 0-12H8z"/>
              </svg>
              <span>AI Basics</span>
            </a>
          </li>
        `;

        // 1) If #toc-sidebar exists, we insert the "tocHTML" (a <div>)
        let tocSidebar = document.getElementById("toc-sidebar");
        if (tocSidebar) {
          tocSidebar.insertAdjacentHTML("beforeend", tocHTML);
        } else {
          // 2) Otherwise, we append the "navHTML" (<li>) to the navbar
          //    so user sees the link in the top nav
          let navbarUl = document.querySelector("#navbarNav ul.navbar-nav");
          if (navbarUl) {
            navbarUl.insertAdjacentHTML("beforeend", navHTML);
          }
        }
        
      });
      </script>

    <!-- Inline JS to number the TOC items, handling userOffset for ANY > 2 -->
    <script>
      window.addEventListener("load", function() {
        // If page.start_h1_number is not set, or not an integer,
        // treat it as "no offset" => start at 1, 2, 3
        

        const parsedOffset = parseInt("3", 10);
        const hasUserOffset = !isNaN(parsedOffset);
        
        const rootUl = document.querySelector("#toc-sidebar > ul");
        if (!rootUl) return;

        function numberList(ul, prefix = "", startIndex = 1) {
          const liElements = ul.querySelectorAll(":scope > li");
          let idx = startIndex;

          liElements.forEach(li => {
            let label;

            if (!prefix) {
              // Top-level items
              if (!hasUserOffset) {
                // If there's no valid user offset, use 1,2,3...
                label = String(idx);
              } else {
                // If user provided offset => (parsedOffset + 1).1, (parsedOffset + 1).2, etc.
                label = `${parsedOffset + 1}.${idx}`;
              }
            } else {
              // Nested => prefix + "." + idx
              label = `${prefix}.${idx}`;
            }

            const link = li.querySelector(":scope > a");
            if (link) {
              link.textContent = label + " " + link.textContent;
            }

            const nested = li.querySelector(":scope > ul");
            if (nested) {
              numberList(nested, label, 1);
            }

            idx++;
          });
        }

        numberList(rootUl);
      });
    </script>
  </body>
</html>
