<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      PyTorch Neural Network Classification | Jue Guo
    
  
</title>
<meta name="author" content="Jue Guo">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/personal/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/personal/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/personal/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/personal/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/personal/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/personal/assets/courses/basicai/03_pytorch_classification/">

<!-- Dark Mode -->
<script src="/personal/assets/js/theme.js?5ce9accf63efc46cd59e47ccb47fd172"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->





  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/personal//">
          
            
              <span class="font-weight-bold">Jue</span>
            
            
            Guo
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/personal/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/teaching/">courses
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>

    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- main content area -->
            <div class="col-sm-9">


  <div class="fixed-bottom-button">
    <a href="/personal/teaching/aibasic" class="btn-back-course">
      ‚Üê AI Basics
    </a>
  </div>


<!-- Always apply separation lines (scoped to article) -->

  <style type="text/css">
    /* Heading 1 */
    .post > article h1 {
      border-bottom: 4px solid #4CAF50; /* Green separation line */
      padding-bottom: 12px;
      margin-top: 40px; /* Increased spacing above */
      margin-bottom: 32px; /* Increased spacing below */
    }

    /* Heading 2 */
    .post > article h2 {
      border-bottom: 3px solid #2196F3; /* Blue separation line */
      padding-bottom: 10px;
      margin-top: 36px; /* Increased spacing above */
      margin-bottom: 28px; /* Increased spacing below */
    }

    /* Heading 3 */
    .post > article h3 {
      border-bottom: 3px dashed #FFC107; /* Yellow dashed line */
      padding-bottom: 8px;
      margin-top: 32px; /* Increased spacing above */
      margin-bottom: 24px; /* Increased spacing below */
    }

    /* Heading 4 */
    .post > article h4 {
      border-bottom: 2px dotted #9C27B0; /* Purple dotted line */
      padding-bottom: 6px;
      margin-top: 28px; /* Increased spacing above */
      margin-bottom: 20px; /* Increased spacing below */
    }

    /* Heading 5 */
    .post > article h5 {
      border-bottom: 2px solid #FF5722; /* Orange solid line */
      padding-bottom: 4px;
      margin-top: 24px; /* Increased spacing above */
      margin-bottom: 16px; /* Increased spacing below */
    }
  </style>



  <!-- Numbered Headings CSS -->
  <style type="text/css">
    /* Base reset for counters */
    body {
      counter-reset: h1-counter 2;
    }

    /* Heading 1 */
    .post > article h1 {
      counter-increment: h1-counter;
      counter-reset: h2-counter;
      
      /* Show h1 number */
      
    }

    
    .post > article h1::before {
      content: counter(h1-counter) ". ";
    }
    

    /* Heading 2 */
    .post > article h2 {
      counter-increment: h2-counter;
      counter-reset: h3-counter;
    }

    .post > article h2::before {
      content: counter(h1-counter) "." counter(h2-counter) " ";
    }

    /* Heading 3 */
    .post > article h3 {
      counter-increment: h3-counter;
      counter-reset: h4-counter;
    }

    .post > article h3::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) " ";
    }

    /* Heading 4 */
    .post > article h4 {
      counter-increment: h4-counter;
      counter-reset: h5-counter;
    }

    .post > article h4::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) " ";
    }

    /* Heading 5 */
    .post > article h5 {
      counter-increment: h5-counter;
    }

    .post > article h5::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) "." counter(h5-counter) " ";
    }
  </style>



<div class="post">
  <article>
    <header class="post-header">
      <h1 class="post-title">PyTorch Neural Network Classification</h1>
      <p class="post-description"></p>
    </header>

    <p>A <a href="https://en.wikipedia.org/wiki/Statistical_classification" rel="external nofollow noopener" target="_blank">classification problem</a> involves predicting whether something is one thing or another.</p>

<p>For example, you might want to:</p>

<table class="styled-table">
    <thead>
    <tr>
    <th>Problem type</th>
    <th>What is it?</th>
    <th>Example</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td><strong>Binary classification</strong></td>
    <td>Target can be one of two options, e.g. yes or no</td>
    <td>Predict whether or not someone has heart disease based on their health parameters.</td>
    </tr>
    <tr>
    <td><strong>Multi-class classification</strong></td>
    <td>Target can be one of more than two options</td>
    <td>Decide whether a photo is of food, a person or a dog.</td>
    </tr>
    <tr>
    <td><strong>Multi-label classification</strong></td>
    <td>Target can be assigned more than one option</td>
    <td>Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science &amp; philosophy).</td>
    </tr>
    </tbody>
</table>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0 text-center">
        <figure id="fig_different_classification_problems1">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-different-classification-problems.png" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 1: Various different classification problems in machine learning such as binary, multiclass, and multilabel classification.
    </figcaption>
  
</figure>

    </div>
</div>

<p>Classification, along with regression (predicting a number, covered in <a href="/personal/assets/courses/basicai/02_pytorch_workflow">pytorch workflow</a> is one of the most common types of machine learning problems.</p>

<p>In this session, we‚Äôre going to work through a couple of different classification problems with PyTorch.</p>

<p>In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p>

<h2 id="what-were-going-to-cover">What we‚Äôre going to cover</h2>

<p>In this notebook we‚Äôre going to reiterate over the PyTorch workflow we covered in <a href="/personal/assets/courses/basicai/02_pytorch_workflow">pytorch workflow</a>.</p>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_pytorch_workflow2">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 2: A PyTorch workflow flowchart.
    </figcaption>
  
</figure>

    </div>
</div>

<p>Except instead of trying to predict a straight line (predicting a number, also called a regression problem), we‚Äôll be working on a <strong>classification problem</strong>.</p>

<p>Specifically, we‚Äôre going to cover:</p>

<table class="styled-table">
<thead>
<tr>
<th><strong>Topic</strong></th>
<th><strong>Contents</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>0. Architecture of a classification neural network</strong></td>
<td>Neural networks can come in almost any shape or size, but they typically follow a similar floor plan.</td>
</tr>
<tr>
<td><strong>1. Getting binary classification data ready</strong></td>
<td>Data can be almost anything but to get started we're going to create a simple binary classification dataset.</td>
</tr>
<tr>
<td><strong>2. Building a PyTorch classification model</strong></td>
<td>Here we'll create a model to learn patterns in the data, we'll also choose a <strong>loss function</strong>, <strong>optimizer</strong> and build a <strong>training loop</strong> specific to classification.</td>
</tr>
<tr>
<td><strong>3. Fitting the model to data (training)</strong></td>
<td>We've got data and a model, now let's let the model (try to) find patterns in the (<strong>training</strong>) data.</td>
</tr>
<tr>
<td><strong>4. Making predictions and evaluating a model (inference)</strong></td>
<td>Our model's found patterns in the data, let's compare its findings to the actual (<strong>testing</strong>) data.</td>
</tr>
<tr>
<td><strong>5. Improving a model (from a model perspective)</strong></td>
<td>We've trained and evaluated a model but it's not working, let's try a few things to improve it.</td>
</tr>
<tr>
<td><strong>6. Non-linearity</strong></td>
<td>So far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines?</td>
</tr>
<tr>
<td><strong>7. Replicating non-linear functions</strong></td>
<td>We used <strong>non-linear functions</strong> to help model non-linear data, but what do these look like?</td>
</tr>
<tr>
<td><strong>8. Putting it all together with multi-class classification</strong></td>
<td>Let's put everything we've done so far for binary classification together with a multi-class classification problem.</td>
</tr>
</tbody>
</table>

<h2 id="architecture-of-a-classification-neural-network">Architecture of a classification neural network</h2>

<p>Before we get into writing code, let‚Äôs look at the general architecture of a classification neural network.</p>

<table class="styled-table">
    <thead>
    <tr>
    <th><strong>Hyperparameter</strong></th>
    <th><strong>Binary Classification</strong></th>
    <th><strong>Multiclass classification</strong></th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>
<strong>Input layer shape</strong> (<code>in_features</code>)</td>
    <td>Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction)</td>
    <td>Same as binary classification</td>
    </tr>
    <tr>
    <td><strong>Hidden layer(s)</strong></td>
    <td>Problem specific, minimum = 1, maximum = unlimited</td>
    <td>Same as binary classification</td>
    </tr>
    <tr>
    <td><strong>Neurons per hidden layer</strong></td>
    <td>Problem specific, generally 10 to 512</td>
    <td>Same as binary classification</td>
    </tr>
    <tr>
    <td>
<strong>Output layer shape</strong> (<code>out_features</code>)</td>
    <td>1 (one class or the other)</td>
    <td>1 per class (e.g. 3 for food, person or dog photo)</td>
    </tr>
    <tr>
    <td><strong>Hidden layer activation</strong></td>
    <td>Usually <a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU" rel="external nofollow noopener" target="_blank">ReLU</a> (rectified linear unit) but <a href="https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions" rel="external nofollow noopener" target="_blank">can be many others</a>
</td>
    <td>Same as binary classification</td>
    </tr>
    <tr>
    <td><strong>Output activation</strong></td>
    <td>
<a href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="external nofollow noopener" target="_blank">Sigmoid</a> (<a href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html" rel="external nofollow noopener" target="_blank"><code>torch.sigmoid</code></a> in PyTorch)</td>
    <td>
<a href="https://en.wikipedia.org/wiki/Softmax_function" rel="external nofollow noopener" target="_blank">Softmax</a> (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html" rel="external nofollow noopener" target="_blank"><code>torch.softmax</code></a> in PyTorch)</td>
    </tr>
    <tr>
    <td><strong>Loss function</strong></td>
    <td>
<a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression" rel="external nofollow noopener" target="_blank">Binary crossentropy</a> (<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html" rel="external nofollow noopener" target="_blank"><code>torch.nn.BCELoss</code></a> in PyTorch)</td>
    <td>Cross entropy (<a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="external nofollow noopener" target="_blank"><code>torch.nn.CrossEntropyLoss</code></a> in PyTorch)</td>
    </tr>
    <tr>
    <td><strong>Optimizer</strong></td>
    <td>
<a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html" rel="external nofollow noopener" target="_blank">SGD</a> (stochastic gradient descent), <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html" rel="external nofollow noopener" target="_blank">Adam</a> (see <a href="https://pytorch.org/docs/stable/optim.html" rel="external nofollow noopener" target="_blank"><code>torch.optim</code></a> for more options)</td>
    <td>Same as binary classification</td>
    </tr>
    </tbody>
</table>

<p>Of course, this ingredient list of classification neural network components will vary depending on the problem you‚Äôre working on.</p>

<p>But it‚Äôs more than enough to get started.</p>

<p>We‚Äôre going to get hands-on with this setup throughout this notebook.</p>

<h2 id="make-classification-data-and-get-it-ready">Make classification data and get it ready</h2>

<p>Let‚Äôs begin by making some data.</p>

<p>We‚Äôll use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">make_circles()</code></a> method from Scikit-Learn to generate two circles with different coloured dots.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_circles</span>


<span class="c1"># Make 1000 samples 
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Create circles
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span>
<span class="o">&lt;</span><span class="n">div</span> <span class="n">class</span><span class="o">=</span><span class="sh">"</span><span class="s">bash-block</span><span class="sh">"</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="c1"># a little bit of noise to the dots
</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># keep random state so we get the same values&lt;/code&gt;&lt;/pre&gt;
</span><span class="o">&lt;/</span><span class="n">div</span><span class="o">&gt;</span>
</code></pre></div></div>

<p>Alright, now let‚Äôs view the first 5 <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">y</code> values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">First 5 X features:</span><span class="se">\n</span><span class="si">{</span><span class="n">X</span><span class="p">[</span><span class="si">:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">First 5 y labels:</span><span class="se">\n</span><span class="si">{</span><span class="n">y</span><span class="p">[</span><span class="si">:</span><span class="mi">5</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>First 5 X features:
[[ 0.75424625  0.23148074]
[-0.75615888  0.15325888]
[-0.81539193  0.17328203]
[-0.39373073  0.69288277]
[ 0.44220765 -0.89672343]]

First 5 y labels:
[1 1 1 1 0]</code></pre>
</div>

<p>Looks like there‚Äôs two <code class="language-plaintext highlighter-rouge">X</code> values per one <code class="language-plaintext highlighter-rouge">y</code> value.</p>

<p>Let‚Äôs keep following the data explorer‚Äôs motto of <em>visualize, visualize, visualize</em> and put them into a pandas DataFrame.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make DataFrame of circle data
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">circles</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">"</span><span class="s">X1</span><span class="sh">"</span><span class="p">:</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                        <span class="sh">"</span><span class="s">X2</span><span class="sh">"</span><span class="p">:</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="sh">"</span><span class="s">label</span><span class="sh">"</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>
<span class="n">circles</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<div>
  <table class="dataframe">
    <thead>
      <tr style="text-align: right;">
        <th></th>
        <th>X1</th>
        <th>X2</th>
        <th>label</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th>0</th>
        <td>0.754246</td>
        <td>0.231481</td>
        <td>1</td>
      </tr>
      <tr>
        <th>1</th>
        <td>-0.756159</td>
        <td>0.153259</td>
        <td>1</td>
      </tr>
      <tr>
        <th>2</th>
        <td>-0.815392</td>
        <td>0.173282</td>
        <td>1</td>
      </tr>
      <tr>
        <th>3</th>
        <td>-0.393731</td>
        <td>0.692883</td>
        <td>1</td>
      </tr>
      <tr>
        <th>4</th>
        <td>0.442208</td>
        <td>-0.896723</td>
        <td>0</td>
      </tr>
      <tr>
        <th>5</th>
        <td>-0.479646</td>
        <td>0.676435</td>
        <td>1</td>
      </tr>
      <tr>
        <th>6</th>
        <td>-0.013648</td>
        <td>0.803349</td>
        <td>1</td>
      </tr>
      <tr>
        <th>7</th>
        <td>0.771513</td>
        <td>0.147760</td>
        <td>1</td>
      </tr>
      <tr>
        <th>8</th>
        <td>-0.169322</td>
        <td>-0.793456</td>
        <td>1</td>
      </tr>
      <tr>
        <th>9</th>
        <td>-0.121486</td>
        <td>1.021509</td>
        <td>0</td>
      </tr>
    </tbody>
  </table>
</div>

<p>It looks like each pair of <code class="language-plaintext highlighter-rouge">X</code> features (<code class="language-plaintext highlighter-rouge">X1</code> and <code class="language-plaintext highlighter-rouge">X2</code>) has a label (<code class="language-plaintext highlighter-rouge">y</code>) value of either 0 or 1.</p>

<p>This tells us that our problem is <strong>binary classification</strong> since there‚Äôs only two options (0 or 1).</p>

<p>How many values of each class are there?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check different labels
</span><span class="n">circles</span><span class="p">.</span><span class="n">label</span><span class="p">.</span><span class="nf">value_counts</span><span class="p">()</span>
</code></pre></div></div>
<div class="bash-block">
<pre><code>1    500
0    500
Name: label, dtype: int64</code></pre>
</div>

<p>500 each, nice and balanced.</p>

<p>Let‚Äôs plot them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualize with a plot
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> 
<span class="o">&lt;</span><span class="n">div</span> <span class="n">class</span><span class="o">=</span><span class="sh">"</span><span class="s">bash-block</span><span class="sh">"</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
<span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">RdYlBu</span><span class="p">);</span><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</code></pre></div></div>
<div style="text-align: left;">
  <img src="/personal/assets/img/02_pytorch_classification_files/02_pytorch_classification_14_0.png" alt="png" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<p>Alrighty, looks like we‚Äôve got a problem to solve.</p>

<p>Let‚Äôs find out how we could build a PyTorch neural network to classify dots into red (0) or blue (1).</p>

<div class="note-box">
<strong>Note:</strong> This dataset is often what's considered a **toy problem** (a problem that's used to try and test things out on) in machine learning.  But it represents the major key of classification, you have some kind of data represented as numerical values and you'd like to build a model that's able to classify it, in our case, separate it into red or blue dots.
</div>

<h3 id="input-and-output-shapes">Input and output shapes</h3>

<p>One of the most common errors in deep learning is shape errors.</p>

<p>Mismatching the shapes of tensors and tensor operations will result in errors in your models.</p>

<p>We‚Äôre going to see plenty of these throughout the course.</p>

<p>And there‚Äôs no surefire way to make sure they won‚Äôt happen, they will.</p>

<p>What you can do instead is continually familiarize yourself with the shape of the data you‚Äôre working with.</p>

<p>I like referring to it as input and output shapes.</p>

<p>Ask yourself:</p>

<p>‚ÄúWhat shapes are my inputs and what shapes are my outputs?‚Äù</p>

<p>Let‚Äôs find out.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check the shapes of our features and labels
</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>
<div class="bash-block">
<pre><code>((1000, 2), (1000,))</code></pre>
</div>

<p>Looks like we‚Äôve got a match on the first dimension of each.</p>

<p>There‚Äôs 1000 <code class="language-plaintext highlighter-rouge">X</code> and 1000 <code class="language-plaintext highlighter-rouge">y</code>.</p>

<p>But what‚Äôs the second dimension on <code class="language-plaintext highlighter-rouge">X</code>?</p>

<p>It often helps to view the values and shapes of a single sample (features and labels).</p>

<p>Doing so will help you understand what input and output shapes you‚Äôd be expecting from your model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># View the first example of features and labels
</span><span class="n">X_sample</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y_sample</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Values for one sample of X: </span><span class="si">{</span><span class="n">X_sample</span><span class="si">}</span><span class="s"> and the same for y: </span><span class="si">{</span><span class="n">y_sample</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Shapes for one sample of X: </span><span class="si">{</span><span class="n">X_sample</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> and the same for y: </span><span class="si">{</span><span class="n">y_sample</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1
Shapes for one sample of X: (2,) and the same for y: ()</code></pre>
</div>

<p>This tells us the second dimension for <code class="language-plaintext highlighter-rouge">X</code> means it has two features (vector) where as <code class="language-plaintext highlighter-rouge">y</code> has a single feature (scalar).</p>

<p>We have two inputs for one output.</p>

<h3 id="turn-data-into-tensors-and-create-train-and-test-splits">Turn data into tensors and create train and test splits</h3>

<p>We‚Äôve investigated the input and output shapes of our data, now let‚Äôs prepare it for being used with PyTorch and for modelling.</p>

<p>Specifically, we‚Äôll need to:</p>
<ol>
  <li>Turn our data into tensors (right now our data is in NumPy arrays and PyTorch prefers to work with PyTorch tensors).</li>
  <li>Split our data into training and test sets (we‚Äôll train a model on the training set to learn the patterns between <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">y</code> and then evaluate those learned patterns on the test dataset).</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Turn data into tensors
# Otherwise this causes issues with computations later on
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="c1"># View the first five samples
</span><span class="n">X</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>(tensor([[ 0.7542,  0.2315],
[-0.7562,  0.1533],
[-0.8154,  0.1733],
[-0.3937,  0.6929],
[ 0.4422, -0.8967]]),
tensor([1., 1., 1., 1., 0.]))</code></pre>
</div>

<p>Now our data is in tensor format, let‚Äôs split it into training and test sets.</p>

<p>To do so, let‚Äôs use the helpful function <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">train_test_split()</code></a> from Scikit-Learn.</p>

<p>We‚Äôll use <code class="language-plaintext highlighter-rouge">test_size=0.2</code> (80% training, 20% testing) and because the split happens randomly across the data, let‚Äôs use <code class="language-plaintext highlighter-rouge">random_state=42</code> so the split is reproducible.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Split data into train and test sets
</span><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> 
<span class="o">&lt;</span><span class="n">div</span> <span class="n">class</span><span class="o">=</span><span class="sh">"</span><span class="s">bash-block</span><span class="sh">"</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">y</span><span class="p">,</span>
<span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="c1"># 20% test, 80% train
</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># make the random split reproducible&lt;/code&gt;&lt;/pre&gt;
</span><span class="o">&lt;/</span><span class="n">div</span><span class="o">&gt;</span>

<span class="nf">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>(800, 200, 800, 200)</code></pre>
</div>

<p>Nice! Looks like we‚Äôve now got 800 training samples and 200 testing samples.</p>

<h2 id="building-a-model">Building a model</h2>

<p>We‚Äôve got some data ready, now it‚Äôs time to build a model.</p>

<p>We‚Äôll break it down into a few parts.</p>

<ol>
  <li>Setting up device agnostic code (so our model can run on CPU or GPU if it‚Äôs available).</li>
  <li>Constructing a model by subclassing <code class="language-plaintext highlighter-rouge">nn.Module</code>.</li>
  <li>Defining a loss function and optimizer.</li>
  <li>Creating a training loop (this‚Äôll be in the next section).</li>
</ol>

<p>The good news is we‚Äôve been through all of the above steps before in notebook 01.</p>

<p>Except now we‚Äôll be adjusting them so they work with a classification dataset.</p>

<p>Let‚Äôs start by importing PyTorch and <code class="language-plaintext highlighter-rouge">torch.nn</code> as well as setting up device agnostic code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Standard PyTorch imports
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># Make device agnostic code
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="n">device</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>'cuda'</code></pre>
</div>

<p>Excellent, now <code class="language-plaintext highlighter-rouge">device</code> is setup, we can use it for any data or models we create and PyTorch will handle it on the CPU (default) or GPU if it‚Äôs available.</p>

<p>How about we create a model?</p>

<p>We‚Äôll want a model capable of handling our <code class="language-plaintext highlighter-rouge">X</code> data as inputs and producing something in the shape of our <code class="language-plaintext highlighter-rouge">y</code> data as outputs.</p>

<p>In other words, given <code class="language-plaintext highlighter-rouge">X</code> (features) we want our model to predict <code class="language-plaintext highlighter-rouge">y</code> (label).</p>

<p>This setup where you have features and labels is referred to as <strong>supervised learning</strong>. Because your data is telling your model what the outputs should be given a certain input.</p>

<p>To create such a model it‚Äôll need to handle the input and output shapes of <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">y</code>.</p>

<p>Remember how I said input and output shapes are important? Here we‚Äôll see why.</p>

<p>Let‚Äôs create a model class that:</p>
<ol>
  <li>Subclasses <code class="language-plaintext highlighter-rouge">nn.Module</code> (almost all PyTorch models are subclasses of <code class="language-plaintext highlighter-rouge">nn.Module</code>).</li>
  <li>Creates 2 <code class="language-plaintext highlighter-rouge">nn.Linear</code> layers in the constructor capable of handling the input and output shapes of <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">y</code>.</li>
  <li>Defines a <code class="language-plaintext highlighter-rouge">forward()</code> method containing the forward pass computation of the model.</li>
  <li>Instantiates the model class and sends it to the target <code class="language-plaintext highlighter-rouge">device</code>.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. Construct a model class that subclasses nn.Module
</span><span class="k">class</span> <span class="nc">CircleModelV0</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># takes in 2 features (X), produces 5 features
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># takes in 5 features, produces 1 feature (y)
</span>    
    <span class="c1"># 3. Define a forward method containing the forward pass computation
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Return the output of layer_2, a single feature, the same shape as y
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># computation goes through layer_1 first then the output of layer_1 goes through layer_2
</span>
<span class="c1"># 4. Create an instance of the model and send it to target device
</span><span class="n">model_0</span> <span class="o">=</span> <span class="nc">CircleModelV0</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_0</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>CircleModelV0(
(layer_1): Linear(in_features=2, out_features=5, bias=True)
(layer_2): Linear(in_features=5, out_features=1, bias=True)
)</code></pre>
</div>

<p>What‚Äôs going on here?</p>

<p>We‚Äôve seen a few of these steps before.</p>

<p>The only major change is what‚Äôs happening between <code class="language-plaintext highlighter-rouge">self.layer_1</code> and <code class="language-plaintext highlighter-rouge">self.layer_2</code>.</p>

<p><code class="language-plaintext highlighter-rouge">self.layer_1</code> takes 2 input features <code class="language-plaintext highlighter-rouge">in_features=2</code> and produces 5 output features <code class="language-plaintext highlighter-rouge">out_features=5</code>.</p>

<p>This is known as having 5 <strong>hidden units</strong> or <strong>neurons</strong>.</p>

<p>This layer turns the input data from having 2 features to 5 features.</p>

<p>Why do this?</p>

<p>This allows the model to learn patterns from 5 numbers rather than just 2 numbers, <em>potentially</em> leading to better outputs.</p>

<p>I say potentially because sometimes it doesn‚Äôt work.</p>

<p>The number of hidden units you can use in neural network layers is a <strong>hyperparameter</strong> (a value you can set yourself) and there‚Äôs no set in stone value you have to use.</p>

<p>Generally more is better but there‚Äôs also such a thing as too much. The amount you choose will depend on your model type and dataset you‚Äôre working with.</p>

<p>Since our dataset is small and simple, we‚Äôll keep it small.</p>

<p>The only rule with hidden units is that the next layer, in our case, <code class="language-plaintext highlighter-rouge">self.layer_2</code> has to take the same <code class="language-plaintext highlighter-rouge">in_features</code> as the previous layer <code class="language-plaintext highlighter-rouge">out_features</code>.</p>

<p>That‚Äôs why <code class="language-plaintext highlighter-rouge">self.layer_2</code> has <code class="language-plaintext highlighter-rouge">in_features=5</code>, it takes the <code class="language-plaintext highlighter-rouge">out_features=5</code> from <code class="language-plaintext highlighter-rouge">self.layer_1</code> and performs a linear computation on them, turning them into <code class="language-plaintext highlighter-rouge">out_features=1</code> (the same shape as <code class="language-plaintext highlighter-rouge">y</code>).</p>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_a_visual_example_of_what_a_classification_neural_network_with_linear_activation_looks_like_on_the_tensorflow_playground3">
  <picture>
    <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-tensorflow-playground-linear-activation.png" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 3: A visual example of what a classification neural network with linear activation looks like on the tensorflow playground
    </figcaption>
  
</figure>

    </div>
</div>

<p><em>A visual example of what a similar classification neural network to the one we‚Äôve just built looks like. Try creating one of your own on the <a href="https://playground.tensorflow.org/" rel="external nofollow noopener" target="_blank">TensorFlow Playground website</a>.</em></p>

<p>You can also do the same as above using <a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.Sequential</code></a>.</p>

<p><code class="language-plaintext highlighter-rouge">nn.Sequential</code> performs a forward pass computation of the input data through the layers in the order they appear.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Replicate CircleModelV0 with nn.Sequential
</span><span class="n">model_0</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">model_0</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>Sequential(
(0): Linear(in_features=2, out_features=5, bias=True)
(1): Linear(in_features=5, out_features=1, bias=True)
)</code></pre>
</div>

<p>Woah, that looks much simpler than subclassing <code class="language-plaintext highlighter-rouge">nn.Module</code>, why not just always use <code class="language-plaintext highlighter-rouge">nn.Sequential</code>?</p>

<p><code class="language-plaintext highlighter-rouge">nn.Sequential</code> is fantastic for straight-forward computations, however, as the namespace says, it <em>always</em> runs in sequential order.</p>

<p>So if you‚Äôd like something else to happen (rather than just straight-forward sequential computation) you‚Äôll want to define your own custom <code class="language-plaintext highlighter-rouge">nn.Module</code> subclass.</p>

<p>Now we‚Äôve got a model, let‚Äôs see what happens when we pass some data through it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Make predictions with the model
</span><span class="n">untrained_preds</span> <span class="o">=</span> <span class="nf">model_0</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Length of predictions: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">untrained_preds</span><span class="p">)</span><span class="si">}</span><span class="s">, Shape: </span><span class="si">{</span><span class="n">untrained_preds</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Length of test samples: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s">, Shape: </span><span class="si">{</span><span class="n">y_test</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">First 10 predictions:</span><span class="se">\n</span><span class="si">{</span><span class="n">untrained_preds</span><span class="p">[</span><span class="si">:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">First 10 test labels:</span><span class="se">\n</span><span class="si">{</span><span class="n">y_test</span><span class="p">[</span><span class="si">:</span><span class="mi">10</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="bash-block">
  <pre><code>tensor([[-0.4279],
        [-0.3417],
        [-0.5975],
        [-0.3801],
        [-0.5078],
        [-0.4559],
        [-0.2842],
        [-0.3107],
        [-0.6010],
        [-0.3350]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>

<p>Hmm, it seems there are the same amount of predictions as there are test labels but the predictions don‚Äôt look like they‚Äôre in the same form or shape as the test labels.</p>

<p>We‚Äôve got a couple steps we can do to fix this, we‚Äôll see these later on.</p>

<h3 id="setup-loss-function-and-optimizer">Setup loss function and optimizer</h3>

<p>We‚Äôve setup a loss (also called a criterion or cost function) and optimizer before in <a href="https://www.learnpytorch.io/01_pytorch_workflow/#creating-a-loss-function-and-optimizer-in-pytorch" rel="external nofollow noopener" target="_blank">notebook 01</a>.</p>

<p>But different problem types require different loss functions.</p>

<p>For example, for a regression problem (predicting a number) you might use mean absolute error (MAE) loss.</p>

<p>And for a binary classification problem (like ours), you‚Äôll often use <a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" rel="external nofollow noopener" target="_blank">binary cross entropy</a> as the loss function.</p>

<p>However, the same optimizer function can often be used across different problem spaces.</p>

<p>For example, the stochastic gradient descent optimizer (SGD, <code class="language-plaintext highlighter-rouge">torch.optim.SGD()</code>) can be used for a range of problems, and the same applies to the Adam optimizer (<code class="language-plaintext highlighter-rouge">torch.optim.Adam()</code>).</p>

<table class="styled-table">
<thead>
<tr>
<th>Loss function/Optimizer</th>
<th>Problem type</th>
<th>PyTorch Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stochastic Gradient Descent (SGD) optimizer</td>
<td>Classification, regression, many others.</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html" rel="external nofollow noopener" target="_blank"><code>torch.optim.SGD()</code></a></td>
</tr>
<tr>
<td>Adam Optimizer</td>
<td>Classification, regression, many others.</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html" rel="external nofollow noopener" target="_blank"><code>torch.optim.Adam()</code></a></td>
</tr>
<tr>
<td>Binary cross entropy loss</td>
<td>Binary classification</td>
<td>
<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html" rel="external nofollow noopener" target="_blank"><code>torch.nn.BCELossWithLogits</code></a> or <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html" rel="external nofollow noopener" target="_blank"><code>torch.nn.BCELoss</code></a>
</td>
</tr>
<tr>
<td>Cross entropy loss</td>
<td>Multi-class classification</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="external nofollow noopener" target="_blank"><code>torch.nn.CrossEntropyLoss</code></a></td>
</tr>
<tr>
<td>Mean absolute error (MAE) or L1 Loss</td>
<td>Regression</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html" rel="external nofollow noopener" target="_blank"><code>torch.nn.L1Loss</code></a></td>
</tr>
<tr>
<td>Mean squared error (MSE) or L2 Loss</td>
<td>Regression</td>
<td><a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss" rel="external nofollow noopener" target="_blank"><code>torch.nn.MSELoss</code></a></td>
</tr>
</tbody>
</table>

<p><em>Table of various loss functions and optimizers, there are more but these are some common ones you‚Äôll see.</em></p>

<p>Since we‚Äôre working with a binary classification problem, let‚Äôs use a binary cross entropy loss function.</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    A <em>loss function</em> measures how <em>wrong</em> your model predictions are. 
    The higher the loss, the worse your model is performing.
  </p>
  <p>
    In the PyTorch documentation, loss functions are sometimes called 
    "<em>loss criterion</em>" or just "<em>criterion</em>". These terms all describe 
    the same concept.
  </p>
</div>

<p>PyTorch has two binary cross entropy implementations:</p>
<ol>
  <li>
<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.BCELoss()</code></a> - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).</li>
  <li>
<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.BCEWithLogitsLoss()</code></a> - This is the same as above except it has a sigmoid layer (<a href="https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nn.Sigmoid</code></a>) built-in (we‚Äôll see what this means soon).</li>
</ol>

<p>Which one should you use?</p>

<p>The <a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html" rel="external nofollow noopener" target="_blank">documentation for <code class="language-plaintext highlighter-rouge">torch.nn.BCEWithLogitsLoss()</code></a> states that it‚Äôs more numerically stable than using <code class="language-plaintext highlighter-rouge">torch.nn.BCELoss()</code> after a <code class="language-plaintext highlighter-rouge">nn.Sigmoid</code> layer.</p>

<p>So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of <code class="language-plaintext highlighter-rouge">nn.Sigmoid</code> and <code class="language-plaintext highlighter-rouge">torch.nn.BCELoss()</code> but that is beyond the scope of this notebook.</p>

<p>Knowing this, let‚Äôs create a loss function and an optimizer.</p>

<p>For the optimizer we‚Äôll use <code class="language-plaintext highlighter-rouge">torch.optim.SGD()</code> to optimize the model parameters with learning rate 0.1.</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    There's a 
    <a href="https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/4" rel="external nofollow noopener" target="_blank">
      discussion on the PyTorch forums
    </a> 
    about using <code>nn.BCELoss</code> vs. <code>nn.BCEWithLogitsLoss</code>. 
  </p>
  <p>
    It can be confusing at first, but‚Äîas with many things‚Äîpractice helps solidify 
    the differences and best usage scenarios.
  </p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a loss function
# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BCEWithLogitsLoss</span><span class="p">()</span> <span class="c1"># BCEWithLogitsLoss = sigmoid built-in
</span>
<span class="c1"># Create an optimizer
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model_0</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> 
                            <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>Now let‚Äôs also create an <strong>evaluation metric</strong>.</p>

<p>An evaluation metric can be used to offer another perspective on how your model is going.</p>

<p>If a loss function measures how <em>wrong</em> your model is, I like to think of evaluation metrics as measuring how <em>right</em> it is.</p>

<p>Of course, you could argue both of these are doing the same thing but evaluation metrics offer a different perspective.</p>

<p>After all, when evaluating your models it‚Äôs good to look at things from multiple points of view.</p>

<p>There are several evaluation metrics that can be used for classification problems but let‚Äôs start out with <strong>accuracy</strong>.</p>

<p>Accuracy can be measured by dividing the total number of correct predictions over the total number of predictions.</p>

<p>For example, a model that makes 99 correct predictions out of 100 will have an accuracy of 99%.</p>

<p>Let‚Äôs write a function to do so.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate accuracy (a classification metric)
</span><span class="k">def</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span> <span class="c1"># torch.eq() calculates where two tensors are equal
</span>    <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span> 
    <span class="k">return</span> <span class="n">acc</span>
</code></pre></div></div>

<p>Excellent! We can now use this function whilst training our model to measure it‚Äôs performance alongside the loss.</p>

<h2 id="train-model">Train model</h2>

<p>Okay, now we‚Äôve got a loss function and optimizer ready to go, let‚Äôs train a model.</p>

<p>Do you remember the steps in a PyTorch training loop?</p>

<p>If not, here‚Äôs a reminder.</p>

<p>Steps in training:</p>

<details class="collapsible-callout">
  <summary>PyTorch training loop steps</summary>
  <ol>
    <li>
<strong>Forward pass</strong> - The model goes through all of the training data once, performing its 
      <code>forward()</code> function calculations (<code>model(x_train)</code>).
    </li>
    <li>
<strong>Calculate the loss</strong> - The model's outputs (predictions) are compared to the ground truth and 
      evaluated to see how wrong they are (<code>loss = loss_fn(y_pred, y_train)</code>).
    </li>
    <li>
<strong>Zero gradients</strong> - The optimizer's gradients are set to zero (they are accumulated by default) 
      so they can be recalculated for the specific training step (<code>optimizer.zero_grad()</code>).
    </li>
    <li>
<strong>Perform backpropagation on the loss</strong> - Computes the gradient of the loss with respect to every 
      model parameter to be updated (each parameter with <code>requires_grad=True</code>). This is known as 
      <strong>backpropagation</strong>, hence "backwards" (<code>loss.backward()</code>).
    </li>
    <li>
<strong>Step the optimizer (gradient descent)</strong> - Update the parameters with 
      <code>requires_grad=True</code> with respect to the loss gradients in order to improve them 
      (<code>optimizer.step()</code>).
    </li>
  </ol>
</details>

<h3 id="going-from-raw-model-outputs-to-predicted-labels-logits---prediction-probabilities---prediction-labels">Going from raw model outputs to predicted labels (logits -&gt; prediction probabilities -&gt; prediction labels)</h3>

<p>Before the training loop steps, let‚Äôs see what comes out of our model during the forward pass (the forward pass is defined by the <code class="language-plaintext highlighter-rouge">forward()</code> method).</p>

<p>To do so, let‚Äôs pass the model some data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># View the frist 5 outputs of the forward pass on the test data
</span><span class="n">y_logits</span> <span class="o">=</span> <span class="nf">model_0</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))[:</span><span class="mi">5</span><span class="p">]</span>
<span class="n">y_logits</span>
</code></pre></div></div>

<div class="bash-block">
  <pre><code>tensor([[-0.4279],
        [-0.3417],
        [-0.5975],
        [-0.3801],
        [-0.5078]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>

<p>Since our model hasn‚Äôt been trained, these outputs are basically random.</p>

<p>But <em>what</em> are they?</p>

<p>They‚Äôre the output of our <code class="language-plaintext highlighter-rouge">forward()</code> method.</p>

<p>Which implements two layers of <code class="language-plaintext highlighter-rouge">nn.Linear()</code> which internally calls the following equation:</p>

\[\mathbf{y} = x \cdot \mathbf{Weights}^T  + \mathbf{bias}\]

<p>The <em>raw outputs</em> (unmodified) of this equation ($\mathbf{y}$) and in turn, the raw outputs of our model are often referred to as <a href="https://datascience.stackexchange.com/a/31045" rel="external nofollow noopener" target="_blank"><strong>logits</strong></a>.</p>

<p>That‚Äôs what our model is outputing above when it takes in the input data ($x$ in the equation or <code class="language-plaintext highlighter-rouge">X_test</code> in the code), logits.</p>

<p>However, these numbers are hard to interpret.</p>

<p>We‚Äôd like some numbers that are comparable to our truth labels.</p>

<p>To get our model‚Äôs raw outputs (logits) into such a form, we can use the <a href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html" rel="external nofollow noopener" target="_blank">sigmoid activation function</a>.</p>

<p>Let‚Äôs try it out.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use sigmoid on model logits
</span><span class="n">y_pred_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">y_logits</span><span class="p">)</span>
<span class="n">y_pred_probs</span>
</code></pre></div></div>

<div class="bash-block">
  <pre><code>tensor([[0.3946],
        [0.4154],
        [0.3549],
        [0.4061],
        [0.3757]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>

<p>Okay, it seems like the outputs now have some kind of consistency (even though they‚Äôre still random).</p>

<p>They‚Äôre now in the form of <strong>prediction probabilities</strong> (I usually refer to these as <code class="language-plaintext highlighter-rouge">y_pred_probs</code>), in other words, the values are now how much the model thinks the data point belongs to one class or another.</p>

<p>In our case, since we‚Äôre dealing with binary classification, our ideal outputs are 0 or 1.</p>

<p>So these values can be viewed as a decision boundary.</p>

<p>The closer to 0, the more the model thinks the sample belongs to class 0, the closer to 1, the more the model thinks the sample belongs to class 1.</p>

<p>More specificially:</p>
<ul>
  <li>If <code class="language-plaintext highlighter-rouge">y_pred_probs</code> &gt;= 0.5, <code class="language-plaintext highlighter-rouge">y=1</code> (class 1)</li>
  <li>If <code class="language-plaintext highlighter-rouge">y_pred_probs</code> &lt; 0.5, <code class="language-plaintext highlighter-rouge">y=0</code> (class 0)</li>
</ul>

<p>To turn our prediction probabilities into prediction labels, we can round the outputs of the sigmoid activation function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Find the predicted labels (round the prediction probabilities)
</span><span class="n">y_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">y_pred_probs</span><span class="p">)</span>

<span class="c1"># In full
</span><span class="n">y_pred_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="nf">model_0</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))[:</span><span class="mi">5</span><span class="p">]))</span>

<span class="c1"># Check for equality
</span><span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">y_preds</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span> <span class="n">y_pred_labels</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">()))</span>

<span class="c1"># Get rid of extra dimension
</span><span class="n">y_preds</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">()</span>
</code></pre></div></div>

<div class="bash-block">
    <pre><code>tensor([True, True, True, True, True], device='cuda:0')</code></pre>
</div>

<div class="bash-block">
  <pre><code>tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=&lt;SqueezeBackward0&gt;)</code></pre>
</div>

<p>Excellent! Now it looks like our model‚Äôs predictions are in the same form as our truth labels (<code class="language-plaintext highlighter-rouge">y_test</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>tensor([1., 0., 1., 0., 1.])</code></pre>
</div>

<p>This means we‚Äôll be able to compare our model‚Äôs predictions to the test labels to see how well it‚Äôs performing.</p>

<p>To recap, we converted our model‚Äôs raw outputs (logits) to prediction probabilities using a sigmoid activation function.</p>

<p>And then converted the prediction probabilities to prediction labels by rounding them.</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    The use of the sigmoid activation function is often only for binary classification logits. For multi-class classification, we'll be looking at using the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html" rel="external nofollow noopener" target="_blank">softmax activation function</a> (this will come later on).
  </p>
  <p>
    Also, the sigmoid activation function is <em>not</em> required when passing our model's raw outputs to <code>nn.BCEWithLogitsLoss</code>. The "logits" in logits loss indicates it works on the model's raw logits output‚Äîbecause it has a sigmoid function built in.
  </p>
</div>

<h3 id="building-a-training-and-testing-loop">Building a training and testing loop</h3>

<p>Alright, we‚Äôve discussed how to take our raw model outputs and convert them to prediction labels, now let‚Äôs build a training loop.</p>

<p>Let‚Äôs start by training for 100 epochs and outputing the model‚Äôs progress every 10 epochs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Set the number of epochs
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Put data to target device
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_test</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Build training and evaluation loop
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1">### Training
</span>    <span class="n">model_0</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

    <span class="c1"># 1. Forward pass (model outputs raw logits)
</span>    <span class="n">y_logits</span> <span class="o">=</span> <span class="nf">model_0</span><span class="p">(</span><span class="n">X_train</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span> <span class="c1"># squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device 
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">y_logits</span><span class="p">))</span> <span class="c1"># turn logits -&gt; pred probs -&gt; pred labls
</span>  
    <span class="c1"># 2. Calculate loss/accuracy
</span>    <span class="c1"># loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()
</span>    <span class="c1">#                y_train) 
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="c1"># Using nn.BCEWithLogitsLoss works with raw logits
</span>                   <span class="n">y_train</span><span class="p">)</span> 
    <span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> 
                      <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span> 

    <span class="c1"># 3. Optimizer zero grad
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="c1"># 4. Loss backwards
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="c1"># 5. Optimizer step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1">### Testing
</span>    <span class="n">model_0</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
        <span class="c1"># 1. Forward pass
</span>        <span class="n">test_logits</span> <span class="o">=</span> <span class="nf">model_0</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span> 
        <span class="n">test_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">test_logits</span><span class="p">))</span>
        <span class="c1"># 2. Caculate loss/accuracy
</span>        <span class="n">test_loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">test_logits</span><span class="p">,</span>
                            <span class="n">y_test</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
                               <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="p">)</span>

    <span class="c1"># Print out what's happening every 10 epochs
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> | Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s">, Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">% | Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s">, Test acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>Epoch: 0 | Loss: 0.72090, Accuracy: 50.00% | Test loss: 0.72196, Test acc: 50.00%
Epoch: 10 | Loss: 0.70291, Accuracy: 50.00% | Test loss: 0.70542, Test acc: 50.00%
Epoch: 20 | Loss: 0.69659, Accuracy: 50.00% | Test loss: 0.69942, Test acc: 50.00%
Epoch: 30 | Loss: 0.69432, Accuracy: 43.25% | Test loss: 0.69714, Test acc: 41.00%
Epoch: 40 | Loss: 0.69349, Accuracy: 47.00% | Test loss: 0.69623, Test acc: 46.50%
Epoch: 50 | Loss: 0.69319, Accuracy: 49.00% | Test loss: 0.69583, Test acc: 46.00%
Epoch: 60 | Loss: 0.69308, Accuracy: 50.12% | Test loss: 0.69563, Test acc: 46.50%
Epoch: 70 | Loss: 0.69303, Accuracy: 50.38% | Test loss: 0.69551, Test acc: 46.00%
Epoch: 80 | Loss: 0.69302, Accuracy: 51.00% | Test loss: 0.69543, Test acc: 46.00%
Epoch: 90 | Loss: 0.69301, Accuracy: 51.00% | Test loss: 0.69537, Test acc: 46.00%</code></pre>
</div>

<h2 id="make-predictions-and-evaluate-the-model">Make predictions and evaluate the model</h2>

<p>From the metrics it looks like our model is random guessing.</p>

<p>How could we investigate this further?</p>

<p>I‚Äôve got an idea.</p>

<p>The data explorer‚Äôs motto!</p>

<p>‚ÄúVisualize, visualize, visualize!‚Äù</p>

<p>Let‚Äôs make a plot of our model‚Äôs predictions, the data it‚Äôs trying to predict on and the decision boundary it‚Äôs creating for whether something is class 0 or class 1.</p>

<p>To do so, we‚Äôll write some code to download and import the <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">helper_functions.py</code> script</a> from the <a href="https://github.com/mrdbourke/pytorch-deep-learning" rel="external nofollow noopener" target="_blank">Learn PyTorch for Deep Learning repo</a>.</p>

<p>It contains a helpful function called <code class="language-plaintext highlighter-rouge">plot_decision_boundary()</code> which creates a NumPy meshgrid to visually plot the different points where our model is predicting certain classes.</p>

<p>We‚Äôll also import <code class="language-plaintext highlighter-rouge">plot_predictions()</code> which we wrote in notebook 01 to use later.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>
<span class="kn">from</span> <span class="n">pathlib</span> <span class="kn">import</span> <span class="n">Path</span> 

<span class="c1"># Download helper functions from Learn PyTorch repo (if not already downloaded)
</span><span class="k">if</span> <span class="nc">Path</span><span class="p">(</span><span class="sh">"</span><span class="s">helper_functions.py</span><span class="sh">"</span><span class="p">).</span><span class="nf">is_file</span><span class="p">():</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">helper_functions.py already exists, skipping download</span><span class="sh">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Downloading helper_functions.py</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">request</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py</span><span class="sh">"</span><span class="p">)</span>
  <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">helper_functions.py</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">request</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">helper_functions</span> <span class="kn">import</span> <span class="n">plot_predictions</span><span class="p">,</span> <span class="n">plot_decision_boundary</span>
</code></pre></div></div>

<div class="bash-block">
<pre><code>helper_functions.py already exists, skipping download</code></pre>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot decision boundaries for training and test sets
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Train</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model_0</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Test</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model_0</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div style="text-align: left;">
  <img src="/personal/assets/img/02_pytorch_classification_files/02_pytorch_classification_55_0.png" alt="png" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<p>Oh wow, it seems like we‚Äôve found the cause of model‚Äôs performance issue.</p>

<p>It‚Äôs currently trying to split the red and blue dots using a straight line‚Ä¶</p>

<p>That explains the 50% accuracy. Since our data is circular, drawing a straight line can at best cut it down the middle.</p>

<p>In machine learning terms, our model is <strong>underfitting</strong>, meaning it‚Äôs not learning predictive patterns from the data.</p>

<p>How could we improve this?</p>

<h2 id="improving-a-model-from-a-model-perspective">Improving a model (from a model perspective)</h2>

<p>Let‚Äôs try to fix our model‚Äôs underfitting problem.</p>

<p>Focusing specifically on the model (not the data), there are a few ways we could do this.</p>

<table class="styled-table">
<thead>
<tr>
<th>Model improvement technique*</th>
<th>What does it do?</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Add more layers</strong></td>
<td>Each layer <em>potentially</em> increases the learning capabilities of the model with each layer being able to learn some kind of new pattern in the data. More layers are often referred to as making your neural network <em>deeper</em>.</td>
</tr>
<tr>
<td><strong>Add more hidden units</strong></td>
<td>Similar to the above, more hidden units per layer means a <em>potential</em> increase in learning capabilities of the model. More hidden units are often referred to as making your neural network <em>wider</em>.</td>
</tr>
<tr>
<td><strong>Fitting for longer (more epochs)</strong></td>
<td>Your model might learn more if it had more opportunities to look at the data.</td>
</tr>
<tr>
<td><strong>Changing the activation functions</strong></td>
<td>Some data just can't be fit with only straight lines (like what we've seen), using non-linear activation functions can help with this (hint, hint).</td>
</tr>
<tr>
<td><strong>Change the learning rate</strong></td>
<td>Less model specific, but still related, the learning rate of the optimizer decides how much a model should change its parameters each step, too much and the model overcorrects, too little and it doesn't learn enough.</td>
</tr>
<tr>
<td><strong>Change the loss function</strong></td>
<td>Again, less model specific but still important, different problems require different loss functions. For example, a binary cross entropy loss function won't work with a multi-class classification problem.</td>
</tr>
<tr>
<td><strong>Use transfer learning</strong></td>
<td>Take a pretrained model from a problem domain similar to yours and adjust it to your own problem.</td>
</tr>
</tbody>
</table>

<div class="note-box">
  <strong>Note:</strong> 
  <p>
    Because you can adjust all of these by hand, they're referred to as <strong>hyperparameters</strong>. And this is also where machine learning's half art, half science aspect comes in‚Äîthere's no real way to know upfront what the best combination of values is for your project. 
  </p>
  <p>
    Best to follow the data scientist's motto: <em>"experiment, experiment, experiment"</em>.
  </p>
</div>

<p>Let‚Äôs see what happens if we add an extra layer to our model, fit for longer (<code class="language-plaintext highlighter-rouge">epochs=1000</code> instead of <code class="language-plaintext highlighter-rouge">epochs=100</code>) and increase the number of hidden units from <code class="language-plaintext highlighter-rouge">5</code> to <code class="language-plaintext highlighter-rouge">10</code>.</p>

<p>We‚Äôll follow the same steps we did above but with a few changed hyperparameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CircleModelV1</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># extra layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">layer_3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="c1"># note: always make sure forward is spelt correctly!
</span>        <span class="c1"># Creating a model like this is the same as below, though below
</span>        <span class="c1"># generally benefits from speedups where possible.
</span>        <span class="c1"># z = self.layer_1(x)
</span>        <span class="c1"># z = self.layer_2(z)
</span>        <span class="c1"># z = self.layer_3(z)
</span>        <span class="c1"># return z
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer_3</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">layer_2</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="n">model_1</span> <span class="o">=</span> <span class="nc">CircleModelV1</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_1</span>
</code></pre></div></div>

<div class="bash-block">
  <pre><code>CircleModelV1(
  (layer_1): Linear(in_features=2, out_features=10, bias=True)
  (layer_2): Linear(in_features=10, out_features=10, bias=True)
  (layer_3): Linear(in_features=10, out_features=1, bias=True)
)</code></pre>
</div>

<p>Now we‚Äôve got a model, we‚Äôll recreate a loss function and optimizer instance, using the same settings as before.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># loss_fn = nn.BCELoss() # Requires sigmoid on input
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BCEWithLogitsLoss</span><span class="p">()</span> <span class="c1"># Does not require sigmoid on input
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model_1</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>Beautiful, model, optimizer and loss function ready, let‚Äôs make a training loop.</p>

<p>This time we‚Äôll train for longer (<code class="language-plaintext highlighter-rouge">epochs=1000</code> vs <code class="language-plaintext highlighter-rouge">epochs=100</code>) and see if it improves our model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># Train for longer
</span>
<span class="c1"># Put data to target device
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_test</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1">### Training
</span>    <span class="c1"># 1. Forward pass
</span>    <span class="n">y_logits</span> <span class="o">=</span> <span class="nf">model_1</span><span class="p">(</span><span class="n">X_train</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">y_logits</span><span class="p">))</span> <span class="c1"># logits -&gt; prediction probabilities -&gt; prediction labels
</span>
    <span class="c1"># 2. Calculate loss/accuracy
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> 
                      <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="c1"># 3. Optimizer zero grad
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="c1"># 4. Loss backwards
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="c1"># 5. Optimizer step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1">### Testing
</span>    <span class="n">model_1</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
        <span class="c1"># 1. Forward pass
</span>        <span class="n">test_logits</span> <span class="o">=</span> <span class="nf">model_1</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span> 
        <span class="n">test_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">test_logits</span><span class="p">))</span>
        <span class="c1"># 2. Caculate loss/accuracy
</span>        <span class="n">test_loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">test_logits</span><span class="p">,</span>
                            <span class="n">y_test</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="nf">accuracy_fn</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
                               <span class="n">y_pred</span><span class="o">=</span><span class="n">test_pred</span><span class="p">)</span>

    <span class="c1"># Print out what's happening every 10 epochs
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> | Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s">, Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">% | Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s">, Test acc: </span><span class="si">{</span><span class="n">test_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<div class="bash-block">
  <pre><code>Epoch: 0   | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%
Epoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%
Epoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%
Epoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%
Epoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%
Epoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%
Epoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%
Epoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%</code></pre>
</div>

<p>What? Our model trained for longer and with an extra layer but it still looks like it didn‚Äôt learn any patterns better than random guessing.</p>

<p>Let‚Äôs visualize.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot decision boundaries for training and test sets
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Train</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model_1</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Test</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model_1</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<div style="text-align: left;">
  <img src="/personal/assets/img/02_pytorch_classification_files/02_pytorch_classification_64_0.png" alt="A local PyTorch classification image" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<p>Hmmm.</p>

<p>Our model is still drawing a straight line between the red and blue dots.</p>

<p>If our model is drawing a straight line, could it model linear data?</p>

<h3 id="preparing-data-to-see-if-our-model-can-model-a-straight-line">Preparing data to see if our model can model a straight line</h3>
<p>Let‚Äôs create some linear data to see if our model‚Äôs able to model it and we‚Äôre not just using a model that can‚Äôt learn anything.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create some data (same as notebook 01)
</span><span class="n">weight</span> <span class="o">=</span> <span class="mf">0.7</span>
<span class="n">bias</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">end</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># Create data
</span><span class="n">X_regression</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_regression</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">X_regression</span> <span class="o">+</span> <span class="n">bias</span> <span class="c1"># linear regression formula
</span>
<span class="c1"># Check the data
</span><span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_regression</span><span class="p">))</span>
<span class="n">X_regression</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y_regression</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<div class="bash-block">
  <pre><code>100

(tensor([[0.0000],
         [0.0100],
         [0.0200],
         [0.0300],
         [0.0400]]),
 tensor([[0.3000],
         [0.3070],
         [0.3140],
         [0.3210],
         [0.3280]]))</code></pre>
</div>

<p>Wonderful, now let‚Äôs split our data into training and test sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create train and test splits
</span><span class="n">train_split</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">X_regression</span><span class="p">))</span> <span class="c1"># 80% of data used for training set
</span><span class="n">X_train_regression</span><span class="p">,</span> <span class="n">y_train_regression</span> <span class="o">=</span> <span class="n">X_regression</span><span class="p">[:</span><span class="n">train_split</span><span class="p">],</span> <span class="n">y_regression</span><span class="p">[:</span><span class="n">train_split</span><span class="p">]</span>
<span class="n">X_test_regression</span><span class="p">,</span> <span class="n">y_test_regression</span> <span class="o">=</span> <span class="n">X_regression</span><span class="p">[</span><span class="n">train_split</span><span class="p">:],</span> <span class="n">y_regression</span><span class="p">[</span><span class="n">train_split</span><span class="p">:]</span>

<span class="c1"># Check the lengths of each split
</span><span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_regression</span><span class="p">),</span> 
    <span class="nf">len</span><span class="p">(</span><span class="n">y_train_regression</span><span class="p">),</span> 
    <span class="nf">len</span><span class="p">(</span><span class="n">X_test_regression</span><span class="p">),</span> 
    <span class="nf">len</span><span class="p">(</span><span class="n">y_test_regression</span><span class="p">))</span>
</code></pre></div></div>

<div class="bash-block">
  <pre><code>80 80 20 20</code></pre>
</div>

<p>Beautiful, let‚Äôs see how the data looks.</p>

<p>To do so, we‚Äôll use the <code class="language-plaintext highlighter-rouge">plot_predictions()</code> function we created in notebook 01.</p>

<p>It‚Äôs contained within the <a href="https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">helper_functions.py</code> script</a> on the Learn PyTorch for Deep Learning repo which we downloaded above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">plot_predictions</span><span class="p">(</span><span class="n">train_data</span><span class="o">=</span><span class="n">X_train_regression</span><span class="p">,</span>
    <span class="n">train_labels</span><span class="o">=</span><span class="n">y_train_regression</span><span class="p">,</span>
    <span class="n">test_data</span><span class="o">=</span><span class="n">X_test_regression</span><span class="p">,</span>
    <span class="n">test_labels</span><span class="o">=</span><span class="n">y_test_regression</span>
<span class="p">);</span>
</code></pre></div></div>

<div style="text-align: left;">
  <img src="/personal/assets/img/02_pytorch_classification_files/02_pytorch_classification_71_0.png" alt="png" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<h3 id="adjusting-model_1-to-fit-a-straight-line">Adjusting <code class="language-plaintext highlighter-rouge">model_1</code> to fit a straight line</h3>

<p>Now we‚Äôve got some data, let‚Äôs recreate <code class="language-plaintext highlighter-rouge">model_1</code> but with a loss function suited to our regression data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Same architecture as model_1 (but using nn.Sequential)
</span><span class="n">model_2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">model_2</span>
</code></pre></div></div>

<div class="bash-block">
  <pre><code>Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): Linear(in_features=10, out_features=10, bias=True)
  (2): Linear(in_features=10, out_features=1, bias=True)
)</code></pre>
</div>

<p>We‚Äôll setup the loss function to be <code class="language-plaintext highlighter-rouge">nn.L1Loss()</code> (the same as mean absolute error) and the optimizer to be <code class="language-plaintext highlighter-rouge">torch.optim.SGD()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Loss and optimizer
</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">L1Loss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model_2</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>Now let‚Äôs train the model using the regular training loop steps for <code class="language-plaintext highlighter-rouge">epochs=1000</code> (just like <code class="language-plaintext highlighter-rouge">model_1</code>).</p>

<div class="note-box">
  <strong>Note:</strong>
  <p>
    We've been writing similar training loop code over and over again. 
    I've made it that way on purpose though, to keep practicing.
  </p>
  <p>
    However, do you have ideas how we could functionize this? 
    That would save a fair bit of coding in the future. 
    Potentially there could be a function for training and a function for testing.
  </p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train the model
</span><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Set the number of epochs
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Put data to target device
</span><span class="n">X_train_regression</span><span class="p">,</span> <span class="n">y_train_regression</span> <span class="o">=</span> <span class="n">X_train_regression</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_train_regression</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_test_regression</span><span class="p">,</span> <span class="n">y_test_regression</span> <span class="o">=</span> <span class="n">X_test_regression</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_test_regression</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1">### Training 
</span>    <span class="c1"># 1. Forward pass
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model_2</span><span class="p">(</span><span class="n">X_train_regression</span><span class="p">)</span>
    
    <span class="c1"># 2. Calculate loss (no accuracy since it's a regression problem, not classification)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_train_regression</span><span class="p">)</span>

    <span class="c1"># 3. Optimizer zero grad
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

    <span class="c1"># 4. Loss backwards
</span>    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="c1"># 5. Optimizer step
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="c1">### Testing
</span>    <span class="n">model_2</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
      <span class="c1"># 1. Forward pass
</span>      <span class="n">test_pred</span> <span class="o">=</span> <span class="nf">model_2</span><span class="p">(</span><span class="n">X_test_regression</span><span class="p">)</span>
      <span class="c1"># 2. Calculate the loss 
</span>      <span class="n">test_loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">test_pred</span><span class="p">,</span> <span class="n">y_test_regression</span><span class="p">)</span>

    <span class="c1"># Print out what's happening
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> | Train loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s">, Test loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="bash-block">
  <pre><code>
Epoch: 0   | Train loss: 0.75986, Test loss: 0.54143
Epoch: 100 | Train loss: 0.09309, Test loss: 0.02901
Epoch: 200 | Train loss: 0.07376, Test loss: 0.02850
Epoch: 300 | Train loss: 0.06745, Test loss: 0.00615
Epoch: 400 | Train loss: 0.06107, Test loss: 0.02004
Epoch: 500 | Train loss: 0.05698, Test loss: 0.01061
Epoch: 600 | Train loss: 0.04857, Test loss: 0.01326
Epoch: 700 | Train loss: 0.06109, Test loss: 0.02127
Epoch: 800 | Train loss: 0.05599, Test loss: 0.01426
Epoch: 900 | Train loss: 0.05571, Test loss: 0.00603</code></pre>
</div>

<p>Okay, unlike <code class="language-plaintext highlighter-rouge">model_1</code> on the classification data, it looks like <code class="language-plaintext highlighter-rouge">model_2</code>‚Äôs loss is actually going down.</p>

<p>Let‚Äôs plot its predictions to see if that‚Äôs so.</p>

<p>And remember, since our model and data are using the target <code class="language-plaintext highlighter-rouge">device</code>, and this device may be a GPU, however, our plotting function uses matplotlib and matplotlib can‚Äôt handle data on the GPU.</p>

<p>To handle that, we‚Äôll send all of our data to the CPU using <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">.cpu()</code></a> when we pass it to <code class="language-plaintext highlighter-rouge">plot_predictions()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Turn on evaluation mode
</span><span class="n">model_2</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># Make predictions (inference)
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
    <span class="n">y_preds</span> <span class="o">=</span> <span class="nf">model_2</span><span class="p">(</span><span class="n">X_test_regression</span><span class="p">)</span>

<span class="c1"># Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)
# (try removing .cpu() from one of the below and see what happens)
</span><span class="nf">plot_predictions</span><span class="p">(</span><span class="n">train_data</span><span class="o">=</span><span class="n">X_train_regression</span><span class="p">.</span><span class="nf">cpu</span><span class="p">(),</span>
                 <span class="n">train_labels</span><span class="o">=</span><span class="n">y_train_regression</span><span class="p">.</span><span class="nf">cpu</span><span class="p">(),</span>
                 <span class="n">test_data</span><span class="o">=</span><span class="n">X_test_regression</span><span class="p">.</span><span class="nf">cpu</span><span class="p">(),</span>
                 <span class="n">test_labels</span><span class="o">=</span><span class="n">y_test_regression</span><span class="p">.</span><span class="nf">cpu</span><span class="p">(),</span>
                 <span class="n">predictions</span><span class="o">=</span><span class="n">y_preds</span><span class="p">.</span><span class="nf">cpu</span><span class="p">());</span>
</code></pre></div></div>
<div style="text-align: left;">
  <img src="/personal/assets/img/02_pytorch_classification_files/02_pytorch_classification_79_0.png" alt="png" class="img-fluid" style="max-width: 80%; height: auto; display: block; margin-bottom: 1rem;">
</div>

<p>Alright, it looks like our model is able to do far better than random guessing on straight lines.</p>

<p>This is a good thing.</p>

<p>It means our model at least has <em>some</em> capacity to learn.</p>



    

    
  </article>
</div>
</div>
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      ¬© Copyright 2024
      Jue
      
      Guo. 
      
      
        Last updated: December 27, 2024.
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/personal/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/personal/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/personal/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/personal/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/personal/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/personal/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/personal/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  

    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/personal/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
