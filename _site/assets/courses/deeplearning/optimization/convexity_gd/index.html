<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Convexity and Gradient Descent | Jue Guo
    
  
</title>
<meta name="author" content="Jue Guo">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/personal/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/personal/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/personal/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/personal/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/personal/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/personal/assets/courses/deeplearning/optimization/convexity_gd/">

<!-- Dark Mode -->
<script src="/personal/assets/js/theme.js?5ce9accf63efc46cd59e47ccb47fd172"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->





  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/personal//">
          
            
              <span class="font-weight-bold">Jue</span>
            
            
            Guo
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/personal/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/teaching/">courses
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>

    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">


  <div class="fixed-bottom-button">
    <a href="/personal/teaching/deeplearnig" class="btn-back-course">
      ← Deep Learning Course Page
    </a>
  </div>


<!-- Always apply separation lines (scoped to article) -->

  <style type="text/css">
    /* Heading 1 */
    .post > article h1 {
      border-bottom: 4px solid #4CAF50; /* Green separation line */
      padding-bottom: 12px;
      margin-top: 40px; /* Increased spacing above */
      margin-bottom: 32px; /* Increased spacing below */
    }

    /* Heading 2 */
    .post > article h2 {
      border-bottom: 3px solid #2196F3; /* Blue separation line */
      padding-bottom: 10px;
      margin-top: 36px; /* Increased spacing above */
      margin-bottom: 28px; /* Increased spacing below */
    }

    /* Heading 3 */
    .post > article h3 {
      border-bottom: 3px dashed #FFC107; /* Yellow dashed line */
      padding-bottom: 8px;
      margin-top: 32px; /* Increased spacing above */
      margin-bottom: 24px; /* Increased spacing below */
    }

    /* Heading 4 */
    .post > article h4 {
      border-bottom: 2px dotted #9C27B0; /* Purple dotted line */
      padding-bottom: 6px;
      margin-top: 28px; /* Increased spacing above */
      margin-bottom: 20px; /* Increased spacing below */
    }

    /* Heading 5 */
    .post > article h5 {
      border-bottom: 2px solid #FF5722; /* Orange solid line */
      padding-bottom: 4px;
      margin-top: 24px; /* Increased spacing above */
      margin-bottom: 16px; /* Increased spacing below */
    }
  </style>





<div class="post">
  <article>
    <header class="post-header">
      <h1 class="post-title">Convexity and Gradient Descent</h1>
      <p class="post-description"></p>
    </header>

    <h2 id="convexity">Convexity</h2>
<p>Convexity plays a vital role in the design of optimization algorithms. This is largely due to the fact that it is much easier to analyze and test algorithms in such a context.</p>
<ul>
  <li>In other words, if the algorithm performs poorly even in the convex setting, typically we should not hope to see great results otherwise. Furthermore, even though the optimization problems in deep learning are generally nonconvex, they often exhibit some properties of convex ones near local minima.</li>
</ul>

<h3 id="definition">Definition</h3>

<p>Before convex analysis, we need to define <em>convex sets</em> and <em>convex functions</em>. They lead to mathematical tools that are commonly applied to machine learning.</p>

<h4 id="convex-sets">Convex Sets</h4>
<p>Sets are the basis of convexity. Simply put, a set \(\mathcal{X}\) in a vector space is convex if for any \(a, b \in \mathcal{X}\) the line segment connecting \(a\) and \(b\) is also in \(\mathcal{X}\). In mathematical terms this means that for all \(\lambda \in[0,1]\) we have</p>

\[\lambda a+(1-\lambda) b \in \mathcal{X} \text { whenever } a, b \in \mathcal{X}\]

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure id="figure-">
  <picture>
    <img src="https://d2l.ai/_images/pacman.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
</figure>

    </div>
</div>
<div class="caption">
Fig. 1. The first set is nonconvex and the other two are convex.
</div>

<p>Something useful:</p>
<ul>
  <li>Assume that \(\mathcal{X}\) and \(\mathcal{Y}\) are convex sets. Then \(\mathcal{X} \cap \mathcal{Y}\) is also convex.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure id="figure-">
  <picture>
    <img src="https://d2l.ai/_images/convex-intersect.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
</figure>

    </div>
</div>
<div class="caption">
Fig. 2. The intersection between two convex sets is convex.
</div>

<ul>
  <li>We can strengthen this result with little effort: given convex sets \(\mathcal{X}_{i}\), their intersection \(\cap_{i} \mathcal{X}_{i}\) is
convex.
    <ul>
      <li>To see that the converse is not true, consider two disjoint sets \(\mathcal{X} \cap \mathcal{Y}=\emptyset\). Now pick \(a \in \mathcal{X}\) and \(b \in \mathcal{Y}\).</li>
      <li>The line segment in <a href="#fig3">fig. 3</a> connecting \(a\) and \(b\) needs to contain some part
  that is neither in \(\mathcal{X}\) nor in \(\mathcal{Y}\), since we assumed that \(\mathcal{X} \cap \mathcal{Y}=\emptyset\). Hence the line segment is not in
  \(\mathcal{X} \cup \mathcal{Y}\) either, thus proving that in general unions of convex sets need not be convex.</li>
    </ul>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <a id="fig3"></a>
        <figure id="figure-">
  <picture>
    <img src="https://d2l.ai/_images/nonconvex.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
</figure>

    </div>
</div>
<div class="caption">
Fig. 3. The union of two convex sets need not be convex.
</div>
<p>Typically the problems in deep learning are defined on convex sets. For instance, \(\mathbb{R}^{d}\), the set of \(d\) - dimensional vectors of real numbers, is a convex set (after all, the line between any two points in \(\mathbb{R}^{d}\) remains in \(\mathbb{R}^{d}\) ). In some cases we work with variables of bounded length, such as balls of radius \(r\) as defined by \(\{\mathbf{x} | \mathbf{x} \in \mathbb{R}^d \textrm{ and } \|\mathbf{x}\| \leq r\}\).</p>

<h3 id="convex-functions">Convex Functions</h3>

<p>Now that we have convex sets we can introduce <em>convex functions</em> $f$.
Given a convex set \(\mathcal{X}\), a function \(f: \mathcal{X} \to \mathbb{R}\) is <em>convex</em> if for all \(x, x' \in \mathcal{X}\) and for all \(\lambda \in [0, 1]\) we have</p>

\[\lambda f(x) + (1-\lambda) f(x') \geq f(\lambda x + (1-\lambda) x').\]

<p>To illustrate this let’s plot a few functions and check which ones satisfy the requirement.
Below we define a few functions, both convex and nonconvex.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure id="figure-">
  <picture>
    <img src="https://d2l.ai/_images/output_convexity_94e148_15_0.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
</figure>

    </div>
</div>

<h3 id="jensens-inequality">Jensen’s Inequality</h3>

<p>Given a convex function \(f\), one of the most useful mathematical tools is <em>Jensen’s inequality</em>.
It amounts to a generalization of the definition of convexity:</p>

\[\sum_i \alpha_i f(x_i)  \geq f\left(\sum_i \alpha_i x_i\right)    \textrm{ and }    E_X[f(X)]  \geq f\left(E_X[X]\right),\]

<p>where \(\alpha_i\) are nonnegative real numbers such that \(\sum_i \alpha_i = 1\) and \(X\) is a random variable. In other words, the expectation of a convex function is no less than the convex function of an expectation, where the latter is usually a simpler expression. 
To prove the first inequality we repeatedly apply the definition of convexity to one term in the sum at a time.</p>

<p>One of the common applications of Jensen’s inequality is to bound a more complicated expression by a simpler one.</p>
<ul>
  <li>
    <p>For example, its application can be with regard to the log-likelihood of partially observed random variables. That is, we use</p>

\[E_{Y \sim P(Y)}[-\log P(X \mid Y)] \geq -\log P(X),\]

    <ul>
      <li>since \(\int P(Y) P(X \mid Y) dY = P(X)\). This can be used in variational methods.</li>
      <li>Here \(Y\) is typically the unobserved random variable, \(P(Y)\) is the best guess of how it might be distributed,</li>
      <li>and \(P(X)\) is the distribution with \(Y\) integrated out. For instance, in clustering \(Y\) might be the cluster labels and \(P(X \mid Y)\) is the generative model when applying cluster labels.</li>
    </ul>
  </li>
</ul>

<h3 id="properties">Properties</h3>

<p>Convex functions have many useful properties. We describe a few commonly-used ones below.</p>

<h4 id="local-minima-are-global-minima">Local Minima Are Global Minima</h4>

<p>This can be proved by <a href="https://en.wikipedia.org/wiki/Proof_by_contradiction" rel="external nofollow noopener" target="_blank">contradiction</a>:</p>

<p>Consider a convex function \(f\) defined on a convex set \(\mathcal{X}\).</p>

<ul>
  <li>Suppose that \(x^{*}\in \mathcal{X}\) is a local minimum: there exists a small positive value \(p\) so that for \(x \in \mathcal{X}\) that satisfies 
  \(0&lt;\left|x-x^{*}\right| \leq p\) we have \(f\left(x^{*}\right)&lt;f(x)\)</li>
</ul>

<p>Now, let’s make an assumption:</p>

<p>Assume that the local minimum \(x^{*}\) is not the global minimum of \(f\): there exists \(x^{\prime} \in \mathcal{X}\) for which \(f\left(x^{\prime}\right)&lt;f\left(x^{*}\right)\). Remember the fact that we constrain a range for the condition of \(p\), therefore we also need to make sure that the \(x^{\prime}\) also exists within this range of \(p\) to make the contradiction stands in the following proof.</p>

<p>Since \(\mathcal{X}\) is convex, the line segment between \(x^{*}\) and \(x^{\prime}\) is entirely within \(\mathcal{X}\). Any point \(x\) on this line can be expressed as:</p>

\[x=\lambda x^{*}+(1-\lambda) x^{\prime}, \quad \lambda \in[0,1]\]

<p>The distance between \(x\) and \(x^{*}\) is:</p>

\[\left|x-x^{*}\right|=\left|\lambda x^{*}+(1-\lambda) x^{\prime}-x^{*}\right|=(1-\lambda)\left|x^{\prime}-x^{*}\right|\]

<p>To ensure \(\textcolor{red}{\left|x - x^{*}\right| \leq p}\) and \(\textcolor{red}{\left|x - x^{*}\right| &gt; 0}\)</p>

\[(1-\lambda)\left|x^{\prime}-x^{*}\right|=p\]

<p>Solving for \(\lambda\), we get:</p>

\[1-\lambda=\frac{p}{\left|x^{\prime}-x^{*}\right|} \Longrightarrow \lambda=1-\frac{p}{\left|x^{\prime}-x^{*}\right|}\]

<p>However, according to the definition of convex functions, we have</p>

\[\begin{aligned} f\left(\lambda x^{*}+(1-\lambda) x^{\prime}\right) &amp; \leq \lambda f\left(x^{*}\right)+(1-\lambda) f\left(x^{\prime}\right) \\ &amp; &lt;\lambda f\left(x^{*}\right)+(1-\lambda) f\left(x^{*}\right) \\ &amp; =f\left(x^{*}\right)\end{aligned}\]

<p>which contradicts with our statement that \(x^{*}\) is a local minimum. Therefore, there does not exist
\(x^{\prime} \in \mathcal{X}\) for which \(f\left(x^{\prime}\right)&lt;f\left(x^{*}\right)\). The local minimum \(x^{*}\) is also the global minimum.</p>
<h4 id="below-sets-of-convex-functions-are-convex">Below Sets of Convex Functions Are Convex</h4>
<p>We can conveniently define convex sets via <em>below sets</em> of convex functions. Concretely, given a convex function \(f\) defined on a convex set \(\mathcal{X}\), any below set</p>

\[\mathcal{S}_b \stackrel{\textrm{def}}{=} \{ x \in \mathcal{X} \mid f(x) \leq b \}\]

<p>is convex.</p>

<p><strong>Proof:</strong></p>

<p>Take any two points \(x, x' \in \mathcal{S}_b\). By definition, this means that \(f(x) \leq b\) and \(f(x') \leq b\).</p>

<p>Consider any \(\lambda \in [0,1]\) and define \(y = \lambda x + (1 - \lambda) x'\). Since \(\mathcal{X}\) is convex and \(x, x' \in \mathcal{X}\), it follows that \(y \in \mathcal{X}\).</p>

<p>Using the convexity of \(f\), we have:</p>

\[f(y) = f(\lambda x + (1 - \lambda) x') \leq \lambda f(x) + (1 - \lambda) f(x') \leq \lambda b + (1 - \lambda) b = b.\]

<p>Therefore, \(f(y) \leq b\), which means \(y \in \mathcal{S}_b\).</p>

<p>Since any convex combination of points in \(\mathcal{S}_b\) is also in \(\mathcal{S}_b\), the set \(\mathcal{S}_b\) is convex.</p>

<h4 id="convexity-and-second-derivatives">Convexity and Second Derivatives</h4>

<p>Whenever the second derivative of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) exists, it is easy to check whether \(f\) is convex. All we need to do is check whether the Hessian of \(f\) is positive semidefinite: \(\nabla^2 f \succeq 0\), i.e., denoting the Hessian matrix \(\nabla^2 f\) by \(\mathbf{H}\), we have \(\mathbf{x}^\top \mathbf{H} \mathbf{x} \geq 0\) for all \(\mathbf{x} \in \mathbb{R}^n\).</p>

<p>For instance, the function \(f(\mathbf{x}) = \frac{1}{2} \|\mathbf{x}\|^2\) is convex since \(\nabla^2 f = \mathbf{I}\), i.e., its Hessian is the identity matrix.</p>

<p>Formally, a twice-differentiable one-dimensional function \(f: \mathbb{R} \rightarrow \mathbb{R}\) is convex if and only if its second derivative \(f''(x) \geq 0\). For any twice-differentiable multidimensional function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\), it is convex if and only if its Hessian \(\nabla^2 f \succeq 0\).</p>

<p><strong>Proof:</strong></p>

<p>First, we prove the one-dimensional case.</p>

<p>Assume that \(f\) is convex. Then, for any \(\epsilon &gt; 0\):</p>

\[\frac{1}{2} f(x + \epsilon) + \frac{1}{2} f(x - \epsilon) \geq f\left( \frac{x + \epsilon + x - \epsilon}{2} \right) = f(x).\]

<p>This inequality follows from the definition of convexity.</p>

<p>Since the second derivative is given by the limit over finite differences, it follows that:</p>

\[f''(x) = \lim_{\epsilon \to 0} \frac{f(x + \epsilon) + f(x - \epsilon) - 2 f(x)}{\epsilon^2} \geq 0.\]

<p>Thus, \(f''(x) \geq 0\).</p>

<p>Conversely, suppose that \(f''(x) \geq 0\). Then, \(f'\) is a monotonically nondecreasing function.</p>

<p>Let \(a &lt; x &lt; b\) be points in \(\mathbb{R}\), where \(x = (1 - \lambda)a + \lambda b\) and \(\lambda \in (0, 1)\).</p>

<p>By the <a href="https://en.wikipedia.org/wiki/Mean_value_theorem" rel="external nofollow noopener" target="_blank">mean value theorem</a>, there exist \(\alpha \in [a, x]\) and \(\beta \in [x, b]\) such that:</p>

\[f'(\alpha) = \frac{f(x) - f(a)}{x - a}, \quad f'(\beta) = \frac{f(b) - f(x)}{b - x}.\]

<p>Since \(f'\) is nondecreasing, \(f'(\beta) \geq f'(\alpha)\).</p>

<p>Therefore:</p>

\[\frac{f(x) - f(a)}{x - a} \leq \frac{f(b) - f(x)}{b - x}.\]

<p>Cross-multiplying:</p>

\[(b - x)[f(x) - f(a)] \leq (x - a)[f(b) - f(x)].\]

<p>Rewriting (carefully expand it out on the previous step):</p>

\[\frac{x - a}{b - a}[f(b) - f(x)] + \frac{b - x}{b - a}[f(x) - f(a)] \geq 0.\]

<p>This implies(rewrite \(\lambda=\frac{x-a}{b-a}\)):</p>

\[(1 - \lambda) f(a) + \lambda f(b) \geq f(x).\]

<p>Thus, \(f\) is convex.</p>

<p>Next, we introduce a lemma before proving the multidimensional case.</p>

<p><strong>Lemma:</strong> A function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) is convex if and only if for all \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\):</p>

\[g(z) \stackrel{\textrm{def}}{=} f(z \mathbf{x} + (1 - z) \mathbf{y}), \quad z \in [0,1],\]

<p>is convex.</p>

<p>To show that convexity of \(f\) implies that \(g\) is convex, observe that for all \(a, b, \lambda \in [0, 1]\):</p>

\[\begin{aligned}
g(\lambda a + (1 - \lambda) b) &amp;= f\left( (\lambda a + (1 - \lambda) b) \mathbf{x} + \left(1 - (\lambda a + (1 - \lambda) b)\right) \mathbf{y} \right) \\
&amp;= f\left( \lambda \left( a \mathbf{x} + (1 - a) \mathbf{y} \right) + (1 - \lambda) \left( b \mathbf{x} + (1 - b) \mathbf{y} \right) \right) \\
&amp;\leq \lambda f(a \mathbf{x} + (1 - a) \mathbf{y}) + (1 - \lambda) f(b \mathbf{x} + (1 - b) \mathbf{y}) \\
&amp;= \lambda g(a) + (1 - \lambda) g(b).
\end{aligned}\]

<p>Therefore, \(g\) is convex.</p>

<p>Conversely, suppose \(g\) is convex for all \(\mathbf{x}, \mathbf{y}\). Then, for all \(\lambda \in [0, 1]\):</p>

\[\begin{aligned}
f(\lambda \mathbf{x} + (1 - \lambda) \mathbf{y}) &amp;= g(\lambda \cdot 1 + (1 - \lambda) \cdot 0) \\
&amp;\leq \lambda g(1) + (1 - \lambda) g(0) \\
&amp;= \lambda f(\mathbf{x}) + (1 - \lambda) f(\mathbf{y}).
\end{aligned}\]

<p>Thus, \(f\) is convex.</p>

<p>Finally, using the lemma and the one-dimensional result, we can prove the multidimensional case.</p>

<p>A multidimensional function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) is convex if and only if for all \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\), the function \(g(z) = f(z \mathbf{x} + (1 - z) \mathbf{y})\) is convex.</p>

<p>According to the one-dimensional case, this holds if and only if:</p>

\[g''(z) = (\mathbf{x} - \mathbf{y})^\top \nabla^2 f(z \mathbf{x} + (1 - z) \mathbf{y})(\mathbf{x} - \mathbf{y}) \geq 0,\]

<p>for all \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\).</p>

<p>This condition is equivalent to \(\nabla^2 f \succeq 0\), per the definition of positive semidefinite matrices.</p>

<h3 id="constraints">Constraints</h3>
<p>One of the nice properties of convex optimization is that it allows us to handle constraints efficiently. That is, it allows us to solve <em>constrained optimization</em> problems of the form:</p>

\[\begin{array}{l}
\underset{\mathbf{x}}{\operatorname{minimize}} f(\mathbf{x}) \\
\text { subject to } c_{i}(\mathbf{x}) \leq 0 \text { for all } i \in\{1, \ldots, n\}
\end{array}\]

<p>where \(f\) is the objective and the functions \(c_{i}\) are constraint functions.</p>

<p><strong>Question</strong>: Imagine a unit ball \(c_{1}(\mathbf{x})=\|\mathbf{x}\|_{2}-1\); Now we have a second constraint: \(c_{2}(\mathbf{x})=\mathbf{v}^{\top} \mathbf{x}+b\). What does this visually represent?</p>

<h4 id="lagrangian">Lagrangian</h4>

<p>A ball inside a box; the ball will roll to the place that is lowest, and the forces of gravity will be balanced out with the forces that the sides of the box can impose on the ball.</p>

<ul>
  <li>The gradient of the objective function (gravity) will be offset by the gradient of the constraint function (the ball needs to remain inside the box by virtue of the walls “pushing back”).</li>
</ul>

<p>The above reasoning can be expressed via the following saddle point optimization problem:</p>

\[L\left(\mathbf{x}, \alpha_{1}, \ldots, \alpha_{n}\right) = f(\mathbf{x}) + \sum_{i=1}^{n} \alpha_{i} c_{i}(\mathbf{x}) \text{ where } \alpha_{i} \geq 0\]

<ul>
  <li>The variables \(\alpha_{i}(i=1, \ldots, n)\) are <em>Lagrange multipliers</em> that ensure that constraints are properly enforced.</li>
</ul>

<p><strong>What are Lagrange Multipliers \(\alpha_{i}\)?</strong><br>
If the constraint \(c_{i}(x) \leq 0\) is active (i.e., if \(c_{i}(x) = 0\)), the corresponding multiplier \(\alpha_{i}\) can take any positive value. If the constraint \(c_{i}(x) &lt; 0\) is inactive (i.e., not binding or strictly less than zero), then the multiplier \(\alpha_{i}\) is set to zero. <em>Question? Can you think of an example to demonstrate the activation of the constraint function?</em></p>

<p><strong>Example</strong><br>
Suppose you have a function \(f(x) = x^{2}\), and you want to minimize it, but with the constraint that \(x \geq 1\) (meaning \(c(x) = 1 - x \leq 0\)).</p>

<ul>
  <li>If you ignore the constraint, the minimum would be at \(x = 0\), but this violates the constraint \(x \geq 1\).</li>
  <li>The Lagrangian would add a term \(\alpha \cdot (1 - x)\), where \(\alpha \geq 0\).</li>
  <li>The solution to the Lagrangian will find \(x = 1\) (the smallest value of \(x\) that satisfies the constraint \(x \geq 1\)) and a corresponding value of \(\alpha\) that ensures the constraint is properly enforced.</li>
</ul>

<h4 id="penalties">Penalties</h4>

<p>Rather than satisfying \(c_{i}(\mathrm{x}) \leq 0\), we simply add \(\alpha_{i} c_{i}(\mathbf{x})\) to the objective function \(f(x)\). This ensures that the constraints will not be violated too badly. It is a common trick: we add \(\frac{\lambda}{2}\|\mathbf{w}\|^{2}\) to the objective function to ensure that \(\mathbf{w}\) does not grow too large. We can see this will ensure \(\|\mathbf{w}\|^{2} - r^{2} \leq 0\) for some radius \(r\).</p>

<p>In general, adding penalties is a good way of ensuring approximate constraint satisfaction. In practice, this turns out to be much more robust than exact satisfaction. Furthermore, for nonconvex problems, many of the properties that make the exact approach so appealing in the convex case (e.g., optimality) no longer hold.</p>

<h2 id="gradient-descent">Gradient Descent</h2>

<p>Consider some continuously differentiable real-valued function \(f: \mathbb{R} \rightarrow \mathbb{R}\). Using a Taylor expansion, we obtain:</p>

\[f(x + \epsilon) = f(x) + \epsilon f^{\prime}(x) + O\left(\epsilon^{2}\right).\]

<p>It is not unreasonable to assume that for small \(\epsilon\), moving in the direction of the negative gradient will decrease \(f\). Under this assumption, we pick a fixed step size \(\eta &gt; 0\) and choose \(\epsilon = -\eta f^{\prime}(x)\):</p>

\[f\left(x - \eta f^{\prime}(x)\right) = f(x) - \eta f^{\prime 2}(x) + O\left(\eta^{2} f^{\prime 2}(x)\right)\]

<p><strong>Observations</strong></p>
<ol>
  <li>If the derivative \(f^{\prime}(x) \neq 0\) does not vanish, we make progress since \(\eta f^{\prime 2}(x) &gt; 0\).</li>
  <li>A small enough \(\eta\) will make the higher-order terms become irrelevant.</li>
</ol>

\[f\left(x - \eta f^{\prime}(x)\right) \lesssim f(x).\]

<p>This means if we use \(x \leftarrow x - \eta f^{\prime}(x)\) to iterate \(x\), the value of the function \(f(x)\) might decline.</p>

<h3 id="multivariate-gradient-descent">Multivariate Gradient Descent</h3>

<p>Let \(\mathbf{x} = [x_{1}, x_{2}, \ldots, x_{d}]^{\top}\); the objective function \(f: \mathbb{R}^{d} \rightarrow \mathbb{R}\) maps vectors into scalars.</p>

\[\nabla f(\mathbf{x}) = \left[\frac{\partial f(\mathbf{x})}{\partial x_{1}}, \frac{\partial f(\mathbf{x})}{\partial x_{2}}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_{d}}\right]^{\top}\]

<p>Each partial derivative element \(\partial f(\mathbf{x}) / \partial x_{i}\) in the gradient indicates the rate of change of \(f\) at \(\mathbf{x}\) with respect to the input \(x_i\). Following the same idea from the one-dimensional case, we have:</p>

\[f(\mathbf{x} + \boldsymbol{\epsilon}) = f(\mathbf{x}) + \boldsymbol{\epsilon}^{\top} \nabla f(\mathbf{x}) + O\left(\|\boldsymbol{\epsilon}\|^{2}\right)\]

<p>Choosing a suitable learning rate \(\eta &gt; 0\) yields the prototypical gradient descent algorithm:</p>

\[\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f(\mathbf{x})\]

<h3 id="adaptive-methods">Adaptive Methods</h3>

<p>Picking a suitable learning rate \(\eta\) is tricky; <em>Too small, we learn too slow; Too large, the solution oscillates, and in the worst case, it diverges.</em> Can we determine \(\eta\) automatically or eliminate the selection process? <em>Second-order methods</em> that consider not only the value and gradient of the objective function but also its <em>curvature</em> can help.</p>

<p>Please note that while these methods cannot be applied to deep learning directly due to the computational cost, they provide useful intuition into designing advanced optimization algorithms that mimic many of the desirable properties of these algorithms.</p>

<h3 id="newtons-method">Newton’s Method</h3>

<p>Reviewing the Taylor expansion of some function \(f: \mathbb{R}^{d} \rightarrow \mathbb{R}\) shows that there is no need to stop after the first term.</p>

\[f(\mathbf{x} + \boldsymbol{\epsilon}) = f(\mathbf{x}) + \boldsymbol{\epsilon}^{\top} \nabla f(\mathbf{x}) + \frac{1}{2} \epsilon^{\top} \nabla^{2} f(\mathbf{x}) \boldsymbol{\epsilon} + O\left(\|\boldsymbol{\epsilon}\|^{3}\right).\]

<p>To avoid cumbersome notation, we define \(\mathbf{H} \stackrel{\text { def }}{=} \nabla^{2} f(\mathbf{x})\) to be the Hessian of \(f\), which is a \(d \times d\) matrix.</p>

<p><strong>Problems with this dimensionality?</strong> For small \(d\) and simple problems, \(\mathbf{H}\) is easy to compute. For deep neural networks, on the other hand, \(\mathbf{H}\) may be prohibitively large due to the cost of storing \(\mathcal{O}(d^2)\) entries. Furthermore, it may be too expensive to compute via backpropagation.</p>

<p>Now we have:</p>

\[f(\mathbf{x} + \epsilon) = f(\mathbf{x}) + \epsilon^{T} \nabla f(\mathbf{x}) + \frac{1}{2} \epsilon^{T} \mathbf{H} \epsilon + O\left(\|\epsilon\|^{3}\right).\]

<p><strong>Objective: Find \(\epsilon\)</strong><br>
We want to find the value of \(\epsilon\) that minimizes the function \(f(\mathbf{x} + \epsilon)\). This is a standard second-order optimization technique where we use the gradient and the Hessian to adjust the value of \(\mathbf{x}\) for a better approximation (closer to a minimum). Therefore:</p>

\[\frac{\partial}{\partial \epsilon}\left(f(\mathbf{x}) + \epsilon^{T} \nabla f(\mathbf{x}) + \frac{1}{2} \epsilon^{T} \mathbf{H} \epsilon\right).\]

<p>Taking the derivative and setting it to zero, we have:</p>

\[\nabla f(\mathbf{x}) + \mathbf{H} \boldsymbol{\epsilon} = 0 \text{ and hence } \boldsymbol{\epsilon} = -\mathbf{H}^{-1} \nabla f(\mathbf{x}).\]

<p><strong>Why are we going through all this trouble?</strong><br>
Let’s say \(f(x) = \frac{1}{2} x^{2}\), \(\nabla f(x) = \frac{d}{d x}\left(\frac{1}{2} x^{2}\right) = x\), and \(\frac{d^{2}}{d x^{2}}\left(\frac{1}{2} x^{2}\right) = 1\). Following Newton’s method, the update step is \(\epsilon = -x\).</p>

<h3 id="convergence-analysis">Convergence Analysis</h3>

<p>Let’s apply convergence analysis on Newton’s method, specifically how quickly the error \(e^{(k)} = x^{(k)} - x^{*}\) (the difference between the current iterate \(x^{(k)}\) and the optimal solution \(x^{*}\)) decreases from one iteration to the next. Quick recap of the key concept:</p>

<ul>
  <li>\(f\left(x^{*}\right)\) is minimized when \(f^{\prime}\left(x^{*}\right) = 0\).</li>
  <li>Newton’s method updates the current point \(x^{(k)}\) using the rule:</li>
</ul>

\[x^{(k+1)} = x^{(k)} - \frac{f^{\prime}\left(x^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\]

<p>The error at iteration \(k, e^{(k)} = x^{(k)} - x^{*}\), is used in a <strong>Taylor expansion</strong> around the optimal point \(x^{*}\) to analyze how the error behaves. We expand the derivative \(f^{\prime}\left(x^{*}\right) = 0\) around \(x^{(k)}\) as follows:</p>

\[0 = f^{\prime}\left(x^{*}\right) = f^{\prime}\left(x^{(k)}\right) - e^{(k)} f^{\prime \prime}\left(x^{(k)}\right) + \frac{1}{2}\left(e^{(k)}\right)^{2} f^{(3)}\left(\xi^{(k)}\right)\]

<p>This equation gives us an approximation of the derivative \(f^{\prime}\left(x^{*}\right)\) using the second and third derivatives of \(f(x)\) evaluated at points near \(x^{(k)}\).</p>

<ul>
  <li>\(e^{(k)} f^{\prime \prime}\left(x^{(k)}\right)\): first-order correction using the second derivative.</li>
  <li>\(\frac{1}{2}\left(e^{(k)}\right)^{2} f^{(3)}\left(\xi^{(k)}\right)\): second-order correction using the third derivative.</li>
</ul>

<p>Here, \(\xi^{(k)}\) is some point between \(x^{(k)}\) and \(x^{*}\) according to the Mean Value Theorem. To make the equation more tractable, we divide the Taylor expansion by \(f^{\prime \prime}\left(x^{(k)}\right)\):</p>

\[e^{(k)} - \frac{f^{\prime}\left(x^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)} = \frac{1}{2}\left(e^{(k)}\right)^{2} \frac{f^{(3)}\left(\xi^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\]

<p>This equation tells us how far the next iterate \(x^{(k+1)}\) is from the optimal solution \(x^{*}\) after one Newton step. Now we plug in the update rule from Newton’s method:</p>

\[x^{(k+1)} = x^{(k)} - \frac{f^{\prime}\left(x^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\]

<p>The error \(e^{(k+1)}\) at the next iteration is:</p>

\[e^{(k+1)} = x^{(k+1)} - x^{*} = x^{(k)} - \frac{f^{\prime}\left(x^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)} - x^{*} = e^{(k)} - \frac{f^{\prime}\left(x^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\]

<p>Recall the expanded form of \(f^{\prime}\left(x^{*}\right) = 0\):</p>

\[f^{\prime}\left(x^{(k)}\right) = e^{(k)} f^{\prime \prime}\left(x^{(k)}\right) - \frac{1}{2}\left(e^{(k)}\right)^{2} f^{(3)}\left(\xi^{(k)}\right)\]

<p>Substitution:</p>

\[e^{(k+1)} = -\frac{e^{(k)} f^{\prime \prime}\left(x^{(k)}\right) - \frac{1}{2}\left(e^{(k)}\right)^{2} f^{(3)}\left(\xi^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\]

<p>Simplification:</p>

\[e^{(k+1)} = -\left(e^{(k)} - \frac{1}{2}\left(e^{(k)}\right)^{2} \frac{f^{(3)}\left(\xi^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\right)\]

<p>We can ignore \(e^{(k)}\); why? <strong>The error at iteration \(k+1\) is mostly driven by the quadratic correction</strong>:</p>

<ol>
  <li>At convergence, the linear term \(e^{(k)}\) drives the approximation initially.</li>
  <li>As the error gets small, Newton’s method updates the solution in such a way that the remaining error becomes dominated by the quadratic term.</li>
</ol>

<p>In other words, even though the quadratic term \(\left(e^{(k)}\right)^{2}\) is smaller in magnitude, it determines the next iteration’s error because the method’s convergence accelerates rapidly near the optimum. Therefore:</p>

\[e^{(k+1)} = \frac{1}{2}\left(e^{(k)}\right)^{2} \frac{f^{(3)}\left(\xi^{(k)}\right)}{f^{\prime \prime}\left(x^{(k)}\right)}\]

<p>We want to analyze the magnitude regardless of the sign, therefore taking the absolute value of both sides, we get:</p>

\[\left|e^{(k+1)}\right| = \frac{1}{2}\left(e^{(k)}\right)^{2} \frac{\left|f^{(3)}\left(\xi^{(k)}\right)\right|}{f^{\prime \prime}\left(x^{(k)}\right)}\]

<p>With a bounded condition \(\left|f^{\prime \prime \prime}\left(\xi^{(k)}\right)\right| /\left(2 f^{\prime \prime}\left(x^{(k)}\right)\right) \leq c\), we have:</p>

\[\left|e^{(k+1)}\right| \leq c\left(e^{(k)}\right)^{2}\]

<h3 id="preconditioning">Preconditioning</h3>

<p>Preconditioning provides a cheaper alternative to using the full Hessian matrix. Instead of computing and storing the entire Hessian, preconditioning only uses the diagonal entries of the Hessian matrix. This dramatically simplifies the process while still providing useful information about the curvature of the function.</p>

<p>The update equation in this preconditioned version is:</p>

\[\mathbf{x} \leftarrow \mathbf{x} - \eta \operatorname{diag}(\mathbf{H})^{-1} \nabla f(\mathbf{x})\]

<p>where:</p>

<ul>
  <li>\(\operatorname{diag}(\mathbf{H})^{-1}\) is the inverse of this diagonal matrix.</li>
</ul>

<p><strong>Real-World Analogy: Units Mismatch</strong><br>
Imagine you are optimizing a function where one variable is <em>height in millimeters</em> and another variable is <em>height in kilometers</em>.</p>

<p>If you try to use the same learning rate (step size) for both variables, it will create problems because the scales of the two variables are wildly different. This mismatch in units (millimeters vs. kilometers) will lead to slow convergence because one variable might need very small updates, while the other requires larger updates.</p>

<p>Preconditioning solves this problem by effectively allowing for different learning rates for each variable. It adjusts the step size based on the curvature (as estimated by the diagonal of the Hessian) so that each variable gets an update that is appropriate for its scale.</p>


    

    
  </article>
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      Jue
      
      Guo. 
      
      
        Last updated: December 11, 2024.
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/personal/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/personal/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/personal/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/personal/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/personal/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/personal/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/personal/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  

    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/personal/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
