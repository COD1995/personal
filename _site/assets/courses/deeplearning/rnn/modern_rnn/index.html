<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Modern Recurrent Neural Networks | Jue Guo
    
  
</title>
<meta name="author" content="Jue Guo">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/personal/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/personal/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/personal/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/personal/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/personal/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/personal/assets/courses/deeplearning/rnn/modern_rnn/">

<!-- Dark Mode -->
<script src="/personal/assets/js/theme.js?5ce9accf63efc46cd59e47ccb47fd172"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->





  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/personal//">
          
            
              <span class="font-weight-bold">Jue</span>
            
            
            Guo
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/personal/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/teaching/">courses
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>

    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">


  <div class="fixed-bottom-button">
    <a href="/personal/teaching/deeplearnig" class="btn-back-course">
      ‚Üê Deep Learning Course Page
    </a>
  </div>


<!-- Always apply separation lines (scoped to article) -->

  <style type="text/css">
    /* Heading 1 */
    .post > article h1 {
      border-bottom: 4px solid #4CAF50; /* Green separation line */
      padding-bottom: 12px;
      margin-top: 40px; /* Increased spacing above */
      margin-bottom: 32px; /* Increased spacing below */
    }

    /* Heading 2 */
    .post > article h2 {
      border-bottom: 3px solid #2196F3; /* Blue separation line */
      padding-bottom: 10px;
      margin-top: 36px; /* Increased spacing above */
      margin-bottom: 28px; /* Increased spacing below */
    }

    /* Heading 3 */
    .post > article h3 {
      border-bottom: 3px dashed #FFC107; /* Yellow dashed line */
      padding-bottom: 8px;
      margin-top: 32px; /* Increased spacing above */
      margin-bottom: 24px; /* Increased spacing below */
    }

    /* Heading 4 */
    .post > article h4 {
      border-bottom: 2px dotted #9C27B0; /* Purple dotted line */
      padding-bottom: 6px;
      margin-top: 28px; /* Increased spacing above */
      margin-bottom: 20px; /* Increased spacing below */
    }

    /* Heading 5 */
    .post > article h5 {
      border-bottom: 2px solid #FF5722; /* Orange solid line */
      padding-bottom: 4px;
      margin-top: 24px; /* Increased spacing above */
      margin-bottom: 16px; /* Increased spacing below */
    }
  </style>



  <!-- Numbered Headings CSS -->
  <style type="text/css">
    /* Base reset for counters */
    body {
      counter-reset: h1-counter 4;
    }

    /* Heading 1 */
    .post > article h1 {
      counter-increment: h1-counter;
      counter-reset: h2-counter;
      
      /* Show h1 number */
      
    }

    
    .post > article h1::before {
      content: counter(h1-counter) ". ";
    }
    

    /* Heading 2 */
    .post > article h2 {
      counter-increment: h2-counter;
      counter-reset: h3-counter;
    }

    .post > article h2::before {
      content: counter(h1-counter) "." counter(h2-counter) " ";
    }

    /* Heading 3 */
    .post > article h3 {
      counter-increment: h3-counter;
      counter-reset: h4-counter;
    }

    .post > article h3::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) " ";
    }

    /* Heading 4 */
    .post > article h4 {
      counter-increment: h4-counter;
      counter-reset: h5-counter;
    }

    .post > article h4::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) " ";
    }

    /* Heading 5 */
    .post > article h5 {
      counter-increment: h5-counter;
    }

    .post > article h5::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) "." counter(h5-counter) " ";
    }
  </style>



<div class="post">
  <article>
    <header class="post-header">
      <h1 class="post-title">Modern Recurrent Neural Networks</h1>
      <p class="post-description"></p>
    </header>

    <p>We introduced the key ideas behind recurrent neural networks (RNNs). However, just as with convolutional neural networks, there has been a tremendous amount of innovation in RNN architectures, culminating in several complex designs that have proven successful in practice. In particular, the most popular designs feature mechanisms for mitigating the notorious numerical instability faced by RNNs, as typified by vanishing and exploding gradients. Recall that we dealt with exploding gradients by applying a blunt gradient clipping heuristic. Despite the efficacy of this hack, it leaves open the problem of vanishing gradients.</p>

<p>In this section, we introduce the key ideas behind the most successful RNN architectures for sequences, which stem from two landmark papers:</p>

<p><strong>Long Short-Term Memory (LSTM)</strong> Long Short-Term Memory (LSTM) networks, introduced by Hochreiter and Schmidhuber in 1997, replace traditional nodes in the hidden layer of an RNN with <em>memory cells</em>. These memory cells overcome the training difficulties of earlier recurrent networks. Intuitively:</p>
<ul>
  <li>LSTM memory cells avoid the vanishing gradient problem by maintaining values in the memory state over long time sequences.</li>
  <li>A recurrent edge with weight 1 allows information to cascade across many successive time steps.</li>
  <li>
<strong>Multiplicative gates</strong> control:
    <ol>
      <li>Which inputs are allowed into the memory state,</li>
      <li>When the memory state influences the network‚Äôs output.</li>
    </ol>
  </li>
</ul>

<p>This innovation enables LSTMs to handle long-range dependencies in sequences, which were previously infeasible with basic RNNs.</p>

<p><strong>Bidirectional Recurrent Neural Networks (BiRNNs)</strong> Bidirectional Recurrent Neural Networks (Schuster and Paliwal, 1997) introduce the concept of processing sequences in both forward and backward directions. Unlike traditional RNNs, where only past inputs affect the output:</p>
<ul>
  <li>
<strong>BiRNNs</strong> use information from both preceding and subsequent time steps to determine the output at any point.</li>
  <li>This architecture is particularly effective for sequence labeling tasks, such as:
    <ul>
      <li>
<strong>Natural Language Processing</strong>: Named Entity Recognition (NER), Part-of-Speech (POS) tagging,</li>
      <li>
<strong>Speech Recognition</strong>,</li>
      <li>
<strong>Handwriting Recognition</strong>.</li>
    </ul>
  </li>
</ul>

<p><strong>Combined Innovations</strong> LSTM and BiRNN architectures are not mutually exclusive. They have been successfully combined for tasks such as:</p>
<ul>
  <li>
<strong>Phoneme Classification</strong> (Graves and Schmidhuber, 2005),</li>
  <li>
<strong>Handwriting Recognition</strong> (Graves et al., 2008).</li>
</ul>

<p>we will cover the following topics:</p>
<ol>
  <li>
<strong>LSTM Architecture</strong>: A detailed breakdown of its structure and function,</li>
  <li>
<strong>Gated Recurrent Units (GRUs)</strong>: A lightweight variation of LSTMs,</li>
  <li>
<strong>Bidirectional RNNs</strong>: Leveraging future and past sequence information,</li>
  <li>
<strong>Deep RNNs</strong>: Stacking RNN layers for greater capacity,</li>
  <li>
<strong>Sequence-to-Sequence Tasks</strong>: Applying RNNs to machine translation with key ideas like encoder-decoder architectures and beam search.</li>
</ol>

<h2 id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h2>

<p>Shortly after the first Elman-style RNNs were trained using backpropagation, the challenges of learning long-term dependencies (due to vanishing and exploding gradients) became evident. These problems were discussed by Bengio and Hochreiter. Hochreiter had identified these issues as early as 1991 in his Master‚Äôs thesis, although the work was not widely recognized since it was written in German.</p>

<p>While <strong>gradient clipping</strong> provides a solution for exploding gradients, vanishing gradients require a more intricate approach. One of the first and most effective solutions came with the <strong>long short-term memory (LSTM)</strong> model proposed by Hochreiter and Schmidhuber. 
LSTMs are similar to standard recurrent neural networks, but they replace each ordinary recurrent node with a <em>memory cell</em>. Each memory cell contains an <em>internal state</em>, a node with a self-connected recurrent edge of fixed weight 1, allowing the gradient to flow across many time steps without vanishing or exploding.</p>

<h3 id="gated-memory-cell">Gated Memory Cell</h3>

<p>Each memory cell is equipped with an <em>internal state</em>
and a number of multiplicative gates that determine whether:</p>
<ol>
  <li>A given input should impact the internal state (<em>input gate</em>),</li>
  <li>The internal state should be flushed to \(0\) (<em>forget gate</em>),</li>
  <li>The internal state of a given neuron should influence the cell‚Äôs output (<em>output gate</em>).</li>
</ol>

<p><strong style="color: red; font-weight: 900;">Gated Hidden States</strong>The key distinction between vanilla RNNs and LSTMs
is that the latter support gating of the hidden state.
This gating mechanism determines:</p>
<ul>
  <li>When a hidden state should be <em>updated</em>,</li>
  <li>When it should be <em>reset</em>.</li>
</ul>

<p>These mechanisms are learned, enabling the network to:</p>
<ul>
  <li>Retain critical information from the first token by learning not to update the hidden state,</li>
  <li>Ignore irrelevant observations,</li>
  <li>Reset the latent state when necessary.</li>
</ul>

<p><strong style="color: red; font-weight: 900;">Input Gate, Forget Gate, and Output Gate</strong>
The data feeding into the LSTM gates are the input at the current time step and the hidden state of the previous time step, as illustrated in Figure .</p>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_lstm_01">
  <picture>
    <img src="https://d2l.ai/_images/lstm-0.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 1: Computing the input gate, forget gate, and output gate in an LSTM model.
    </figcaption>
  
</figure>

    </div>
</div>

<p>Mathematically, given:</p>
<ul>
  <li>\(\mathbf{X}_t \in \mathbb{R}^{n \times d}\) (input),</li>
  <li>\(\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}\) (previous hidden state),
the gates are computed as:</li>
</ul>

\[\begin{aligned}
\mathbf{I}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xi}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hi}} + \mathbf{b}_\textrm{i}),\\
\mathbf{F}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xf}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hf}} + \mathbf{b}_\textrm{f}),\\
\mathbf{O}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xo}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{ho}} + \mathbf{b}_\textrm{o}),
\end{aligned}\]

<p>where:</p>
<ul>
  <li>\(\mathbf{W}_{\textrm{xi}}, \mathbf{W}_{\textrm{xf}}, \mathbf{W}_{\textrm{xo}} \in \mathbb{R}^{d \times h}\),</li>
  <li>\(\mathbf{W}_{\textrm{hi}}, \mathbf{W}_{\textrm{hf}}, \mathbf{W}_{\textrm{ho}} \in \mathbb{R}^{h \times h}\),</li>
  <li>\(\mathbf{b}_\textrm{i}, \mathbf{b}_\textrm{f}, \mathbf{b}_\textrm{o} \in \mathbb{R}^{1 \times h}\).</li>
</ul>

<p><strong>Input Node</strong> The <em>input node</em> \(\tilde{\mathbf{C}}_t\) is calculated as:</p>

\[\tilde{\mathbf{C}}_t = \textrm{tanh}(\mathbf{X}_t \mathbf{W}_{\textrm{xc}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hc}} + \mathbf{b}_\textrm{c}),\]

<p>where:</p>
<ul>
  <li>\(\mathbf{W}_{\textrm{xc}} \in \mathbb{R}^{d \times h}\),</li>
  <li>\(\mathbf{W}_{\textrm{hc}} \in \mathbb{R}^{h \times h}\),</li>
  <li>\(\mathbf{b}_\textrm{c} \in \mathbb{R}^{1 \times h}\).</li>
</ul>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_lstm_12">
  <picture>
    <img src="https://d2l.ai/_images/lstm-1.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 2: Computing the input node in an LSTM model.
    </figcaption>
  
</figure>

    </div>
</div>

<p><strong style="color: red; font-weight: 900;">Memory Cell Internal State</strong> The memory cell internal state \(\mathbf{C}_t\) is updated using:</p>

\[\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t.\]

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_lstm_23">
  <picture>
    <img src="https://d2l.ai/_images/lstm-2.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 3: Computing the memory cell internal state in an LSTM model.
    </figcaption>
  
</figure>

    </div>
</div>

<p><strong style="color: red; font-weight: 900;">Hidden State</strong> The hidden state \(\mathbf{H}_t\) is computed as:</p>

\[\mathbf{H}_t = \mathbf{O}_t \odot \textrm{tanh}(\mathbf{C}_t).\]

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_lstm_34">
  <picture>
    <img src="https://d2l.ai/_images/lstm-3.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 4: Computing the hidden state in an LSTM model.
    </figcaption>
  
</figure>

    </div>
</div>

<h2 id="gated-recurrent-units-grus">Gated Recurrent Units (GRUs)</h2>

<p>As RNNs and particularly the LSTM architecture gained popularity during the 2010s, researchers sought simplified architectures that retained the core concepts of internal state and gating mechanisms but with faster computation. The <strong>gated recurrent unit (GRU)</strong> proposed by Cho et al. is one such architecture, offering a streamlined version of the LSTM that achieves comparable performance but is computationally faster.</p>

<p><strong>Reset Gate and Update Gate</strong> In GRUs, the LSTM‚Äôs three gates are replaced by two:</p>
<ol>
  <li>
<strong>Reset Gate</strong>: Controls how much of the previous state is retained.</li>
  <li>
<strong>Update Gate</strong>: Determines how much of the new state is derived from the old state.</li>
</ol>

<p>Both gates use sigmoid activation functions, ensuring their values lie in the range \((0, 1)\). Fig. <a href="#fig_gru_1">5</a> illustrates the inputs for both gates, which include the input of the current time step and the hidden state of the previous time step.</p>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_gru_15">
  <picture>
    <img src="https://d2l.ai/_images/gru-1.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 5: Computing the reset gate and the update gate in a GRU model.
    </figcaption>
  
</figure>

    </div>
</div>

<p><strong>Mathematics</strong>: For a given time step \(t\), let:</p>
<ul>
  <li>\(\mathbf{X}_t \in \mathbb{R}^{n \times d}\) be the input (batch size \(n\), input features \(d\)),</li>
  <li>\(\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}\) be the hidden state (hidden units \(h\)).</li>
</ul>

<p>The gates are computed as:</p>

\[\begin{aligned}
\mathbf{R}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xr}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hr}} + \mathbf{b}_\textrm{r}),\\
\mathbf{Z}_t &amp;= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xz}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hz}} + \mathbf{b}_\textrm{z}),
\end{aligned}\]

<p>where:</p>
<ul>
  <li>\(\mathbf{W}_{\textrm{xr}}, \mathbf{W}_{\textrm{xz}} \in \mathbb{R}^{d \times h}\) and \(\mathbf{W}_{\textrm{hr}}, \mathbf{W}_{\textrm{hz}} \in \mathbb{R}^{h \times h}\) are weights,</li>
  <li>\(\mathbf{b}_\textrm{r}, \mathbf{b}_\textrm{z} \in \mathbb{R}^{1 \times h}\) are biases.</li>
</ul>

<p><strong>Candidate Hidden State</strong> The reset gate integrates with the computation of the <em>candidate hidden state</em> \(\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}\):</p>

\[\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{\textrm{xh}} + (\mathbf{R}_t \odot \mathbf{H}_{t-1}) \mathbf{W}_{\textrm{hh}} + \mathbf{b}_\textrm{h}),\]

<p>where:</p>
<ul>
  <li>\(\mathbf{W}_{\textrm{xh}} \in \mathbb{R}^{d \times h}\) and \(\mathbf{W}_{\textrm{hh}} \in \mathbb{R}^{h \times h}\) are weights,</li>
  <li>\(\mathbf{b}_\textrm{h} \in \mathbb{R}^{1 \times h}\) is the bias,</li>
  <li>\(\odot\) represents elementwise multiplication (Hadamard product).</li>
</ul>

<p>Fig.<a href="#fig_gru_2">6</a> shows the computational flow for the candidate hidden state.</p>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_gru_26">
  <picture>
    <img src="https://d2l.ai/_images/gru-2.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 6: Computing the candidate hidden state in a GRU model.
    </figcaption>
  
</figure>

    </div>
</div>

<p><strong>Hidden State</strong> Finally, the <strong>update gate</strong> \(\mathbf{Z}_t\) determines the balance between the old hidden state \(\mathbf{H}_{t-1}\) and the candidate hidden state \(\tilde{\mathbf{H}}_t\):</p>

\[\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t.\]

<p>Fig.<a href="#fig_gru_3">7</a> illustrates this process.</p>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_gru_37">
  <picture>
    <img src="https://d2l.ai/_images/gru-3.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 7: Computing the hidden state in a GRU model.
    </figcaption>
  
</figure>

    </div>
</div>

<p><strong>Summary</strong> GRUs provide two key mechanisms:</p>
<ul>
  <li>
<strong>Reset Gate</strong>: Helps capture short-term dependencies by resetting the hidden state.</li>
  <li>
<strong>Update Gate</strong>: Helps capture long-term dependencies by balancing new and old information.</li>
</ul>


    

    
  </article>
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      ¬© Copyright 2024
      Jue
      
      Guo. 
      
      
        Last updated: December 11, 2024.
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/personal/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/personal/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/personal/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/personal/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/personal/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/personal/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/personal/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  

    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/personal/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
