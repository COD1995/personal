<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Recurrent Neural Networks | Jue Guo
    
  
</title>
<meta name="author" content="Jue Guo">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/personal/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/personal/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/personal/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/personal/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/personal/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/personal/assets/courses/deeplearning/rnn/recurrent_neural_networks/">

<!-- Dark Mode -->
<script src="/personal/assets/js/theme.js?5ce9accf63efc46cd59e47ccb47fd172"></script>

  <link defer rel="stylesheet" href="/personal/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/personal//">
          
            
              <span class="font-weight-bold">Jue</span>
            
            
            Guo
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/personal/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/teaching/">courses
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">


  <div class="fixed-bottom-button">
    <a href="/personal/teaching/deeplearnig" class="btn-back-course">
      ← Deep Learning Course Page
    </a>
  </div>


<!-- Always apply separation lines (scoped to article) -->


  <!-- Numbered Headings CSS with Clearer and Colored Separation Lines -->
  <style type="text/css">
    /* Base reset for counters */
    body {
      counter-reset: h1-counter;
    }

    /* Heading 1 */
    article h1 {
      counter-increment: h1-counter;
      counter-reset: h2-counter;
      border-bottom: 4px solid #4CAF50; /* Green separation line */
      padding-bottom: 12px; /* Increased spacing for clarity */
      margin-bottom: 24px; /* Adjust margin to keep spacing */
    }

    article h1::before {
      content: counter(h1-counter) ". ";
    }

    /* Heading 2 */
    article h2 {
      counter-increment: h2-counter;
      counter-reset: h3-counter;
      border-bottom: 3px solid #2196F3; /* Blue separation line */
      padding-bottom: 10px; /* Adjusted spacing */
      margin-bottom: 20px; /* Adjust margin to keep spacing */
    }

    article h2::before {
      content: counter(h1-counter) "." counter(h2-counter) " ";
    }

    /* Heading 3 */
    article h3 {
      counter-increment: h3-counter;
      counter-reset: h4-counter;
      border-bottom: 3px dashed #FFC107; /* Yellow dashed line */
      padding-bottom: 8px; /* Adjusted spacing */
      margin-bottom: 16px; /* Adjust margin to keep spacing */
    }

    article h3::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) " ";
    }

    /* Heading 4 */
    article h4 {
      counter-increment: h4-counter;
      counter-reset: h5-counter;
      border-bottom: 2px dotted #9C27B0; /* Purple dotted line */
      padding-bottom: 6px; /* Adjusted spacing */
      margin-bottom: 12px; /* Adjust margin to keep spacing */
    }

    article h4::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) " ";
    }

    /* Heading 5 */
    article h5 {
      counter-increment: h5-counter;
      border-bottom: 2px solid #FF5722; /* Orange solid line */
      padding-bottom: 4px; /* Adjusted spacing */
      margin-bottom: 8px; /* Adjust margin to keep spacing */
    }

    article h5::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) "." counter(h5-counter) " ";
    }
  </style>



<div class="post">
  <article>
    <header class="post-header">
      <h1 class="post-title">Recurrent Neural Networks</h1>
      <p class="post-description"></p>
    </header>

    <p>Models like linear/ logistic regression, multilayer perceptrons (MLPs) and convolutional neural networks (CNNs) operate on fixed-length input (tabular or image data) without sequential structure. <em><u>What is a sequential data?</u></em> Tasks like video analysis, time-series prediction, image captioning, speech synthesis, and translation involve sequentially structured inputs and outputs, requiring specialized models.</p>

<h2 id="introduction">Introduction</h2>
<p>RNNs are designed to capture the dynamics of sequences through <em>recurrent connections</em> that pass information across adjacent time steps. Recurrent neural networks are <em>unrolled</em> across time steps (or sequence steps), with the <em>same</em> underlying parameters applied at each step.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="https://d2l.ai/_images/unfolded-rnn.svg" sizes="95vw"></source>
    
    <img src="https://d2l.ai/_images/unfolded-rnn.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px; /* Fixed maximum width */
          
          height: auto; /* Maintain aspect ratio */
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>
    </div>
</div>
<div class="caption">
    RNNs can be visualized as feedforward neural networks where parameters are shared across time steps. On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously.
</div>

<p>RNNs have achieved breakthroughs in tasks like handwritting recognition, translation, and medical diagosis but have been partly replaced by Transformer models.</p>

<h2 id="working-with-sequences">Working with Sequences</h2>

<p><strong>Shift in Perspective</strong>  Traditional models work with a single feature vector \(\mathbf{x} \in \mathbb{R}^{d}\). Sequence models handle ordered lists of feature vectors: \(\mathbf{x}_{1}, \ldots, \mathbf{x}_{T}\), where each vector \(\mathbf{x}_{t} \in \mathbb{R}^{d}\) is indexed by time step \(t \in \mathbb{Z}^{+}\).</p>

<p><strong>Sequential Data</strong>  Some datasets consist of one long sequence (e.g., climate sensor data), sampled into subsequences of length \(T\).<br>
More commonly, data arrive as multiple sequences (e.g., documents, patient stays), where each sequence has its own length \(T_{i}\).<br>
Unlike independent feature vectors, elements within a sequence are dependent:</p>
<ul>
  <li>Later words in a document depend on earlier ones.</li>
  <li>A patient’s medication depends on prior events.</li>
</ul>

<p><strong>Sequential Dependence</strong>  Sequences reflect patterns that make auto-fill features and predictions possible.<br>
Sequences are modeled as samples from a fixed underlying distribution over entire sequences, \(P(X_{1}, \dots, X_{T})\), rather than assuming independence or stationarity.</p>

<p><strong>Examples of Sequential Tasks</strong></p>
<ol>
  <li>
<strong>Fixed input to fixed target</strong>: Predict a label \(y\) from a sequence (e.g., sentiment classification).</li>
  <li>
<strong>Fixed input to sequential target</strong>: Predict \((y_{1}, \ldots, y_{T})\) from an input (e.g., image captioning).</li>
  <li>
<strong>Sequential input to sequential target</strong>:
    <ul>
      <li>
<em>Aligned</em>: Input at each step aligns with the target (e.g., part-of-speech tagging).</li>
      <li>
<em>Unaligned</em>: No step-wise correspondence (e.g., machine translation).</li>
    </ul>
  </li>
</ol>

<p><strong>Sequence Modeling</strong> The simplest task is <strong>unsupervised density modeling</strong>:  Estimate \(p(\mathbf{x}_{1}, \ldots, \mathbf{x}_{T})\), the likelihood of a sequence, where the probability reflects the joint likelihood of the sequence elements based on their relationships. This is useful for understanding and generating sequences.</p>

<h3 id="autoregressive-models">Autoregressive Models</h3>

<p><strong>Sequence Data</strong>  Autoregressive models analyze sequentially structured data. Consider stock prices like those in the FTSE 100 index: \(x_t, x_{t-1}, \ldots, x_1,\) where each \(x_t\) represents the price at time \(t\). The goal is to predict the next value \(x_t\) based on its history.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="https://d2l.ai/_images/ftse100-480.webp 480w,https://d2l.ai/_images/ftse100-800.webp 800w,https://d2l.ai/_images/ftse100-1400.webp 1400w," type="image/webp" sizes="95vw"></source>
    
    <img src="https://d2l.ai/_images/ftse100.png" class="img-fluid rounded" style="
        
          
            max-width: 500px; /* Fixed maximum width */
          
          height: auto; /* Maintain aspect ratio */
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>
    </div>
</div>
<div class="caption">
FTSE 100 index over about 30 years.
</div>

<p><strong>Modeling Sequential Data</strong> Given a sequence, the trader wants to estimate the conditional distribution:</p>

\[P(x_t \mid x_{t-1}, \ldots, x_1),\]

<p>focusing on key statistics like the expected value:</p>

\[\mathbb{E}[x_t \mid x_{t-1}, \ldots, x_1].\]

<p>Autoregressive models perform this task by regressing \(x_t\) onto previous values \(x_{t-1}, \ldots, x_1\). However, the challenge lies in the <em>variable input size</em>, as the number of inputs grows with \(t\).</p>

<p><strong>Strategies for Fixed-Length Inputs</strong></p>
<ol>
  <li>
    <p><strong>Windowing</strong>: Instead of using the full history, consider only a window of size \(\tau\):</p>

\[x_{t-1}, \ldots, x_{t-\tau}.\]

    <p>This reduces the number of inputs to a fixed size for \(t &gt; \tau\), enabling the use of models like linear regression or neural networks.</p>
  </li>
  <li>
    <p><strong>Latent State Representations</strong>: Maintain a latent summary \(h_t\) of past observations.</p>

    <ul>
      <li>
        <p>Predict \(x_t\) using: \(\hat{x}_t = P(x_t \mid h_t).\)</p>
      </li>
      <li>
        <p>Update the latent state with: \(h_t = g(h_{t-1}, x_{t-1}).\)</p>
      </li>
    </ul>

    <p>This approach creates <em>latent autoregressive models</em>, as \(h_t\) is not directly observed.</p>
  </li>
</ol>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="https://d2l.ai/_images/sequence-model.svg" sizes="95vw"></source>
    
    <img src="https://d2l.ai/_images/sequence-model.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px; /* Fixed maximum width */
          
          height: auto; /* Maintain aspect ratio */
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>
    </div>
</div>
<div class="caption">
A latent autoregressive model.
</div>

<p><strong>Training Data and Stationarity</strong> Training data is often constructed by sampling fixed-length windows from historical data. Even though the specific values \(x_t\) may change, the underlying generation process is often assumed to be stationary, meaning the dynamics of \(P(x_t \mid x_{t-1}, \ldots, x_1)\) remain consistent over time.</p>

<h3 id="sequence-models">Sequence Models</h3>

<p><strong>Joint Probability of Sequences</strong> Sequence models estimate the joint probability of an entire sequence, typically for data composed of discrete <em>tokens</em> like words. These models are often referred to as <em>language models</em> when dealing with natural language data. Language models are particularly useful for:</p>

<ul>
  <li>Evaluating the likelihood of sequences (e.g., comparing naturalness of sentences in machine translation or speech recognition).</li>
  <li>Sampling sequences and optimizing for the most likely outcomes.</li>
</ul>

<p>The joint probability of a sequence \(P(x_1, \ldots, x_T)\) can be decomposed using the chain rule of probability into conditional densities:</p>

\[P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1).\]

<p>For discrete signals, the model must act as a probabilistic classifier, outputting a distribution over the vocabulary for the next word based on the leftward context.</p>

<h4 id="markov-models">Markov Models</h4>
<p><img class="emoji" title=":label:" alt=":label:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3f7.png" height="20" width="20"><code class="language-plaintext highlighter-rouge">subsec_markov-models</code></p>

<p>Instead of conditioning on the entire history \(x_{t-1}, \ldots, x_1\), we may limit the context to the previous \(\tau\) time steps, i.e., \(x_{t-1}, \ldots, x_{t-\tau}\). This is known as the <em>Markov condition</em>, where the future is conditionally independent of the past given the recent history. When:</p>

<ul>
  <li>\(\tau = 1\), it is called a <em>first-order Markov model</em>.</li>
  <li>\(\tau = k\), it is called a <em>k\textsuperscript{th}-order Markov model</em>.</li>
</ul>

<p>For a first-order Markov model, the factorization simplifies to:</p>

\[P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}).\]

<p>Markov models are practical even if the condition is not strictly true. With real-world text, additional context improves predictions, but the marginal benefits diminish as the context length increases. Hence, many models rely on the Markov assumption for computational efficiency.</p>

<p>For discrete data like language, Markov models estimate \(P(x_t \mid x_{t-1})\) via relative frequency counts and efficiently compute the most likely sequence using dynamic programming.</p>

<h4 id="the-order-of-decoding">The Order of Decoding</h4>

<p>The factorization of a sequence can follow any order (e.g., left-to-right or right-to-left):</p>

<ol>
  <li>
<strong>Left-to-right (natural reading order)</strong>:
\(P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1).\)</li>
  <li>
<strong>Right-to-left</strong>:
\(P(x_1, \ldots, x_T) = P(x_T) \prod_{t=T-1}^1 P(x_t \mid x_{t+1}, \ldots, x_T).\)</li>
</ol>

<p>While all orders are mathematically valid, left-to-right decoding is preferred for several reasons:</p>
<ul>
  <li>
<strong>Naturalness</strong>: Matches the human reading process.</li>
  <li>
<strong>Incrementality</strong>: Probabilities over sequences can be extended by multiplying by the conditional probability of the next token:
\(P(x_{t+1}, \ldots, x_1) = P(x_{t}, \ldots, x_1) \cdot P(x_{t+1} \mid x_{t}, \ldots, x_1).\)</li>
  <li>
<strong>Predictive Power</strong>: Predicting adjacent tokens is often more feasible than predicting tokens at arbitrary positions.</li>
  <li>
<strong>Causal Relationships</strong>: Forward predictions (e.g., \(P(x_{t+1} \mid x_t)\)) often align with causality, whereas reverse predictions (\(P(x_t \mid x_{t+1})\)) are generally infeasible.</li>
</ul>

<p>For instance, in causal systems, forward predictions might follow:</p>

\[x_{t+1} = f(x_t) + \epsilon,\]

<p>where \(\epsilon\) represents additive noise. The reverse relationship generally does not hold.</p>

<p>For a more detailed exploration of causality and sequence modeling, refer to :cite:<code class="language-plaintext highlighter-rouge">Peters.Janzing.Scholkopf.2017</code>.</p>



    

    
  </article>
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      Jue
      
      Guo. 
      
      
        Last updated: November 23, 2024.
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/personal/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/personal/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/personal/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/personal/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/personal/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/personal/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/personal/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  



    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/personal/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
