<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Recurrent Neural Network | Jue Guo
    
  
</title>
<meta name="author" content="Jue Guo">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/personal/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/personal/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/personal/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/personal/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/personal/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/personal/assets/courses/deeplearning/rnn/vinilla_rnn/">

<!-- Dark Mode -->
<script src="/personal/assets/js/theme.js?5ce9accf63efc46cd59e47ccb47fd172"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->





  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/personal//">
          
            
              <span class="font-weight-bold">Jue</span>
            
            
            Guo
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/personal/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/teaching/">courses
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>

    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- main content area -->
            <div class="col-sm-9">
              

<!-- Always apply separation lines (scoped to article) -->

  <style type="text/css">
    /* Heading 1 */
    .post > article h1 {
      border-bottom: 4px solid #4CAF50; /* Green separation line */
      padding-bottom: 12px;
      margin-top: 40px; /* Increased spacing above */
      margin-bottom: 32px; /* Increased spacing below */
    }

    /* Heading 2 */
    .post > article h2 {
      border-bottom: 3px solid #2196F3; /* Blue separation line */
      padding-bottom: 10px;
      margin-top: 36px; /* Increased spacing above */
      margin-bottom: 28px; /* Increased spacing below */
    }

    /* Heading 3 */
    .post > article h3 {
      border-bottom: 3px dashed #FFC107; /* Yellow dashed line */
      padding-bottom: 8px;
      margin-top: 32px; /* Increased spacing above */
      margin-bottom: 24px; /* Increased spacing below */
    }

    /* Heading 4 */
    .post > article h4 {
      border-bottom: 2px dotted #9C27B0; /* Purple dotted line */
      padding-bottom: 6px;
      margin-top: 28px; /* Increased spacing above */
      margin-bottom: 20px; /* Increased spacing below */
    }

    /* Heading 5 */
    .post > article h5 {
      border-bottom: 2px solid #FF5722; /* Orange solid line */
      padding-bottom: 4px;
      margin-top: 24px; /* Increased spacing above */
      margin-bottom: 16px; /* Increased spacing below */
    }
  </style>



  <!-- Numbered Headings CSS -->
  <style type="text/css">
    /* Base reset for counters */
    body {
      counter-reset: h1-counter 4;
    }

    /* Heading 1 */
    .post > article h1 {
      counter-increment: h1-counter;
      counter-reset: h2-counter;
      
      /* Show h1 number */
      
    }

    
    .post > article h1::before {
      content: counter(h1-counter) ". ";
    }
    

    /* Heading 2 */
    .post > article h2 {
      counter-increment: h2-counter;
      counter-reset: h3-counter;
    }

    .post > article h2::before {
      content: counter(h1-counter) "." counter(h2-counter) " ";
    }

    /* Heading 3 */
    .post > article h3 {
      counter-increment: h3-counter;
      counter-reset: h4-counter;
    }

    .post > article h3::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) " ";
    }

    /* Heading 4 */
    .post > article h4 {
      counter-increment: h4-counter;
      counter-reset: h5-counter;
    }

    .post > article h4::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) " ";
    }

    /* Heading 5 */
    .post > article h5 {
      counter-increment: h5-counter;
    }

    .post > article h5::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) "." counter(h5-counter) " ";
    }
  </style>



<div class="post">
  <article>
    <header class="post-header">
      <h1 class="post-title">Recurrent Neural Network</h1>
      <p class="post-description"></p>
    </header>

    <p><strong>Extending Beyond \(n\)-grams</strong> In <a href="/personal/assets/courses/deeplearning/rnn/markov">Markov Models and n-grams</a>, we introduced Markov models and \(n\)-grams for language modeling, where the conditional probability of a token \(x_t\) at time step \(t\) depends only on the previous \(n-1\) tokens. To account for tokens occurring earlier than \(t-(n-1)\), we could increase \(n\). However, this approach comes at a \(\textcolor{red}{\text{significant cost}}\)
—model parameters grow exponentially with \(n\), as we need to store \(|\mathcal{V}|^{n}\) parameters for a vocabulary set \(\mathcal{V}\). Instead of modeling:</p>

\[P(x_t \mid x_{t-1}, \ldots, x_{t-n+1}),\]

<p>we can introduce a <em>latent variable model</em>:</p>

\[P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1}),\]

<p>where \(h_{t-1}\) is a <em>hidden state</em> that captures all relevant sequence information up to time step \(t-1\).</p>

<p><strong>Hidden State Representation</strong> The hidden state \(h_t\) at any time step \(t\) is computed as a function of the current input \(x_t\) and the previous hidden state \(h_{t-1}\):</p>

<p>\begin{equation}
\label{eq:eq_ht_xt} h_t = f(x_t, h_{t-1}). 
\end{equation}</p>

<p>For a sufficiently expressive function \(f\) in eq.\eqref{eq:eq_ht_xt}, the latent variable model can, in principle, capture all information observed so far, eliminating the need for approximation. However, storing all past information can make computation and storage infeasible.</p>

<p><strong>Distinction Between Hidden Layers and Hidden States</strong> <em>Hidden layers</em> refer to intermediate layers in a neural network that are not directly exposed to input or output. In contrast, <em>hidden states</em> in the context of sequence modeling are technically inputs to the computation at any given time step. Hidden states encapsulate information from past time steps and are crucial for modeling sequences.</p>

<p><strong>Recurrent Neural Network</strong> Recurrent Neural Networks (RNNs) are a class of neural networks designed specifically for sequential data, incorporating <em>hidden states</em> to capture dependencies over time. Before delving into the RNN architecture, let us revisit the foundational concept of Multilayer Perceptrons (MLPs).</p>

<h2 id="neural-networks-without-hidden-states">Neural Networks without Hidden States</h2>
<p>Let’s take a look at an MLP with a single hidden layer.  Let the hidden layer’s activation function be \(\phi\). Given a minibatch of examples \(\mathbf{X} \in \mathbb{R}^{n \times d}\) with batch size \(n\) and \(d\) inputs, the hidden layer output \(\mathbf{H} \in \mathbb{R}^{n \times h}\) is calculated as:</p>

<p>\begin{equation}\label{eq:rnn_h_with_state}
\mathbf{H}=\phi\left(\mathbf{X} \mathbf{W}_{\mathrm{xh}}+\mathbf{b}_\mathrm{h}\right)
\end{equation}</p>

<p>In eq.\eqref{eq:rnn_h_with_state}, we have:</p>
<ul>
  <li>Weight parameter \(\mathbf{W}_{\textrm{xh}} \in \mathbb{R}^{d \times h}\),</li>
  <li>Bias parameter \(\mathbf{b}_\textrm{h} \in \mathbb{R}^{1 \times h}\),</li>
  <li>Number of hidden units \(h\) for the hidden layer.</li>
</ul>

<p>The hidden layer output \(\mathbf{H}\) is then used as input to the output layer:</p>

<p>\begin{equation}
\mathbf{O} = \mathbf{H} \mathbf{W}_{\textrm{hq}} + \mathbf{b}_\textrm{q},
\end{equation}</p>

<p>where:</p>
<ul>
  <li>\(\mathbf{O} \in \mathbb{R}^{n \times q}\) is the output variable,</li>
  <li>\(\mathbf{W}_{\textrm{hq}} \in \mathbb{R}^{h \times q}\) is the weight parameter,</li>
  <li>\(\mathbf{b}_\textrm{q} \in \mathbb{R}^{1 \times q}\) is the bias parameter of the output layer.</li>
</ul>

<p>For classification problems, we can apply \(\mathrm{softmax}(\mathbf{O})\) to compute the probability distribution of the output categories.</p>

<h2 id="recurrent-neural-networks-with-hidden-states">Recurrent Neural Networks with Hidden States</h2>
<p>RNNs differ from MLPs by incorporating <em>hidden states</em>. Assume we have:</p>
<ul>
  <li>A minibatch of inputs \(\mathbf{X}_t \in \mathbb{R}^{n \times d}\) at time step \(t\),</li>
  <li>Hidden state \(\mathbf{H}_t \in \mathbb{R}^{n \times h}\) for the same time step.</li>
</ul>

<p>Unlike MLPs, the hidden state \(\mathbf{H}_{t-1}\) from the previous time step is saved, and a new weight parameter \(\mathbf{W}_{\textrm{hh}} \in \mathbb{R}^{h \times h}\) describes how to use it in the current time step. Specifically, the hidden state for time step \(t\) is calculated as:</p>

<p>\begin{equation}
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{\textrm{xh}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hh}} + \mathbf{b}_\textrm{h}).
\label{rnn_h_with_state}
\end{equation}</p>

<p>In \eqref{rnn_h_with_state}, the term \(\mathbf{H}_{t-1} \mathbf{W}\_{\textrm{hh}}\) introduces recurrent computation. This allows \(\mathbf{H}_t\) to capture historical sequence information, making it a <em>hidden state</em>. Layers performing this recurrent computation are called <em>recurrent layers</em>.</p>

<p>The output layer computes the output for time step \(t\) as:</p>

<p>\begin{equation}
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{\textrm{hq}} + \mathbf{b}_\textrm{q}.
\end{equation}</p>

<p>The parameters of an RNN include:</p>
<ul>
  <li>Weights: \(\mathbf{W}_{\textrm{xh}} \in \mathbb{R}^{d \times h}, \mathbf{W}_{\textrm{hh}} \in \mathbb{R}^{h \times h}, \mathbf{W}_{\textrm{hq}} \in \mathbb{R}^{h \times q}\),</li>
  <li>Biases: \(\mathbf{b}_\textrm{h} \in \mathbb{R}^{1 \times h}, \mathbf{b}_\textrm{q} \in \mathbb{R}^{1 \times q}\).</li>
</ul>

<p>Unlike MLPs, RNN parameters are shared across all time steps, meaning the model’s size does not increase with sequence length.</p>

<p><strong>RNN Computation at Ajacent Time Steps</strong> Fig.<a href="#rnn">1</a> illustrates an RNN across three adjacent time steps. At any time step \(t\):</p>
<ol>
  <li>Concatenate \(\mathbf{X}_t\) (current input) and \(\mathbf{H}_{t-1}\) (previous hidden state),</li>
  <li>Pass the concatenated result into a fully connected layer with activation \(\phi\) to compute \(\mathbf{H}_t\).</li>
</ol>

<p>The hidden state \(\mathbf{H}_t\) is used for:</p>
<ul>
  <li>Computing \(\mathbf{H}_{t+1}\) in the next time step,</li>
  <li>Producing the output \(\mathbf{O}_t\).</li>
</ul>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="rnn1">
  <picture>
    <img src="https://d2l.ai/_images/rnn.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 1: An RNN with a hidden state.
    </figcaption>
  
</figure>

    </div>
</div>

<p><strong>Efficient Computation with Concatenation</strong> We can compute \(\mathbf{X}_t \mathbf{W}_{\textrm{xh}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hh}}\) equivalently as a single matrix multiplication. Concatenate \(\mathbf{X}_t\) and \(\mathbf{H}_{t-1}\), and multiply it by the concatenated weights \([\mathbf{W}_{\textrm{xh}}, \mathbf{W}_{\textrm{hh}}]\). The following Python snippet demonstrates this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Shape: (3, 1)
</span><span class="n">W_xh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Shape: (1, 4)
</span><span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Shape: (3, 4)
</span><span class="n">W_hh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Shape: (4, 4)
</span>
<span class="c1"># Compute hidden state
</span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W_xh</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W_hh</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Output shape: (3, 4)
</span></code></pre></div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor<span class="o">([[</span> 1.2526,  0.0580, <span class="nt">-3</span>.3460, <span class="nt">-0</span>.2519],
        <span class="o">[</span><span class="nt">-1</span>.3064,  1.4132, <span class="nt">-0</span>.1435,  0.3482],
        <span class="o">[</span> 3.1495,  0.8172,  1.5167, <span class="nt">-0</span>.9038]]<span class="o">)</span>
</code></pre></div></div>
<p><em>equivalent to</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">H</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">W_xh</span><span class="p">,</span> <span class="n">W_hh</span><span class="p">),</span> <span class="mi">0</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor<span class="o">([[</span> 1.2526,  0.0580, <span class="nt">-3</span>.3460, <span class="nt">-0</span>.2519],
        <span class="o">[</span><span class="nt">-1</span>.3064,  1.4132, <span class="nt">-0</span>.1435,  0.3482],
        <span class="o">[</span> 3.1495,  0.8172,  1.5167, <span class="nt">-0</span>.9038]]<span class="o">)</span>
</code></pre></div></div>
<h2 id="rnn-based-character-level-language-models">RNN-Based Character-Level Language Models</h2>

<p>For language modeling as discussed before, the goal is to predict the next token based on the current and previous tokens. To achieve this, the original sequence is shifted by one token to create the targets (labels). Neural networks for language modeling, as proposed by <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="external nofollow noopener" target="_blank">A Neural Probabilistic Language Model</a>, can use RNNs to accomplish this task. Here’s how an RNN processes the sequence:</p>

<p><strong>Key Idea</strong> For simplicity, we tokenize text into characters rather than words, building a <strong>character-level language model</strong>. For instance, consider the text sequence <code class="language-plaintext highlighter-rouge">"machine"</code>:</p>
<ul>
  <li>
<strong>Input Sequence</strong>: <code class="language-plaintext highlighter-rouge">"machin"</code>
</li>
  <li>
<strong>Target Sequence</strong>: <code class="language-plaintext highlighter-rouge">"achine"</code>
</li>
</ul>

<p><strong>RNN Processing</strong> Fig.<a href="#fig_rnn_train">2</a> demonstrates how RNNs predict the next character based on the current and previous characters in the sequence.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        
        <figure id="fig_rnn_train2">
  <picture>
    <img src="https://d2l.ai/_images/rnn-train.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 2: A character-level language model based on the RNN. The input and target sequences are 'machin' and 'achine', respectively.
    </figcaption>
  
</figure>

    </div>
</div>

<p>At each time step:</p>
<ul>
  <li>The RNN processes the input token, updating its <strong>hidden state</strong>.</li>
  <li>The output at each time step is used to predict the next character.</li>
</ul>

<p>For example:</p>
<ul>
  <li>
<strong>Input at Time Step 3</strong>: <code class="language-plaintext highlighter-rouge">"m", "a", "c"</code>
</li>
  <li>
<strong>Target</strong>: <code class="language-plaintext highlighter-rouge">"h"</code>
</li>
  <li>
<strong>Output</strong>: \(\mathbf{O}_3\), determined by the sequence <code class="language-plaintext highlighter-rouge">"m", "a", "c"</code>. The cross-entropy loss is computed between the predicted distribution and the actual target (<code class="language-plaintext highlighter-rouge">"h"</code>).</li>
</ul>

<p><strong>Practical Considerations</strong> Each token is represented as a \(d\)-dimensional vector. With a batch size \(n &gt; 1\), the input at time step \(t\), \(\mathbf{X}_t\), is an \(n \times d\) matrix, consistent with the description in <a href="#recurrent-neural-networks-with-hidden-states">Recurrent Neural Networks with Hidden States</a>.</p>

<h3 id="gradient-clipping">Gradient Clipping</h3>
<p>While you are already used to thinking of neural networks as “deep” in the sense that many layers separate the input and output even within a single time step, the length of the sequence introduces a <em>new notion of depth</em>.</p>
<ul>
  <li>In addition to the passing through the network in the input-to-output direction, inputs at the first time step must pass through a chain of \(T\) layers along the time steps in order to influence the output of the model at the final time step.</li>
  <li>Taking the backwards view, in each iteration, we backpropagate gradient through time, resulting in a chain of matrix-products of length \(\mathcal{O}(T)\). This can result in numerical instability, causing gradients either to explode or vanish, depending on the properties of the weight matrices.</li>
</ul>

<p>Dealing with vanishing and exploding gradients is a fundamental problem when designing RNNs and has inspired some of the biggest advances in modern neural network architectures. Later, we will talk about specialized architectures that were designed in hopes of mitigating the vanishing gradient problem. However, even modern RNNs often suffer from exploding gradients. One inelegant but ubiquitous solution is to simply clip the gradients forcing the resulting <em>“clipped”</em> gradients to take smaller values.</p>

<p><strong>Gradient Descent and Objective Changes</strong> In gradient descent, the parameter vector \(\mathbf{x}\) is updated as:</p>

\[\mathbf{x} \leftarrow \mathbf{x} - \eta \mathbf{g},\]

<p>where:</p>
<ul>
  <li>\(\eta &gt; 0\) is the learning rate, controlling the step size,</li>
  <li>\(\mathbf{g}\) is the gradient of \(f\) at \(\mathbf{x}\), indicating the direction of steepest ascent.</li>
</ul>

\[|f(\mathbf{x})-f(\mathbf{y})| \leq L\|\mathbf{x}-\mathbf{y}\|\]

<p>If the objective function \(f\) is <strong>Lipschitz continuous</strong> with constant \(L\), then:</p>

\[|f(\mathbf{x}) - f(\mathbf{y})| \leq L \|\mathbf{x} - \mathbf{y}\|,\]

<p>for any \(\mathbf{x}\) and \(\mathbf{y}\). Applying this to a gradient update:</p>

\[\mathbf{y} = \mathbf{x} - \eta \mathbf{g},\]

<p>the change in the objective function is bounded by:</p>

\[|f(\mathbf{x}) - f(\mathbf{x} - \eta \mathbf{g})| \leq L \eta \|\mathbf{g}\|.\]

<p>Thus, the change in the objective depends on the gradient norm \(\|\mathbf{g}\|\), the learning rate \(\eta\), and \(L\). Large gradient norms can cause excessively large changes, leading to instability in training.</p>

<ol>
  <li>
    <p><strong>Controlled Updates</strong>: The Lipschitz constant \(L\), the learning rate \(\eta\), and the gradient norm \(\|\mathbf{g}\|\) together determine the maximum change in the objective.</p>
  </li>
  <li>
    <p><strong>Stability</strong>: Large gradient norms \(\|\mathbf{g}\|\) can cause excessively large changes in \(f(\mathbf{x})\), potentially destabilizing training. This is why gradient clipping or smaller \(\eta\) may be necessary in practice.</p>
  </li>
</ol>

<p><strong>Gradient Clipping</strong> To prevent exploding gradients, the gradient clipping heuristic modifies the gradient \(\mathbf{g}\) as follows:</p>

\[\mathbf{g} \leftarrow \min \left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}\]

<p>where \(\theta\) is a predefined threshold. This ensures the gradient norm does not exceed \(\theta\) while preserving the direction of \(\mathbf{g}\).</p>

<p><strong>Benefits and Limitations</strong> Gradient clipping:</p>
<ul>
  <li>Limits the gradient norm, improving stability.</li>
  <li>Reduces the influence of individual minibatches or samples, enhancing robustness.
However, it introduces a bias since the true gradient is not always followed, making analytical
reasoning about side effects difficult. Despite being a heuristic, gradient clipping is widely
adopted in RNN implementations.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">clip_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Clip gradients of a model to a maximum norm theta.</span><span class="sh">"""</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">]</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">norm</span> <span class="o">&gt;</span> <span class="n">theta</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">[:]</span> <span class="o">*=</span> <span class="n">theta</span> <span class="o">/</span> <span class="n">norm</span>
</code></pre></div></div>
<h2 id="backpropagation-through-time">Backpropagation Through Time</h2>
<p>We mentioned that gradient clipping is vital for preventing the occasional massive gradients from destabilizing training. We hinted that the exploding gradients stem from backpropagating across long sequences. Before introducing a slew of modern RNN architectures, let’s take a closer look at how <em>backpropagation</em> works in sequence models in mathematical detail. Hopefully, this discussion will bring some precision to the notion of <em>vanishing</em> and <em>exploding</em> gradients.</p>

<p>Applying backpropagation in RNNs is called <em>backpropagation through time</em> (BPTT) . This procedure requires us to expand (or unroll) the computational graph of an RNN one time step at a time. The unrolled RNN is essentially a feedforward neural network with the special property that the same parameters are repeated throughout the unrolled network, appearing at each time step. Then, just as in any feedforward neural network, we can apply the chain rule, backpropagating gradients through the unrolled net.</p>

<h3 id="analysis-of-gradients-in-rnns">Analysis of Gradients in RNNs</h3>
<p>We start with a simplified model of how an RNN works. In this simplified model, let \(h_t\) be the hidden state, \(x_t\) the input, and \(o_t\) the output at time step \(t\). Recall that the input and the hidden state can be concatenated before being multiplied by one weight variable in the hidden layer. Let \(w_\textrm{h}\) and \(w_\textrm{o}\) represent the weights of the hidden and output layers, respectively. Then, we can represent the hidden states and outputs as:</p>

\[\begin{aligned}
h_t &amp;= f(x_t, h_{t-1}, w_\textrm{h}), \\
o_t &amp;= g(h_t, w_\textrm{o}),
\end{aligned}\]

<p>where \(f\) and \(g\) are transformations of the hidden layer and output layer, respectively.</p>

<p>The objective function \(L\) over \(T\) time steps is defined as:</p>

\[L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_\textrm{h}, w_\textrm{o}) = \frac{1}{T} \sum_{t=1}^T l(y_t, o_t).\]

<p><strong>Gradient Computation</strong> Using the chain rule, the gradient with respect to \(w_\textrm{h}\) is:</p>

\[\frac{\partial L}{\partial w_\textrm{h}} = \frac{1}{T} \sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_\textrm{h}} = \frac{1}{T} \sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_\textrm{o})}{\partial h_t} \frac{\partial h_t}{\partial w_\textrm{h}}.\]

<p>The term \(\frac{\partial h_t}{\partial w_\textrm{h}}\) depends on both \(h_{t-1}\) and \(w_\textrm{h}\), requiring recursive computation. Using the chain rule, this can be expressed as:</p>

\[\frac{\partial h_t}{\partial w_\textrm{h}} = \frac{\partial f(x_t, h_{t-1}, w_\textrm{h})}{\partial w_\textrm{h}} + \frac{\partial f(x_t, h_{t-1}, w_\textrm{h})}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_\textrm{h}}.\]

<p>This recursive structure leads to challenges when sequences are long, as gradients can either vanish or explode.</p>

<h3 id="backpropagation-through-time-in-detail">Backpropagation Through Time in Detail</h3>

<p>Let \(\mathbf{h}_t\), \(\mathbf{x}_t\), and \(y_t\) represent the hidden state, input, and target at time step \(t\). For simplicity, assume the activation function is an identity mapping (\(\phi(x) = x\)). The hidden state and output are computed as:</p>

\[\begin{aligned}
\mathbf{h}_t &amp;= \mathbf{W}_\textrm{hx} \mathbf{x}_t + \mathbf{W}_\textrm{hh} \mathbf{h}_{t-1}, \\
\mathbf{o}_t &amp;= \mathbf{W}_\textrm{qh} \mathbf{h}_t.
\end{aligned}\]

<p>The objective function over \(T\) time steps is:</p>

\[L = \frac{1}{T} \sum_{t=1}^T l(\mathbf{o}_t, y_t).\]

<p><strong>Gradient Computation</strong> For gradients with respect to the output layer weights \(\mathbf{W}_\textrm{qh}\):</p>

\[\frac{\partial L}{\partial \mathbf{W}_\textrm{qh}} = \sum_{t=1}^T \frac{\partial L}{\partial \mathbf{o}_t} \mathbf{h}_t^\top,\]

<p>where \(\frac{\partial L}{\partial \mathbf{o}_t}\) can be computed directly from the loss.</p>

<p>For the hidden layer at time step \(T\):</p>

\[\frac{\partial L}{\partial \mathbf{h}_T} = \mathbf{W}_\textrm{qh}^\top \frac{\partial L}{\partial \mathbf{o}_T}.\]

<p>For earlier time steps \(t &lt; T\):</p>

\[\frac{\partial L}{\partial \mathbf{h}_t} = \mathbf{W}_\textrm{hh}^\top \frac{\partial L}{\partial \mathbf{h}_{t+1}} + \mathbf{W}_\textrm{qh}^\top \frac{\partial L}{\partial \mathbf{o}_t}.\]

<p>This recurrence highlights the issue of vanishing or exploding gradients due to repeated multiplication with \(\mathbf{W}_\textrm{hh}\).</p>

<h3 id="visualization-of-gradient-strategies">Visualization of Gradient Strategies</h3>

<p>Below, Fig.<a href="#fig_truncated_bptt">1</a> compares different gradient computation strategies:</p>

<div class="row mt-3">
    
    <div class="col-sm mt-3 mt-md-0">
        <figure id="fig_truncated_bptt3">
  <picture>
    <img src="https://d2l.ai/_images/truncated-bptt.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
    <figcaption class="caption">
      Figure 3: Comparing strategies for computing gradients in RNNs. From top to bottom: randomized truncation, regular truncation, and full computation.
    </figcaption>
  
</figure>

    </div>
</div>

<ol>
  <li>Randomized truncation partitions text into segments of varying lengths.</li>
  <li>Regular truncation breaks the text into fixed-length subsequences.</li>
  <li>Full computation considers the entire sequence, which is computationally infeasible.</li>
</ol>

<h2 id="summary">Summary</h2>

<p>Backpropagation through time applies backpropagation to sequence models. Key takeaways:</p>
<ul>
  <li>Truncation methods improve computational feasibility and numerical stability.</li>
  <li>Long sequences amplify challenges with vanishing or exploding gradients.</li>
  <li>Efficient computation requires caching intermediate values.</li>
</ul>



    

    
  </article>
</div>

            </div>
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top">
                <!-- The TOC is injected by JS or jekyll-toc plugin -->
              </nav>
            </div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      Jue
      
      Guo. 
      
      
        Last updated: December 29, 2024.
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/personal/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/personal/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/personal/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/personal/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/personal/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/personal/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/personal/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  

    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/personal/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    


    <!-- Inline JS snippet: always show the Back link if page.back_link is set,
     either in #toc-sidebar or, if that doesn't exist, in the navbar. -->
    <script>
      window.addEventListener("load", function() {
        // If page.back_link is present, show a back button
        
        // If page.back_text is missing, default to "Back"
        

        // We'll build two versions of the snippet:
        // 1) A <div> version for the TOC sidebar (no bullet marker)
        // 2) A <li> version for the navbar fallback

        const tocHTML = `
          <div class="back-link" style="margin-top:1rem;">
            <a href="/personal/teaching/deeplearnig" style="text-decoration: none; display: inline-flex; align-items: center;">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                  viewBox="0 0 24 24" style="margin-right:0.4rem;">
                <path d="M0 0h24v24H0z" fill="none"/>
                <path d="M8 7v4L2 6l6-5v4h5a8 8 0 1 1 0 16H4v-2h9a6 6 0 1 0 0-12H8z"/>
              </svg>
              <span>Deep Learning</span>
            </a>
          </div>
        `;

        const navHTML = `
          <li class="nav-item back-link" style=""display: inline-flex; align-items: center;"">
            <a class="nav-link" href="/personal/teaching/deeplearnig" style="display: inline-flex; align-items: center;">
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                  viewBox="0 0 24 24" style="margin-right:0.4rem;">
                <path d="M0 0h24v24H0z" fill="none"/>
                <path d="M8 7v4L2 6l6-5v4h5a8 8 0 1 1 0 16H4v-2h9a6 6 0 1 0 0-12H8z"/>
              </svg>
              <span>Deep Learning</span>
            </a>
          </li>
        `;

        // 1) If #toc-sidebar exists, we insert the "tocHTML" (a <div>)
        let tocSidebar = document.getElementById("toc-sidebar");
        if (tocSidebar) {
          tocSidebar.insertAdjacentHTML("beforeend", tocHTML);
        } else {
          // 2) Otherwise, we append the "navHTML" (<li>) to the navbar
          //    so user sees the link in the top nav
          let navbarUl = document.querySelector("#navbarNav ul.navbar-nav");
          if (navbarUl) {
            navbarUl.insertAdjacentHTML("beforeend", navHTML);
          }
        }
        
      });
      </script>

    <!-- Inline JS to number the TOC items, handling userOffset for ANY > 2 -->
    <script>
      window.addEventListener("load", function() {
        // If page.start_h1_number is not set, or not an integer,
        // treat it as "no offset" => start at 1, 2, 3
        

        const parsedOffset = parseInt("4", 10);
        const hasUserOffset = !isNaN(parsedOffset);
        
        const rootUl = document.querySelector("#toc-sidebar > ul");
        if (!rootUl) return;

        function numberList(ul, prefix = "", startIndex = 1) {
          const liElements = ul.querySelectorAll(":scope > li");
          let idx = startIndex;

          liElements.forEach(li => {
            let label;

            if (!prefix) {
              // Top-level items
              if (!hasUserOffset) {
                // If there's no valid user offset, use 1,2,3...
                label = String(idx);
              } else {
                // If user provided offset => (parsedOffset + 1).1, (parsedOffset + 1).2, etc.
                label = `${parsedOffset + 1}.${idx}`;
              }
            } else {
              // Nested => prefix + "." + idx
              label = `${prefix}.${idx}`;
            }

            const link = li.querySelector(":scope > a");
            if (link) {
              link.textContent = label + " " + link.textContent;
            }

            const nested = li.querySelector(":scope > ul");
            if (nested) {
              numberList(nested, label, 1);
            }

            idx++;
          });
        }

        numberList(rootUl);
      });
    </script>
  </body>
</html>
