<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Markov Models and $$n$$-grams | Jue Guo
    
  
</title>
<meta name="author" content="Jue Guo">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/personal/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/personal/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/personal/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">


  <!-- Sidebar Table of Contents -->
  <link defer href="/personal/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet">


<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/personal/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/personal/assets/courses/deeplearning/rnn/markov/">

<!-- Dark Mode -->
<script src="/personal/assets/js/theme.js?5ce9accf63efc46cd59e47ccb47fd172"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->





  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/personal//">
          
            
              <span class="font-weight-bold">Jue</span>
            
            
            Guo
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/personal/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/teaching/">courses
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/personal/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>

    <!-- Content -->
    <div class="container mt-5" role="main">
      
        
          <div class="row">
            <!-- sidebar, which will move to the top on a small screen -->
            <div class="col-sm-3">
              <nav id="toc-sidebar" class="sticky-top"></nav>
            </div>
            <!-- main content area -->
            <div class="col-sm-9">


  <div class="fixed-bottom-button">
    <a href="/personal/teaching/deeplearnig" class="btn-back-course">
      ‚Üê Deep Learning Course Page
    </a>
  </div>


<!-- Always apply separation lines (scoped to article) -->

  <style type="text/css">
    /* Heading 1 */
    .post > article h1 {
      border-bottom: 4px solid #4CAF50; /* Green separation line */
      padding-bottom: 12px;
      margin-top: 40px; /* Increased spacing above */
      margin-bottom: 32px; /* Increased spacing below */
    }

    /* Heading 2 */
    .post > article h2 {
      border-bottom: 3px solid #2196F3; /* Blue separation line */
      padding-bottom: 10px;
      margin-top: 36px; /* Increased spacing above */
      margin-bottom: 28px; /* Increased spacing below */
    }

    /* Heading 3 */
    .post > article h3 {
      border-bottom: 3px dashed #FFC107; /* Yellow dashed line */
      padding-bottom: 8px;
      margin-top: 32px; /* Increased spacing above */
      margin-bottom: 24px; /* Increased spacing below */
    }

    /* Heading 4 */
    .post > article h4 {
      border-bottom: 2px dotted #9C27B0; /* Purple dotted line */
      padding-bottom: 6px;
      margin-top: 28px; /* Increased spacing above */
      margin-bottom: 20px; /* Increased spacing below */
    }

    /* Heading 5 */
    .post > article h5 {
      border-bottom: 2px solid #FF5722; /* Orange solid line */
      padding-bottom: 4px;
      margin-top: 24px; /* Increased spacing above */
      margin-bottom: 16px; /* Increased spacing below */
    }
  </style>



  <!-- Numbered Headings CSS -->
  <style type="text/css">
    /* Base reset for counters */
    body {
      counter-reset: h1-counter 3;
    }

    /* Heading 1 */
    .post > article h1 {
      counter-increment: h1-counter;
      counter-reset: h2-counter;
      
      /* Show h1 number */
      
    }

    
    .post > article h1::before {
      content: counter(h1-counter) ". ";
    }
    

    /* Heading 2 */
    .post > article h2 {
      counter-increment: h2-counter;
      counter-reset: h3-counter;
    }

    .post > article h2::before {
      content: counter(h1-counter) "." counter(h2-counter) " ";
    }

    /* Heading 3 */
    .post > article h3 {
      counter-increment: h3-counter;
      counter-reset: h4-counter;
    }

    .post > article h3::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) " ";
    }

    /* Heading 4 */
    .post > article h4 {
      counter-increment: h4-counter;
      counter-reset: h5-counter;
    }

    .post > article h4::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) " ";
    }

    /* Heading 5 */
    .post > article h5 {
      counter-increment: h5-counter;
    }

    .post > article h5::before {
      content: counter(h1-counter) "." counter(h2-counter) "." counter(h3-counter) "." counter(h4-counter) "." counter(h5-counter) " ";
    }
  </style>



<div class="post">
  <article>
    <header class="post-header">
      <h1 class="post-title">Markov Models and $$n$$-grams</h1>
      <p class="post-description"></p>
    </header>

    <p>Models like linear/ logistic regression, multilayer perceptrons (MLPs) and convolutional neural networks (CNNs) operate on fixed-length input (tabular or image data) without sequential structure. <em><u>What is a sequential data?</u></em> Tasks like video analysis, time-series prediction, image captioning, speech synthesis, and translation involve sequentially structured inputs and outputs, requiring specialized models.</p>

<h2 id="introduction">Introduction</h2>
<p>RNNs are designed to capture the dynamics of sequences through <em>recurrent connections</em> that pass information across adjacent time steps. Recurrent neural networks are <em>unrolled</em> across time steps (or sequence steps), with the <em>same</em> underlying parameters applied at each step.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure id="figure-">
  <picture>
    <img src="https://d2l.ai/_images/unfolded-rnn.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
</figure>

    </div>
</div>
<div class="caption">
    RNNs can be visualized as feedforward neural networks where parameters are shared across time steps. On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously.
</div>

<p>RNNs have achieved breakthroughs in tasks like handwritting recognition, translation, and medical diagosis but have been partly replaced by Transformer models.</p>

<h2 id="working-with-sequences">Working with Sequences</h2>

<p><strong>Shift in Perspective</strong>  Traditional models work with a single feature vector \(\mathbf{x} \in \mathbb{R}^{d}\). Sequence models handle ordered lists of feature vectors: \(\mathbf{x}_{1}, \ldots, \mathbf{x}_{T}\), where each vector \(\mathbf{x}_{t} \in \mathbb{R}^{d}\) is indexed by time step \(t \in \mathbb{Z}^{+}\).</p>

<p><strong>Sequential Data</strong>  Some datasets consist of one long sequence (e.g., climate sensor data), sampled into subsequences of length \(T\).<br>
More commonly, data arrive as multiple sequences (e.g., documents, patient stays), where each sequence has its own length \(T_{i}\).<br>
Unlike independent feature vectors, elements within a sequence are dependent:</p>
<ul>
  <li>Later words in a document depend on earlier ones.</li>
  <li>A patient‚Äôs medication depends on prior events.</li>
</ul>

<p><strong>Sequential Dependence</strong>  Sequences reflect patterns that make auto-fill features and predictions possible.<br>
Sequences are modeled as samples from a fixed underlying distribution over entire sequences, \(P(X_{1}, \dots, X_{T})\), rather than assuming independence or stationarity.</p>

<p><strong>Examples of Sequential Tasks</strong></p>
<ol>
  <li>
<strong>Fixed input to fixed target</strong>: Predict a label \(y\) from a sequence (e.g., sentiment classification).</li>
  <li>
<strong>Fixed input to sequential target</strong>: Predict \((y_{1}, \ldots, y_{T})\) from an input (e.g., image captioning).</li>
  <li>
<strong>Sequential input to sequential target</strong>:
    <ul>
      <li>
<em>Aligned</em>: Input at each step aligns with the target (e.g., part-of-speech tagging).</li>
      <li>
<em>Unaligned</em>: No step-wise correspondence (e.g., machine translation).</li>
    </ul>
  </li>
</ol>

<p><strong>Sequence Modeling</strong> The simplest task is <strong>unsupervised density modeling</strong>:  Estimate \(p(\mathbf{x}_{1}, \ldots, \mathbf{x}_{T})\), the likelihood of a sequence, where the probability reflects the joint likelihood of the sequence elements based on their relationships. This is useful for understanding and generating sequences.</p>

<h3 id="autoregressive-models">Autoregressive Models</h3>

<p><strong>Sequence Data</strong>  Autoregressive models analyze sequentially structured data. Consider stock prices like those in the FTSE 100 index: \(x_t, x_{t-1}, \ldots, x_1,\) where each \(x_t\) represents the price at time \(t\). The goal is to predict the next value \(x_t\) based on its history.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure id="figure-">
  <picture>
    <img src="https://d2l.ai/_images/ftse100.png" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
</figure>

    </div>
</div>
<div class="caption">
FTSE 100 index over about 30 years.
</div>

<p><strong>Modeling Sequential Data</strong> Given a sequence, the trader wants to estimate the conditional distribution:</p>

\[P(x_t \mid x_{t-1}, \ldots, x_1),\]

<p>focusing on key statistics like the expected value:</p>

\[\mathbb{E}[x_t \mid x_{t-1}, \ldots, x_1].\]

<p>Autoregressive models perform this task by regressing \(x_t\) onto previous values \(x_{t-1}, \ldots, x_1\). However, the challenge lies in the <em>variable input size</em>, as the number of inputs grows with \(t\).</p>

<p><strong>Strategies for Fixed-Length Inputs</strong></p>
<ol>
  <li>
    <p><strong>Windowing</strong>: Instead of using the full history, consider only a window of size \(\tau\):</p>

\[x_{t-1}, \ldots, x_{t-\tau}.\]

    <p>This reduces the number of inputs to a fixed size for \(t &gt; \tau\), enabling the use of models like linear regression or neural networks.</p>
  </li>
  <li>
    <p><strong>Latent State Representations</strong>: Maintain a latent summary \(h_t\) of past observations.</p>

    <ul>
      <li>
        <p>Predict \(x_t\) using: \(\hat{x}_t = P(x_t \mid h_t).\)</p>
      </li>
      <li>
        <p>Update the latent state with: \(h_t = g(h_{t-1}, x_{t-1}).\)</p>
      </li>
    </ul>

    <p>This approach creates <em>latent autoregressive models</em>, as \(h_t\) is not directly observed.</p>
  </li>
</ol>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure id="figure-">
  <picture>
    <img src="https://d2l.ai/_images/sequence-model.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
</figure>

    </div>
</div>
<div class="caption">
A latent autoregressive model.
</div>

<p><strong>Training Data and Stationarity</strong> Training data is often constructed by sampling fixed-length windows from historical data. Even though the specific values \(x_t\) may change, the underlying generation process is often assumed to be stationary, meaning the dynamics of \(P(x_t \mid x_{t-1}, \ldots, x_1)\) remain consistent over time.</p>

<h3 id="sequence-models">Sequence Models</h3>

<p><strong>Joint Probability of Sequences</strong> <em>Sequence models</em> estimate the joint probability of an entire sequence, typically for data composed of discrete <em>tokens</em> like words. These models are often referred to as <em>language models</em> when dealing with natural language data. Language models are particularly useful for:</p>

<ul>
  <li>Evaluating the likelihood of sequences (e.g., comparing naturalness of sentences in machine translation or speech recognition).</li>
  <li>Sampling sequences and optimizing for the most likely outcomes.</li>
</ul>

<p>The joint probability of a sequence \(P(x_1, \ldots, x_T)\) can be decomposed using the chain rule of probability into conditional densities:</p>

\[P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1).\]

<p>For discrete signals, the model must act as a probabilistic classifier, outputting a distribution over the vocabulary for the next word based on the leftward context.</p>

<h4 id="markov-models">Markov Models</h4>

<p>Instead of conditioning on the entire history \(x_{t-1}, \ldots, x_1\), we may limit the context to the previous \(\tau\) time steps (<em>without any loss in predictive power</em>), i.e., \(x_{t-1}, \ldots, x_{t-\tau}\). This is known as the <em>Markov condition</em>, where the future is conditionally independent of the past given the recent history. When:</p>

<ul>
  <li>\(\tau = 1\), it is called a <em>first - order Markov model</em>.</li>
  <li>\(\tau = k\), it is called a <em>\(k^{th}\) - order Markov model</em>.</li>
</ul>

<p>For a first-order Markov model, the factorization simplifies to:</p>

\[P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}).\]

<p><em>Markov models are practical even if the condition is not strictly true.</em> With real-world text, additional context improves predictions, but the marginal benefits diminish as the context length increases. Hence, many models rely on the Markov assumption for computational efficiency.</p>

<p>For discrete data like language, Markov models estimate \(P(x_t \mid x_{t-1})\) via relative frequency counts and efficiently compute the most likely sequence using <em>dynamic programming</em>.</p>

<h4 id="the-order-of-decoding">The Order of Decoding</h4>

<p>The factorization of a sequence can follow any order (e.g., left-to-right or right-to-left):</p>

<ol>
  <li>
    <p><strong>Left-to-right (natural reading order)</strong>:</p>

\[P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1).\]
  </li>
  <li>
    <p><strong>Right-to-left</strong>:</p>

\[P(x_1, \ldots, x_T) = P(x_T) \prod_{t=T-1}^1 P(x_t \mid x_{t+1}, \ldots, x_T).\]
  </li>
</ol>

<p>While all orders are mathematically valid, left-to-right decoding is preferred for several reasons:</p>
<ul>
  <li>
<strong>Naturalness</strong>: Matches the human reading process.</li>
  <li>
    <p><strong>Incrementality</strong>: Probabilities over sequences can be extended by multiplying by the conditional probability of the next token:</p>

\[P(x_{t+1}, \ldots, x_1) = P(x_{t}, \ldots, x_1) \cdot P(x_{t+1} \mid x_{t}, \ldots, x_1).\]
  </li>
  <li>
<strong>Predictive Power</strong>: Predicting adjacent tokens is often more feasible than predicting tokens at arbitrary positions.</li>
  <li>
<strong>Causal Relationships</strong>: Forward predictions (e.g., \(P(x_{t+1} \mid x_t)\)) often align with causality, whereas reverse predictions (\(P(x_t \mid x_{t+1})\)) are generally infeasible. <em>Future does not affect present</em>.</li>
</ul>

<p>For instance, in causal systems, forward predictions might follow:</p>

\[x_{t+1} = f(x_t) + \epsilon,\]

<p>where \(\epsilon\) represents additive noise. The reverse relationship generally does not hold.</p>

<p>For a more detailed exploration of causality and sequence modeling, refer to <a href="https://mitpress.mit.edu/9780262037310/elements-of-causal-inference/" rel="external nofollow noopener" target="_blank">Elements of Causal Inference</a>.</p>

<h2 id="converting-raw-text-into-sequence-data">Converting Raw Text into Sequence Data</h2>

<p>To make sure the input machine readable we have to do the following:</p>
<ol>
  <li>Load text as strings into memory.</li>
  <li>Split the strings into tokens (e.g., words or characters).</li>
  <li>Build a vocabulary dictionary to associate each vocabulary element with a numerical index.</li>
  <li>Convert the text into sequences of numerical indices.</li>
</ol>

<p>To learn more about this, please read <a href="https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html" rel="external nofollow noopener" target="_blank">[9.2. Converting Raw Text into Sequence Data]</a>. The chapter is really straightforward and should have no confusion.</p>

<h2 id="language-models">Language Models</h2>
<p>Language models play a crucial role in natural language processing and generation tasks. For example, an ideal language model could generate coherent and natural text by sequentially sampling tokens according to their conditional probabilities:</p>

\[x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1).\]

<p>In such a scenario, every token generated would resemble natural language, such as grammatically correct English. This capability would enable the model to engage in meaningful dialogue simply by conditioning its output on previous dialogue fragments. However, achieving this goal would require a model to not only generate syntactically correct text but also <em>understand</em> the underlying context and meaning, a challenge that remains unresolved.</p>

<p><strong>Practical Applications</strong> Despite their limitations, language models offer immense utility in various tasks, such as:</p>

<ol>
  <li>
    <p><strong>Speech Recognition</strong>:
Ambiguities in phonetically similar phrases, such as ‚Äúto recognize speech‚Äù and ‚Äúto wreck a nice beach,‚Äù can be resolved by a language model. The model assigns higher probabilities to plausible interpretations, filtering out nonsensical outputs.</p>
  </li>
  <li>
    <p><strong>Document Summarization</strong>:
Knowing the frequency and naturalness of phrases, a language model can differentiate between:</p>
    <ul>
      <li>‚ÄúDog bites man‚Äù (common) and ‚ÄúMan bites dog‚Äù (rare).</li>
      <li>‚ÄúI want to eat grandma‚Äù (alarming) and ‚ÄúI want to eat, grandma‚Äù (benign).</li>
    </ul>
  </li>
</ol>

<p>Language models enable systems to make contextually appropriate and semantically meaningful decisions, improving the quality of machine-generated language outputs.</p>

<h3 id="learning-language-models">Learning Language Models</h3>

<p>To model a document or a sequence of tokens, we start with the basic probability rules. Suppose we tokenize text data at the word level. The joint probability of a sequence of tokens \(x_1, x_2, \ldots, x_T\) can be expressed as:</p>

\[P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_1, \ldots, x_{t-1}).\]

<p>For instance, the probability of the sequence ‚Äúdeep learning is fun‚Äù can be written as:</p>

\[\begin{aligned}
P(\textrm{deep}, \textrm{learning}, \textrm{is}, \textrm{fun}) = &amp; \, P(\textrm{deep}) \cdot P(\textrm{learning} \mid \textrm{deep}) \\
&amp; \cdot P(\textrm{is} \mid \textrm{deep}, \textrm{learning}) \cdot P(\textrm{fun} \mid \textrm{deep}, \textrm{learning}, \textrm{is}).
\end{aligned}\]

<h4 id="markov-models-and-n-grams">Markov Models and \(n\)-grams</h4>
<p>By applying Markov models, we approximate sequence modeling using limited dependencies. A first-order Markov property assumes that:</p>

\[P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t).\]

<p>This allows us to simplify sequence probabilities. For example:</p>

\[\begin{aligned}
P(x_1, x_2, x_3, x_4) &amp; = P(x_1) P(x_2) P(x_3) P(x_4), \\
P(x_1, x_2, x_3, x_4) &amp; = P(x_1) P(x_2 \mid x_1) P(x_3 \mid x_2) P(x_4 \mid x_3), \\
P(x_1, x_2, x_3, x_4) &amp; = P(x_1) P(x_2 \mid x_1) P(x_3 \mid x_1, x_2) P(x_4 \mid x_2, x_3).
\end{aligned}\]

<p>These approximations correspond to unigram, bigram, and trigram models, respectively. <em>Note that such probabilities are language model parameters.</em></p>

<h4 id="word-frequency">Word Frequency</h4>

<p>Using a large training dataset (e.g., Wikipedia or Project Gutenberg), we estimate probabilities based on word frequencies. For example:</p>

\[\hat{P}(\textrm{deep}) = \frac{\text{count}(\textrm{deep})}{\text{total words}},\]

<p>and for word pairs:</p>

\[\hat{P}(\textrm{learning} \mid \textrm{deep}) = \frac{\text{count}(\textrm{deep, learning})}{\text{count}(\textrm{deep})}.\]

<p>However, estimating probabilities for rare word combinations becomes challenging as data sparsity increases.</p>

<h4 id="laplace-smoothing">Laplace Smoothing</h4>

<p>To handle data sparsity, we use <em>Laplace smoothing</em>, adding a small constant \(\epsilon\) to all counts:</p>

\[\begin{aligned}
\hat{P}(x) &amp; = \frac{n(x) + \epsilon/m}{n + \epsilon}, \\
\hat{P}(x' \mid x) &amp; = \frac{n(x, x') + \epsilon \hat{P}(x')}{n(x) + \epsilon}, \\
\hat{P}(x'' \mid x, x') &amp; = \frac{n(x, x', x'') + \epsilon \hat{P}(x'')}{n(x, x') + \epsilon}.
\end{aligned}\]

<p>Here, \(n\) is the total number of words in the training set, \(m\) is the number of unique words and \(\epsilon\) determines the degree of smoothing, where \(\epsilon = 0\) means no smoothing and \(\epsilon \to \infty\) approximates a uniform distribution.</p>

<p><strong>Challenges with \(n\)-grams</strong> \(n\)-gram models face issues such as large storage requirements, inability to capture word meaning (e.g., ‚Äúcat‚Äù and ‚Äúfeline‚Äù), and poor performance on novel sequences. Deep learning models address these shortcomings by learning contextual representations and generalizing better to unseen data.</p>

<h3 id="perplexity">Perplexity</h3>
<p>A good language model is able to predict, with high accuracy, the tokens that come next.</p>

<ol>
  <li>‚ÄúIt is raining outside‚Äù</li>
  <li>‚ÄúIt is raining banana tree‚Äù</li>
  <li>‚ÄúIt is raining piouw;kcj pwepoiut‚Äù</li>
</ol>

<p>Perplexity measures how well a language model predicts a sequence. Given a sequence of \(n\) tokens, the cross-entropy loss averaged over the sequence is:</p>

\[\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1).\]

<p>The <em>perplexity</em> is defined as the exponential of the average cross-entropy:</p>

\[\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).\]

<p>Interpretations:</p>

<ul>
  <li>
<strong>Best Case</strong>: The model predicts \(P(x_t) = 1\) for all tokens. Perplexity = 1.</li>
  <li>
<strong>Worst Case</strong>: The model predicts \(P(x_t) = 0\) for some token. Perplexity = infinity.</li>
  <li>
<strong>Baseline</strong>: For uniform probability distribution over \(m\) tokens, perplexity = \(m\).</li>
</ul>

<p>Lower perplexity indicates a better language model.</p>

<h3 id="partitioning-sequences">Partitioning Sequences</h3>

<p>Language models process minibatches of sequences (assumption; <strong>question</strong>, how to read minibatches of input sequences and target sequences at random?). Suppose we have a dataset of token indices \(x_1, x_2, \ldots, x_T\). To prepare the data:</p>

<ol>
  <li>Partition the dataset into subsequences of length \(n\).</li>
  <li>Introduce randomness by discarding \(d\) tokens at the beginning, where \(d \in [0, n)\).</li>
  <li>Create \(m = \lfloor (T - d) / n \rfloor\) subsequences:
\(\mathbf{x}_d, \mathbf{x}_{d+n}, \ldots, \mathbf{x}_{d+n(m-1)}.\)</li>
</ol>

<p>Each subsequence \(\mathbf{x}_t = [x_t, \ldots, x_{t+n-1}]\) serves as the input, and the target is the sequence shifted by one token: \(\mathbf{x}_{t+1} = [x_{t+1}, \ldots, x_{t+n}].\)</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure id="figure-">
  <picture>
    <img src="https://d2l.ai/_images/lang-model-data.svg" class="img-fluid rounded" style="
        
          
            max-width: 500px;
          
          height: auto;
          display: block;
          margin-left: auto;
          margin-right: auto;
        
      " loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>
  
</figure>

    </div>
</div>
<div class="caption">
Obtaining five pairs of input sequences and target sequences from partitioned length-5 subsequences \((n=5 \text{ and } d=2)\).
</div>

<p>Before we move on, a <em>summary</em>, Language models estimate the joint probability of a text sequence. For long sequences, \(n\) - grams provide a convenient model by truncating the dependence. However, there is a lot of structure but not enough frequency to deal efficiently with infrequent word combinations via Laplace smoothing. Thus, we will focus on neural language modeling in subsequent sections. To train language models, we can randomly sample pairs of input sequences and target sequences in minibatches. After training, we will use perplexity to measure the language model quality.</p>

<p>Language models can be scaled up with increased data size, model size, and amount in training compute. Large language models can perform desired tasks by predicting output text given input text instructions.</p>

<p>With the background thoroghly introduced, let‚Äôs move on to <a href="/personal/assets/courses/deeplearning/rnn/vinilla_rnn">Recurrent Neural Network</a>. Please post any question you may have on piazza.</p>


    

    
  </article>
</div>
</div>
          </div>
        
      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      ¬© Copyright 2024
      Jue
      
      Guo. 
      
      
        Last updated: December 13, 2024.
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/personal/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/personal/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/personal/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>


  <!-- Sidebar Table of Contents -->
  <script defer src="/personal/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script>


<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/personal/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/personal/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/personal/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/personal/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  

    

    



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/personal/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
